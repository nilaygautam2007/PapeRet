{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fe225c8-3a6b-4ee2-8e49-9f71c0a61946",
   "metadata": {},
   "source": [
    "### This file will download papers to a desired location given its DOI. It should be used right after obtaining a list of DOIs from similar papers to a seed paper (see get_similar_papers.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3561e64a-d6dc-4a8d-a4f8-f847e1f170c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import bibtexparser\n",
    "import requests\n",
    "import urllib\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium import webdriver\n",
    "from urllib.parse import quote\n",
    "\n",
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d36ee4e0-130d-458d-8740-de5da24e998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless')\n",
    "options.add_argument(\n",
    "'user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36')\n",
    "\n",
    "options.add_argument(\"enable-automation\");\n",
    "options.add_argument(\"--window-size=1920,1080\");\n",
    "options.add_argument(\"--no-sandbox\");\n",
    "options.add_argument(\"--disable-extensions\");\n",
    "options.add_argument(\"--dns-prefetch-disable\");\n",
    "options.add_argument(\"--disable-gpu\");\n",
    "\n",
    "\n",
    "options.add_experimental_option('prefs', {      \n",
    "\"download.default_directory\": \"/Users/rishikesh/Downloads/try_openalex_pdfs\",  # Papers will be downloaded at this address\n",
    "\"download.prompt_for_download\": False,  \n",
    "\"download.directory_upgrade\": True,\n",
    "\"plugins.always_open_pdf_externally\": True  \n",
    "})\n",
    "\n",
    "# driver = webdriver.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a125d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = '/Users/rishikesh/Downloads/try_openalex_pdfs'\n",
    "\n",
    "def clear_download_directory():\n",
    "    \"\"\"Clear any files in the download directory.\"\"\"\n",
    "    for file in os.listdir(download_dir):\n",
    "        file_path = os.path.join(download_dir, file)\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting file {file_path}: {e}\")\n",
    "\n",
    "def wait_for_download():\n",
    "    \"\"\"Wait until a new file appears in the download directory and return its path.\"\"\"\n",
    "    t = 0\n",
    "    time.sleep(1)\n",
    "    while t<20:\n",
    "        files = os.listdir(download_dir)\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.pdf'):\n",
    "                return os.path.join(download_dir, file)\n",
    "        t += 1\n",
    "        time.sleep(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"Extract text from a PDF file and return as a string.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            for page in pdf:\n",
    "                text += page.get_text(\"text\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path}: {e}\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a0a49e60-ab0b-4129-8101-ee27f1ee7fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your science_direct api key and insstoken. Insstoken needs to be requested from sciencedirect.com. \n",
    "science_direct_api_key = \"be3a3b1c4e6171897b5a922185dd9104\"  \n",
    "science_direct_insttoken = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f265e26c-b617-4e27-af3c-a62d69e3fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_science_direct(doi, api_key=science_direct_api_key, insttoken=science_direct_insttoken):\n",
    "    try:\n",
    "        url = f\"https://api.elsevier.com/content/article/doi/{doi}?apiKey={api_key}&httpAccept=application%2Fpdf\"#&insttoken={insttoken}\"\n",
    "        # print('sd url', url)\n",
    "        response = urllib.request.urlopen(url)\n",
    "        \n",
    "        pdf_content = response.read()\n",
    "        # print(pdf_content)\n",
    "\n",
    "\n",
    "        directory = \"/Users/rishikesh/Downloads/try_openalex_pdfs\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        filename = f\"{directory}/{doi.replace('/', '_')}.pdf\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(pdf_content)\n",
    "            # print('Saved for SD')\n",
    "\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(f\"HTTPError: {e.code} for DOI {doi}\")\n",
    "        if e.code == 500:\n",
    "            print(\"Internal Server Error. The server encountered an unexpected condition.\")\n",
    "        elif e.code == 403:\n",
    "            print(\"Forbidden. You may not have permission to access this resource. Check your API key and institution token.\")\n",
    "        elif e.code == 404:\n",
    "            print(\"Not Found. The DOI may not exist or is not available in this format.\")\n",
    "        else:\n",
    "            print(\"An HTTP error occurred:\", e)\n",
    "        \n",
    "    except urllib.error.URLError as e:\n",
    "        # Handle other URL errors (e.g., network issues)\n",
    "        print(f\"URLError: {e.reason} for DOI {doi}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download PDF for DOI {doi}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "40296fad-2c84-46ff-941d-5f993285c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_retrieved = []\n",
    "retrieved = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "25041d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def institutional_login(driver):\n",
    "    driver.get('https://ieeexplore.ieee.org/Xplore/home.jsp')\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Click the 'Institutional Sign In' button\n",
    "    inst_sign_in_button = driver.find_element(By.LINK_TEXT, 'Institutional Sign In')\n",
    "    inst_sign_in_button.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    # At this point, the user can manually select the university and enter credentials\n",
    "    print(\"Please select your university and complete the login manually (including Duo authentication if required).\")\n",
    "    input(\"Press Enter here after you have fully logged in...\")  # Wait for manual completion\n",
    "\n",
    "# Function to check if the PDF link is accessible\n",
    "def is_pdf_accessible(pdf_link):\n",
    "    try:\n",
    "        response = requests.head(pdf_link, timeout=5)\n",
    "        content_type = response.headers.get('Content-Type', '')\n",
    "        return 'pdf' in content_type  # Check if Content-Type includes 'pdf'\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error checking URL {pdf_link}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Main function to download IEEE PDFs for multiple DOIs\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Main function to download IEEE PDFs for multiple DOIs\n",
    "def download_ieee_pdfs(driver, doi):\n",
    "    url = f\"https://doi.org/{doi}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    current_url = driver.current_url\n",
    "    if \"ieeexplore.ieee.org\" in current_url:\n",
    "        try:\n",
    "            # Extract the document number and construct the PDF link (stamp page link)\n",
    "            doc_number = current_url.split(\"/\")[-1]\n",
    "            # pdf_link = f\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber={doc_number}\"\n",
    "            pdf_link = f\"https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber={doc_number}&ref=\"\n",
    "            # print(f\"Attempting to access the stamp page for DOI {doi}...\")\n",
    "\n",
    "            # Navigate to the stamp page\n",
    "            driver.get(pdf_link)\n",
    "            time.sleep(1)  # Wait for the page to load\n",
    "\n",
    "            # Look for the \"Open\" button on the page\n",
    "            # try:\n",
    "                # open_button = driver.find_element(By.XPATH, \"//button[contains(text(), 'Open')]\")\n",
    "            #     open_button.click()\n",
    "            #     print(f\"Clicked 'Open' button to download PDF for DOI {doi}\")\n",
    "            #     retrieved.append(doi)\n",
    "            #     time.sleep(3)  # Wait for download to start\n",
    "            # except NoSuchElementException:\n",
    "            #     print(f\"No 'Open' button found on page for DOI {doi}. PDF might have been downloaded automatically.\")\n",
    "            #     retrieved.append(doi)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while attempting to download the PDF for DOI {doi}: {e}\")\n",
    "            not_retrieved.append(doi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3db17ede-8322-4db3-bf61-1304d90006ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will automatically download a paper to a desired location given its DOI\n",
    "# The download location can be defined in the second cell under options.add_experimental_option -> \"download.default_directory\"   \n",
    "\n",
    "def download_pdf(driver, doi, ieee_dois):\n",
    "\n",
    "    clear_download_directory()\n",
    "\n",
    "    fetched = 0\n",
    "\n",
    "    url = \"https://doi.org/\" + doi\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing the URL: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    # driver.get(url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    current_url = driver.current_url\n",
    "    # print(current_url)\n",
    "\n",
    "    if \"sciencedirect\" in current_url and fetched==0:\n",
    "        download_science_direct(doi=doi)\n",
    "        retrieved.append(doi)\n",
    "        fetched = 1\n",
    "        \n",
    "        # return\n",
    "        \n",
    "    url_split = current_url.split(\"/\")\n",
    "    url_find = url_split[-1].split(\"?\")[0]\n",
    "\n",
    "    doi_split = doi.split(\"/\")\n",
    "\n",
    "    # journals.sagepub.com\n",
    "    if \"journals.sagepub.com\" in current_url and fetched==0:\n",
    "        # print('BS')\n",
    "        try:\n",
    "            reader_href = f\"https://journals.sagepub.com/doi/reader/{doi}\"\n",
    "            driver.get(reader_href)\n",
    "            time.sleep(1)\n",
    "            download_links = driver.find_elements(By.CSS_SELECTOR, \"a[class*='btn--light btn format-download-btn download']\")\n",
    "            for link in download_links:\n",
    "                href = link.get_attribute('href')\n",
    "                if href:\n",
    "                    driver.get(href)\n",
    "                    time.sleep(1)\n",
    "                    retrieved.append(doi)\n",
    "                    # fetched=1\n",
    "                    # print('journals.sagepub.com')\n",
    "                    \n",
    "                    try:\n",
    "                        response = requests.head(href, timeout=5)\n",
    "                        content_type = response.headers.get('Content-Type', '')\n",
    "                        # print('pdf' in content_type)  # Checks if Content-Type includes 'pdf'\n",
    "                        if 'pdf' in content_type:\n",
    "                            fetched=1\n",
    "                    except requests.RequestException as e:\n",
    "                        print(f\"Error checking URL {href}: {e}\")\n",
    "                        return \"\"\n",
    "\n",
    "                    # return\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    # Taylor & Francis\n",
    "    if \"www.tandfonline.com\" in current_url and fetched==0:\n",
    "        # print('Other BS')\n",
    "        # try:\n",
    "        #     doi = current_url.split(\"full/\")[1].split(\"#\")[0]\n",
    "        #     reader_href = f\"https://www.tandfonline.com/doi/pdf/{doi}/?download=true\"\n",
    "        #     driver.get(reader_href)\n",
    "        #     # print(2)\n",
    "        #     time.sleep(1)\n",
    "        #     retrieved.append(doi)\n",
    "        #     fetched = 1\n",
    "        #     # try:\n",
    "        #     #     response = requests.head(href, timeout=5)\n",
    "        #     #     content_type = response.headers.get('Content-Type', '')\n",
    "        #     #     print('pdf' in content_type)  # Checks if Content-Type includes 'pdf'\n",
    "        #     # except requests.RequestException as e:\n",
    "        #     #     print(f\"Error checking URL {href}: {e}\")\n",
    "        #         # return False\n",
    "        #     # return\n",
    "            \n",
    "        # except:\n",
    "        #     pass\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "    # ACM DL\n",
    "    if \"dl.acm.org\" in current_url and fetched==0:\n",
    "        pdf_link = f\"https://dl.acm.org/doi/pdf/{doi}\"\n",
    "        driver.get(pdf_link)\n",
    "        retrieved.append(doi)\n",
    "        fetched = 1\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "    # ieee\n",
    "                    \n",
    "\n",
    "    if \"ieeexplore.ieee.org\" in current_url and fetched==0:\n",
    "        try:\n",
    "            ieee_dois.append(doi)\n",
    "            download_ieee_pdfs(driver, doi)\n",
    "            retrieved.append(doi)\n",
    "            fetched=1\n",
    "            # doc_number = current_url.split(\"/\")[-1]\n",
    "            # pdf_link = f\"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber={doc_number}\"\n",
    "            # print('ieee')\n",
    "            # # print(pdf_link)\n",
    "            # driver.get(pdf_link)\n",
    "            # time.sleep(1)\n",
    "            # retrieved.append(doi)\n",
    "            # # print(3)\n",
    "            # try:\n",
    "            #     response = requests.head(pdf_link, timeout=5)\n",
    "            #     content_type = response.headers.get('Content-Type', '')\n",
    "            #     print('pdf' in content_type)  # Checks if Content-Type includes 'pdf'\n",
    "            # except requests.RequestException as e:\n",
    "            #     print(f\"Error checking URL {href}: {e}\")\n",
    "            #     # return False\n",
    "            # return\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # plos:\n",
    "    if \"journals.plos.org\" in current_url and fetched==0:\n",
    "        try:\n",
    "            split = current_url.split(\"?\")\n",
    "            prefix = split[0]\n",
    "            id = split[-1]\n",
    "\n",
    "            url = f\"{prefix}/file?{id}&type=printable\"\n",
    "            driver.get(url)\n",
    "            time.sleep(1)\n",
    "            retrieved.append(doi)\n",
    "            fetched = 1\n",
    "            # print(4)\n",
    "            # try:\n",
    "            #     response = requests.head(href, timeout=5)\n",
    "            #     content_type = response.headers.get('Content-Type', '')\n",
    "            #     print('pdf' in content_type)  # Checks if Content-Type includes 'pdf'\n",
    "            # except requests.RequestException as e:\n",
    "            #     print(f\"Error checking URL {href}: {e}\")\n",
    "            #     # return False\n",
    "            # return\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "\n",
    "    # agupubs.onlinelibrary.wiley.com\n",
    "    if \"agupubs.onlinelibrary.wiley.com\" in current_url and fetched==0:\n",
    "        try:\n",
    "            href = f\"https://agupubs.onlinelibrary.wiley.com/doi/pdfdirect/{doi}?download=true\"\n",
    "            driver.get(href)\n",
    "            time.sleep(1)\n",
    "            retrieved.append(doi)\n",
    "            fetched = 1\n",
    "            # print(4)\n",
    "            # try:\n",
    "            #     response = requests.head(href, timeout=5)\n",
    "            #     content_type = response.headers.get('Content-Type', '')\n",
    "            #     print('pdf' in content_type)  # Checks if Content-Type includes 'pdf'\n",
    "            # except requests.RequestException as e:\n",
    "            #     print(f\"Error checking URL {href}: {e}\")\n",
    "            #     # return False\n",
    "            # return\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    # www.mdpi.com\n",
    "    if \"www.mdpi.com\" in current_url and fetched==0:\n",
    "        try:\n",
    "            links = driver.find_element(By.CSS_SELECTOR, 'a=[class*=\"UD_ArticlePDF')\n",
    "            href = link.get_attribute('href')\n",
    "            driver.get(href)\n",
    "            time.sleep(1)\n",
    "            retrieved.append(doi)\n",
    "            fetched = 1\n",
    "            # print(5)\n",
    "            # try:\n",
    "            #     response = requests.head(href, timeout=5)\n",
    "            #     content_type = response.headers.get('Content-Type', '')\n",
    "            #     print('pdf' in content_type)  # Checks if Content-Type includes 'pdf'\n",
    "            # except requests.RequestException as e:\n",
    "            #     print(f\"Error checking URL {href}: {e}\")\n",
    "            #     # return False\n",
    "            # return\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "                \n",
    "    \n",
    "    # AMS Publications\n",
    "    if \"journals.ametsoc.org\" in current_url and fetched==0:\n",
    "        try:\n",
    "            href = current_url.replace(\"view\", \"downloadpdf/view\").replace(\".xml\", \".pdf\")\n",
    "            driver.get(href)\n",
    "            time.sleep(1)\n",
    "            retrieved.append(doi)\n",
    "            fetched = 1\n",
    "            # print(6)\n",
    "            # try:\n",
    "            #     response = requests.head(href, timeout=5)\n",
    "            #     content_type = response.headers.get('Content-Type', '')\n",
    "            #     print('pdf' in content_type)  # Checks if Content-Type includes 'pdf'\n",
    "            # except requests.RequestException as e:\n",
    "            #     print(f\"Error checking URL {href}: {e}\")\n",
    "            #     # return False\n",
    "            # return\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    # Now publisher\n",
    "    if \"nowpublishers\" in current_url and fetched==0:\n",
    "        try:\n",
    "            href = f\"https://www.nowpublishers.com/article/Download/{url_find}\"\n",
    "            driver.get(href)\n",
    "            time.sleep(1)\n",
    "            retrieved.append(doi)\n",
    "            fetched = 1\n",
    "            # print(7)\n",
    "            # try:\n",
    "            #     response = requests.head(href, timeout=5)\n",
    "            #     content_type = response.headers.get('Content-Type', '')\n",
    "            #     print('pdf' in content_type)  # Checks if Content-Type includes 'pdf'\n",
    "            # except requests.RequestException as e:\n",
    "            #     print(f\"Error checking URL {href}: {e}\")\n",
    "            #     # return False\n",
    "            # return\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    # a general case for many paper websites\n",
    "    try:\n",
    "        pdf_links = driver.find_elements(By.CSS_SELECTOR, \"a[class*='article-pdfLink']\")\n",
    "        for pdf_link in pdf_links:\n",
    "            href = pdf_link.get_attribute('href')\n",
    "            if href:\n",
    "                driver.get(href)\n",
    "                time.sleep(1)\n",
    "                retrieved.append(doi)\n",
    "                fetched = 1\n",
    "                # print(8)\n",
    "                # try:\n",
    "                #     response = requests.head(href, timeout=5)\n",
    "                #     content_type = response.headers.get('Content-Type', '')\n",
    "                #     print('pdf' in content_type)  # Checks if Content-Type includes 'pdf'\n",
    "                # except requests.RequestException as e:\n",
    "                #     print(f\"Error checking URL {href}: {e}\")\n",
    "                #     # return False\n",
    "                # return\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "    # www.degruyter.com\n",
    "    if \"degruyter.com\" in current_url and fetched==0:\n",
    "        try:\n",
    "            pdf_links = driver.find_elements(By.CSS_SELECTOR, \"a[class*='ga_download_button_pdf_article downloadCompletePdfArticle downloadPdf btn btn-primary fw-bold py-2 w-100 vgwort-click']\")\n",
    "            for pdf_link in pdf_links:\n",
    "                href = pdf_link.get_attribute('href')\n",
    "                if href:\n",
    "                    driver.get(href)\n",
    "                    time.sleep(1)\n",
    "                    retrieved.append(doi)\n",
    "                    fetched = 1\n",
    "                    # print(9)\n",
    "                    # try:\n",
    "                    #     response = requests.head(href, timeout=5)\n",
    "                    #     content_type = response.headers.get('Content-Type', '')\n",
    "                    #     print('pdf' in content_type)  # Checks if Content-Type includes 'pdf'\n",
    "                    # except requests.RequestException as e:\n",
    "                    #     print(f\"Error checking URL {href}: {e}\")\n",
    "                    #     # return False\n",
    "                    # return\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    # academic.oup.com\n",
    "    if \"academic.oup.com\" in current_url and fetched==0:\n",
    "        try:\n",
    "            pdf_link = driver.find_element(By.CSS_SELECTOR, \"a.al-link.pdf.article-pdfLink\")\n",
    "            href = pdf_link.get_attribute('href')\n",
    "            if href:\n",
    "                driver.get(href)\n",
    "                time.sleep(1)\n",
    "                retrieved.append(doi)\n",
    "                fetched = 1\n",
    "                # print(10)\n",
    "                # try:\n",
    "                #     response = requests.head(href, timeout=5)\n",
    "                #     content_type = response.headers.get('Content-Type', '')\n",
    "                #     print('pdf' in content_type)  # Checks if Content-Type includes 'pdf'\n",
    "                # except requests.RequestException as e:\n",
    "                #     print(f\"Error checking URL {href}: {e}\")\n",
    "                #     # return False\n",
    "                # return\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    href_conditions = [\n",
    "        doi_split[-1] + \".pdf\",\n",
    "        doi_split[-1] + \"-pdf\",\n",
    "        doi_split[-1] + \"/pdf\",\n",
    "        \"epdf/\" + doi,\n",
    "        \"reader/\" + doi,\n",
    "        url_find + \"/pdf\",\n",
    "        url_find + \"/pdfft\"\n",
    "    ]   \n",
    "        \n",
    "    # If form is not found, search for href-based downloads\n",
    "    for condition in href_conditions:\n",
    "        try:\n",
    "            # Find an element where the href contains the condition\n",
    "            elements = driver.find_elements(By.XPATH, f\"//a[contains(@href, '{condition}')]\")\n",
    "            for element in elements:\n",
    "                href = element.get_attribute('href')\n",
    "                driver.get(href)\n",
    "                retrieved.append(doi)\n",
    "                fetched = 1\n",
    "                # print(11)\n",
    "                # try:\n",
    "                #     response = requests.head(href, timeout=5)\n",
    "                #     content_type = response.headers.get('Content-Type', '')\n",
    "                #     print('pdf' in content_type)  # Checks if Content-Type includes 'pdf'\n",
    "                # except requests.RequestException as e:\n",
    "                #     print(f\"Error checking URL {href}: {e}\")\n",
    "                #     # return False\n",
    "                # return\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    # Additional check for a condition that occurs in ScienceDirect papers: doi and \"pdf\" present without concatenation\n",
    "    try:\n",
    "        elements = driver.find_elements(By.XPATH, \"//a[contains(@href, 'pdf')]\")\n",
    "        for element in elements:\n",
    "            href = element.get_attribute('href')\n",
    "            if (doi in href and \"pdf\" in href) or (url_find in href and \"pdf\" in href):\n",
    "                # print(url_find)\n",
    "                # print(href)\n",
    "                driver.get(href)\n",
    "                retrieved.append(doi)\n",
    "                fetched = 1\n",
    "                # print(12)\n",
    "                # try:\n",
    "                #     response = requests.head(href, timeout=5)\n",
    "                #     content_type = response.headers.get('Content-Type', '')\n",
    "                #     print('pdf' in content_type)  # Checks if Content-Type includes 'pdf'\n",
    "                # except requests.RequestException as e:\n",
    "                #     print(f\"Error checking URL {href}: {e}\")\n",
    "                # return False\n",
    "                # return\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    # Check if there is a form for PDF download\n",
    "    try:\n",
    "        form = driver.find_element(By.XPATH, \"//form[contains(@class, 'ft-download-content__form ft-download-content__form--pdf js-ft-download-form')]\")\n",
    "        if form:                                                       \n",
    "            form.submit()\n",
    "            time.sleep(1)\n",
    "            retrieved.append(doi)\n",
    "            fetched = 1\n",
    "            # print(13)\n",
    "            # try:\n",
    "            #     response = requests.head(href, timeout=5)\n",
    "            #     content_type = response.headers.get('Content-Type', '')\n",
    "            #     print('pdf' in content_type)  # Checks if Content-Type includes 'pdf'\n",
    "            # except requests.RequestException as e:\n",
    "            #     print(f\"Error checking URL {href}: {e}\")\n",
    "                # return False\n",
    "            # return\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "    not_retrieved.append(doi)\n",
    "\n",
    "\n",
    "    if fetched==1:\n",
    "        downloaded_file = wait_for_download()\n",
    "\n",
    "        # Extract text from the downloaded PDF\n",
    "        if downloaded_file:\n",
    "            text_content = extract_text_from_pdf(downloaded_file)\n",
    "\n",
    "        # Delete the PDF file after extracting text\n",
    "            os.remove(downloaded_file)\n",
    "\n",
    "            return text_content  # Return the extracted text\n",
    "        \n",
    "    return \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "07d300ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select your university and complete the login manually (including Duo authentication if required).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4299/45140 [03:36<10:20:21,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4300/45140 [03:44<23:29:15,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Count-4300 : Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4399/45140 [14:05<96:32:48,  8.53s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4400/45140 [14:19<111:02:14,  9.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Count-4400 : Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4499/45140 [21:55<67:48:18,  6.01s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4500/45140 [22:03<75:48:56,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Count-4500 : Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4599/45140 [33:28<61:19:28,  5.45s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4600/45140 [33:42<81:44:42,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Count-4600 : Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4699/45140 [42:42<41:44:07,  3.72s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4700/45140 [42:52<60:22:15,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Count-4700 : Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4737/45140 [47:08<55:01:57,  4.90s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPError: 500 for DOI 10.1016/c2013-0-11671-6\n",
      "Internal Server Error. The server encountered an unexpected condition.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 4799/45140 [53:50<64:37:02,  5.77s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 4800/45140 [54:00<73:09:38,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Count-4800 : Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 4884/45140 [1:01:05<8:23:33,  1.33it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mif\u001b[39;00m paper\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdoi\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     18\u001b[0m     \u001b[39mif\u001b[39;00m (paper\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m): \u001b[39m# and idx > num\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m         paper[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m download_pdf(driver, paper\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdoi\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m), ieee_dois)\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m (idx\u001b[39m%\u001b[39m\u001b[39m100\u001b[39m)\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m: \u001b[39m# and idx > num\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[39m# print(idx)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mOpenAlex_Text_Depth.json\u001b[39m\u001b[39m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[124], line 392\u001b[0m, in \u001b[0;36mdownload_pdf\u001b[0;34m(driver, doi, ieee_dois)\u001b[0m\n\u001b[1;32m    388\u001b[0m not_retrieved\u001b[39m.\u001b[39mappend(doi)\n\u001b[1;32m    391\u001b[0m \u001b[39mif\u001b[39;00m fetched\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m--> 392\u001b[0m     downloaded_file \u001b[39m=\u001b[39m wait_for_download()\n\u001b[1;32m    394\u001b[0m     \u001b[39m# Extract text from the downloaded PDF\u001b[39;00m\n\u001b[1;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m downloaded_file:\n",
      "Cell \u001b[0;32mIn[108], line 22\u001b[0m, in \u001b[0;36mwait_for_download\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(download_dir, file)\n\u001b[1;32m     21\u001b[0m     t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 22\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open (\"OpenAlex_Text_Depth.json\") as file:\n",
    "    papers = json.load(file)\n",
    "    \n",
    "ieee_dois = []\n",
    "idx = 0\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "institutional_login(driver)\n",
    "\n",
    "for paper in tqdm(papers[:]):\n",
    "    idx += 1\n",
    "    if idx>4200:\n",
    "        if paper.get('doi', '') != '':\n",
    "            if (paper.get('text', '') == ''): # and idx > num\n",
    "                paper['text'] = download_pdf(driver, paper.get('doi', ''), ieee_dois)\n",
    "            \n",
    "        if (idx%100)==0: # and idx > num\n",
    "            # print(idx)\n",
    "            filename = f'OpenAlex_Text_Depth.json'\n",
    "            print('Saving')\n",
    "            with open(filename, \"w\") as json_file:\n",
    "                json.dump(papers, json_file, indent=2)\n",
    "            print('\\n')\n",
    "            print(f'Count-{idx} : Saved')\n",
    "# driver.quit()\n",
    "\n",
    "# 2549 / 4800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dbb255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6098458b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "135f31cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"OpenAlex_Text_Depth.json\") as file:\n",
    "    papers = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bdd2a62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ieee_dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ac3c502a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3261\n"
     ]
    }
   ],
   "source": [
    "for i,p in enumerate(papers):\n",
    "    doi = p.get('doi')\n",
    "    if doi in ieee_dois:\n",
    "        if p.get('text', '') == '':\n",
    "            print(i)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eee83578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'A multigrid tutorial',\n",
       " 'doi': '',\n",
       " 'openalex_id': 'https://openalex.org/W1530872699',\n",
       " 'authors': ['William L. Briggs'],\n",
       " 'publication_date': '1987-01-01',\n",
       " 'publish_year': 1987,\n",
       " 'keywords': ['Multigrid method',\n",
       "  'Computer science',\n",
       "  'Nonlinear system',\n",
       "  'Applied mathematics',\n",
       "  'Bibliography',\n",
       "  'Iterative method',\n",
       "  'Index (typography)',\n",
       "  'Mathematics',\n",
       "  'Algorithm',\n",
       "  'Physics',\n",
       "  'Programming language',\n",
       "  'Library science',\n",
       "  'Quantum mechanics'],\n",
       " 'abstract': 'Preface 1. Model problems 2. Basic iterative methods 3. Elements of multigrid 4. Implementation 5. Some theory 6. Nonlinear problems 7. Selected applications 8. Algebraic multigrid (AMG) 9. Multilevel adaptive methods 10. Finite elements Bibliography Index.',\n",
       " 'global_link_openable': 'https://openalex.org/W1530872699',\n",
       " 'citation_count': 1951,\n",
       " 'publication': [],\n",
       " 'references': []}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caffb4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e15b9924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'HOGgles: Visualizing Object Detection Features',\n",
       " 'doi': '10.1109/iccv.2013.8',\n",
       " 'openalex_id': 'https://openalex.org/W1982428585',\n",
       " 'authors': ['Carl Vondrick',\n",
       "  'Aditya Khosla',\n",
       "  'Tomasz Malisiewicz',\n",
       "  'Antonio Torralba'],\n",
       " 'publication_date': '2013-12-01',\n",
       " 'publish_year': 2013,\n",
       " 'keywords': ['Computer science',\n",
       "  'False positive paradox',\n",
       "  'Object detection',\n",
       "  'Feature (linguistics)',\n",
       "  'Artificial intelligence',\n",
       "  'Object (grammar)',\n",
       "  'Detector',\n",
       "  'Visualization',\n",
       "  'Space (punctuation)',\n",
       "  'Feature vector',\n",
       "  'Computer vision',\n",
       "  'Feature extraction',\n",
       "  'Pattern recognition (psychology)',\n",
       "  'Telecommunications',\n",
       "  'Philosophy',\n",
       "  'Linguistics',\n",
       "  'Operating system'],\n",
       " 'abstract': \"We introduce algorithms to visualize feature spaces used by object detectors. The tools in this paper allow a human to put on 'HOG goggles' and perceive the visual world as a HOG based object detector sees it. We found that these visualizations allow us to analyze object detection systems in new ways and gain new insight into the detector's failures. For example, when we visualize the features for high scoring false alarms, we discovered that, although they are clearly wrong in image space, they do look deceptively similar to true positives in feature space. This result suggests that many of these false alarms are caused by our choice of feature space, and indicates that creating a better learning algorithm or building bigger datasets is unlikely to correct these errors. By visualizing feature spaces, we can gain a more intuitive understanding of our detection systems.\",\n",
       " 'global_link_openable': 'https://openalex.org/W1982428585',\n",
       " 'citation_count': 292,\n",
       " 'publication': [{'venue_name': 'DSpace@MIT (Massachusetts Institute of Technology)',\n",
       "   'publisher': 'https://openalex.org/I63966007',\n",
       "   'pdf_url': 'https://dspace.mit.edu/bitstream/1721.1/90950/1/Torralba_HOGgles.pdf'},\n",
       "  {'venue_name': 'CiteSeer X (The Pennsylvania State University)',\n",
       "   'publisher': 'https://openalex.org/I130769515',\n",
       "   'pdf_url': 'http://people.csail.mit.edu/tomasz/papers/vondrick_iccv2013.pdf'}],\n",
       " 'references': [{'openalex_id': 'https://openalex.org/W1491719799'},\n",
       "  {'openalex_id': 'https://openalex.org/W1542534308'},\n",
       "  {'openalex_id': 'https://openalex.org/W1566135517'},\n",
       "  {'openalex_id': 'https://openalex.org/W1832500336'},\n",
       "  {'openalex_id': 'https://openalex.org/W1989684337'},\n",
       "  {'openalex_id': 'https://openalex.org/W1989975849'},\n",
       "  {'openalex_id': 'https://openalex.org/W1995266040'},\n",
       "  {'openalex_id': 'https://openalex.org/W2005876975'},\n",
       "  {'openalex_id': 'https://openalex.org/W2017974076'},\n",
       "  {'openalex_id': 'https://openalex.org/W2036662475'},\n",
       "  {'openalex_id': 'https://openalex.org/W2036989445'},\n",
       "  {'openalex_id': 'https://openalex.org/W2037227137'},\n",
       "  {'openalex_id': 'https://openalex.org/W2046875449'},\n",
       "  {'openalex_id': 'https://openalex.org/W2095430846'},\n",
       "  {'openalex_id': 'https://openalex.org/W2113606819'},\n",
       "  {'openalex_id': 'https://openalex.org/W2120824855'},\n",
       "  {'openalex_id': 'https://openalex.org/W2121058967'},\n",
       "  {'openalex_id': 'https://openalex.org/W2126810579'},\n",
       "  {'openalex_id': 'https://openalex.org/W2161366920'},\n",
       "  {'openalex_id': 'https://openalex.org/W2168356304'},\n",
       "  {'openalex_id': 'https://openalex.org/W82130502'}],\n",
       " 'text': 'HOGgles: Visualizing Object Detection Features∗\\nCarl Vondrick, Aditya Khosla, Tomasz Malisiewicz, Antonio Torralba\\nMassachusetts Institute of Technology\\n{vondrick,khosla,tomasz,torralba}@csail.mit.edu\\nAbstract\\nWe introduce algorithms to visualize feature spaces used\\nby object detectors. The tools in this paper allow a human\\nto put on ‘HOG goggles’ and perceive the visual world as\\na HOG based object detector sees it. We found that these\\nvisualizations allow us to analyze object detection systems\\nin new ways and gain new insight into the detector’s fail-\\nures. For example, when we visualize the features for high\\nscoring false alarms, we discovered that, although they are\\nclearly wrong in image space, they do look deceptively sim-\\nilar to true positives in feature space. This result suggests\\nthat many of these false alarms are caused by our choice of\\nfeature space, and indicates that creating a better learning\\nalgorithm or building bigger datasets is unlikely to correct\\nthese errors. By visualizing feature spaces, we can gain a\\nmore intuitive understanding of our detection systems.\\n1. Introduction\\nFigure 1 shows a high scoring detection from an ob-\\nject detector with HOG features and a linear SVM classiﬁer\\ntrained on PASCAL. Despite our ﬁeld’s incredible progress\\nin object recognition over the last decade, why do our de-\\ntectors still think that sea water looks like a car?\\nUnfortunately, computer vision researchers are often un-\\nable to explain the failures of object detection systems.\\nSome researchers blame the features, others the training set,\\nand even more the learning algorithm. Yet, if we wish to\\nbuild the next generation of object detectors, it seems cru-\\ncial to understand the failures of our current detectors.\\nIn this paper, we introduce a tool to explain some of the\\nfailures of object detection systems.1 We present algorithms\\nto visualize the feature spaces of object detectors. Since\\nfeatures are too high dimensional for humans to directly in-\\nspect, our visualization algorithms work by inverting fea-\\ntures back to natural images. We found that these inversions\\nprovide an intuitive and accurate visualization of the feature\\nspaces used by object detectors.\\n∗Previously: Inverting and Visualizing Features for Object Detection\\n1Code is available online at http://mit.edu/vondrick/ihog\\nFigure 1: An image from PASCAL and a high scoring car\\ndetection from DPM [8]. Why did the detector fail?\\nFigure 2: We show the crop for the false car detection from\\nFigure 1. On the right, we show our visualization of the\\nHOG features for the same patch. Our visualization reveals\\nthat this false alarm actually looks like a car in HOG space.\\nFigure 2 shows the output from our visualization on the\\nfeatures for the false car detection. This visualization re-\\nveals that, while there are clearly no cars in the original\\nimage, there is a car hiding in the HOG descriptor. HOG\\nfeatures see a slightly different visual world than what we\\nsee, and by visualizing this space, we can gain a more intu-\\nitive understanding of our object detectors.\\nFigure 3 inverts more top detections on PASCAL for\\na few categories. Can you guess which are false alarms?\\nTake a minute to study the ﬁgure since the next sentence\\nmight ruin the surprise. Although every visualization looks\\nlike a true positive, all of these detections are actually false\\nalarms. Consequently, even with a better learning algorithm\\nor more data, these false alarms will likely persist. In other\\nwords, the features are to blame.\\nThe principle contribution of this paper is the presenta-\\ntion of algorithms for visualizing features used in object de-\\ntection. To this end, we present four algorithms to invert\\n2013 IEEE International Conference on Computer Vision\\n1550-5499/13 $31.00 © 2013 IEEE\\nDOI 10.1109/ICCV.2013.8\\n1\\n2013 IEEE International Conference on Computer Vision\\n1550-5499/13 $31.00 © 2013 IEEE\\nDOI 10.1109/ICCV.2013.8\\n1\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 12,2024 at 09:00:20 UTC from IEEE Xplore.  Restrictions apply. \\nFigure 3: We visualize some high scoring detections from the deformable parts model [8] for person, chair, and car. Can you\\nguess which are false alarms? Take a minute to study this ﬁgure, then see Figure 16 for the corresponding RGB patches.\\nFigure 4: In this paper, we present algorithms to visualize\\nHOG features. Our visualizations are perceptually intuitive\\nfor humans to understand.\\nobject detection features to natural images. Although we fo-\\ncus on HOG features in this paper, our approach is general\\nand can be applied to other features as well. We evaluate\\nour inversions with both automatic benchmarks and a large\\nhuman study, and we found our visualizations are percep-\\ntually more accurate at representing the content of a HOG\\nfeature than existing methods; see Figure 4 for a compar-\\nison between our visualization and HOG glyphs. We then\\nuse our visualizations to inspect the behaviors of object de-\\ntection systems and analyze their features. Since we hope\\nour visualizations will be useful to other researchers, our\\nﬁnal contribution is a public feature visualization toolbox.\\n2. Related Work\\nOur visualization algorithms extend an actively growing\\nbody of work in feature inversion. Torralba and Oliva, in\\nearly work, described a simple iterative procedure to re-\\ncover images only given gist descriptors [17]. Weinzaepfel\\net al. [22] were the ﬁrst to reconstruct an image given its\\nkeypoint SIFT descriptors [13].\\nTheir approach obtains\\ncompelling reconstructions using a nearest neighbor based\\napproach on a massive database. d’Angelo et al. [4] then de-\\nveloped an algorithm to reconstruct images given only LBP\\nfeatures [2, 1]. Their method analytically solves for the in-\\nverse image and does not require a dataset.\\nWhile [22, 4, 17] do a good job at reconstructing im-\\nages from SIFT, LBP, and gist features, our visualization\\nalgorithms have several advantages. Firstly, while existing\\nmethods are tailored for speciﬁc features, our visualization\\nalgorithms we propose are feature independent. Since we\\ncast feature inversion as a machine learning problem, our\\nalgorithms can be used to visualize any feature. In this pa-\\nper, we focus on features for object detection, the most pop-\\nular of which is HOG. Secondly, our algorithms are fast:\\nour best algorithm can invert features in under a second on\\na desktop computer, enabling interactive visualization. Fi-\\nnally, to our knowledge, this paper is the ﬁrst to invert HOG.\\nOur visualizations enable analysis that complement a re-\\ncent line of papers that provide tools to diagnose object\\nrecognition systems, which we brieﬂy review here. Parikh\\nand Zitnick [18, 19] introduced a new paradigm for human\\ndebugging of object detectors, an idea that we adopt in our\\nexperiments. Hoiem et al. [10] performed a large study an-\\nalyzing the errors that object detectors make. Divvala et al.\\n[5] analyze part-based detectors to determine which com-\\nponents of object detection systems have the most impact\\non performance. Tatu et al. [20] explored the set of images\\nthat generate identical HOG descriptors. Liu and Wang [12]\\ndesigned algorithms to highlight which image regions con-\\ntribute the most to a classiﬁer’s conﬁdence. Zhu et al. [24]\\ntry to determine whether we have reached Bayes risk for\\nHOG. The tools in this paper enable an alternative mode to\\nanalyze object detectors through visualizations. By putting\\non ‘HOG glasses’ and visualizing the world according to\\nthe features, we are able to gain a better understanding of\\nthe failures and behaviors of our object detection systems.\\n3. Feature Visualization Algorithms\\nWe pose the feature visualization problem as one of fea-\\nture inversion, i.e. recovering the natural image that gen-\\nerated a feature vector.\\nLet x ∈RD be an image and\\ny = φ(x) be the corresponding HOG feature descriptor.\\nSince φ(·) is a many-to-one function, no analytic inverse\\nexists. Hence, we seek an image x that, when we compute\\n2\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 12,2024 at 09:00:20 UTC from IEEE Xplore.  Restrictions apply. \\nFigure 5: We found that averaging the images of top detec-\\ntions from an exemplar LDA detector provide one method\\nto invert HOG features.\\nHOG on it, closely matches the original descriptor y:\\nφ−1(y) = argmin\\nx∈RD ||φ(x) −y||2\\n2\\n(1)\\nOptimizing Eqn.1 is challenging. Although Eqn.1 is not\\nconvex, we tried gradient-descent strategies by numerically\\nevaluating the derivative in image space with Newton’s\\nmethod. Unfortunately, we observed poor results, likely be-\\ncause HOG is both highly sensitive to noise and Eqn.1 has\\nfrequent local minima.\\nIn the rest of this section, we present four algorithms for\\ninverting HOG features. Since, to our knowledge, no al-\\ngorithms to invert HOG have yet been developed, we ﬁrst\\ndescribe three simple baselines for HOG inversion. We then\\npresent our main inversion algorithm.\\n3.1. Baseline A: Exemplar LDA (ELDA)\\nConsider the top detections for the exemplar object de-\\ntector [9, 15] for a few images shown in Figure 5. Although\\nall top detections are false positives, notice that each detec-\\ntion captures some statistics about the query. Even though\\nthe detections are wrong, if we squint, we can see parts of\\nthe original object appear in each detection.\\nWe use this simple observation to produce our ﬁrst in-\\nversion baseline. Suppose we wish to invert HOG feature y.\\nWe ﬁrst train an exemplar LDA detector [9] for this query,\\nw = Σ−1(y−μ). We score w against every sliding window\\non a large database. The HOG inverse is then the average of\\nthe top K detections in RGB space: φ−1\\nA (y) =\\n1\\nK\\n\\x02K\\ni=1 zi\\nwhere zi is an image of a top detection.\\nThis method, although simple, produces surprisingly ac-\\ncurate reconstructions, even when the database does not\\ncontain the category of the HOG template. However, it is\\ncomputationally expensive since it requires running an ob-\\nject detector across a large database. We also point out that\\na similar nearest neighbor method is used in brain research\\nto visualize what a person might be seeing [16].\\n3.2. Baseline B: Ridge Regression\\nWe present a fast, parametric inversion baseline based\\noff ridge regression. Let X ∈RD be a random variable\\nrepresenting a gray scale image and Y ∈Rd be a random\\nvariable of its corresponding HOG point. We deﬁne these\\nrandom variables to be normally distributed on a D + d-\\nvariate Gaussian P(X, Y ) ∼N(μ, Σ) with parameters\\nμ = [ μX μY ] and Σ =\\n\\x03 ΣXX ΣXY\\nΣT\\nXY ΣY Y\\n\\x04\\n. In order to invert a\\nHOG feature y, we calculate the most likely image from the\\nconditional Gaussian distribution P(X|Y = y):\\nφ−1\\nB (y) = argmax\\nx∈RD\\nP(X = x|Y = y)\\n(2)\\nIt is well known that Gaussians have a closed form condi-\\ntional mode:\\nφ−1\\nB (y) = ΣXY Σ−1\\nY Y (y −μY ) + μX\\n(3)\\nUnder this inversion algorithm, any HOG point can be in-\\nverted by a single matrix multiplication, allowing for inver-\\nsion in under a second.\\nWe estimate μ and Σ on a large database. In practice, Σ\\nis not positive deﬁnite; we add a small uniform prior (i.e.,\\nˆ\\nΣ = Σ +λI) so Σ can be inverted. Since we wish to in-\\nvert any HOG point, we assume that P(X, Y ) is stationary\\n[9], allowing us to efﬁciently learn the covariance across\\nmassive datasets. We invert an arbitrary dimensional HOG\\npoint by marginalizing out unused dimensions.\\nWe found that ridge regression yields blurred inversions.\\nIntuitively, since HOG is invariant to shifts up to its bin size,\\nthere are many images that map to the same HOG point.\\nRidge regression is reporting the statistically most likely\\nimage, which is the average over all shifts. This causes\\nridge regression to only recover the low frequencies of the\\noriginal image.\\n3.3. Baseline C: Direct Optimization\\nWe now provide a baseline that attempts to ﬁnd im-\\nages that, when we compute HOG on it, sufﬁciently match\\nthe original descriptor. In order to do this efﬁciently, we\\nonly consider images that span a natural image basis. Let\\nU ∈RD×K be the natural image basis. We found using the\\nﬁrst K eigenvectors of ΣXX ∈RD×D worked well for this\\nbasis. Any image x ∈RD can be encoded by coefﬁcients\\nρ ∈RK in this basis: x = Uρ. We wish to minimize:\\nφ−1\\nC (y) = Uρ∗\\nwhere\\nρ∗= argmin\\nρ∈RK ||φ(Uρ) −y||2\\n2\\n(4)\\nEmpirically we found success optimizing Eqn.4 using coor-\\ndinate descent on ρ with random restarts. We use an over-\\ncomplete basis corresponding to sparse Gabor-like ﬁlters\\nfor U. We compute the eigenvectors of ΣXX across dif-\\nferent scales and translate smaller eigenvectors to form U.\\n3.4. Algorithm D: Paired Dictionary Learning\\nIn this section, we present our main inversion algorithm.\\nLet x ∈RD be an image and y ∈Rd be its HOG descriptor.\\n3\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 12,2024 at 09:00:20 UTC from IEEE Xplore.  Restrictions apply. \\nFigure 6: Inverting HOG using paired dictionary learning.\\nWe ﬁrst project the HOG vector on to a HOG basis. By\\njointly learning a coupled basis of HOG features and natural\\nimages, we then transfer the coefﬁcients to the image basis\\nto recover the natural image.\\nFigure 7: Some pairs of dictionaries for U and V . The left\\nof every pair is the gray scale dictionary element and the\\nright is the positive components elements in the HOG dic-\\ntionary. Notice the correlation between dictionaries.\\nSuppose we write x and y in terms of bases U ∈RD×K\\nand V ∈Rd×K respectively, but with shared coefﬁcients\\nα ∈RK:\\nx = Uα\\nand\\ny = V α\\n(5)\\nThe key observation is that inversion can be obtained by\\nﬁrst projecting the HOG features y onto the HOG basis V ,\\nthen projecting α into the natural image basis U:\\nφ−1\\nD (y) = Uα∗\\nwhere\\nα∗= argmin\\nα∈RK ||V α −y||2\\n2\\ns.t.\\n||α||1 ≤λ (6)\\nSee Figure 6 for a graphical representation of the paired dic-\\ntionaries. Since efﬁcient solvers for Eqn.6 exist [14, 11], we\\ncan invert features in under two seconds on a 4 core CPU.\\nPaired dictionaries require ﬁnding appropriate bases U\\nand V such that Eqn.5 holds. To do this, we solve a paired\\ndictionary learning problem, inspired by recent super reso-\\nlution sparse coding work [23, 21]:\\nargmin\\nU,V,α\\nN\\n\\x05\\ni=1\\n\\x06\\n||xi −Uαi||2\\n2 + ||φ(xi) −V αi||2\\n2\\n\\x07\\ns.t.\\n||αi||1 ≤λ ∀i, ||U||2\\n2 ≤γ1, ||V ||2\\n2 ≤γ2\\n(7)\\nAfter a few algebraic manipulations, the above objective\\nsimpliﬁes to a standard sparse coding and dictionary learn-\\nOriginal\\nELDA\\nRidge\\nDirect\\nPairDict\\nFigure 8: We show results for all four of our inversion al-\\ngorithms on held out image patches on similar dimensions\\ncommon for object detection. See supplemental for more.\\ning problem with concatenated dictionaries, which we op-\\ntimize using SPAMS [14]. Optimization typically took a\\nfew hours on medium sized problems. We estimate U and\\nV with a dictionary size K ≈103 and training samples\\nN ≈106 from a large database. See Figure 7 for a visual-\\nization of the learned dictionary pairs.\\n4. Evaluation of Visualizations\\nWe evaluate our inversion algorithms using both quali-\\ntative and quantitative measures. We use PASCAL VOC\\n2011 [6] as our dataset and we invert patches corresponding\\nto objects. Any algorithm that required training could only\\naccess the training set. During evaluation, only images from\\nthe validation set are examined. The database for exemplar\\nLDA excluded the category of the patch we were inverting\\nto reduce the potential effect of dataset biases.\\nWe show our inversions in Figure 8 for a few object cat-\\negories. Exemplar LDA and ridge regression tend to pro-\\n4\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 12,2024 at 09:00:20 UTC from IEEE Xplore.  Restrictions apply. \\nFigure 9: We show results where our paired dictionary al-\\ngorithm is trained to recover RGB images instead of only\\ngrayscale images. The right shows the original image and\\nthe left shows the inverse.\\nPairDict (seconds)\\nGreedy (days)\\nOriginal\\nFigure 10: Although our algorithms are good at inverting\\nHOG, they are not perfect, and struggle to reconstruct high\\nfrequency detail. See text for details.\\nOriginal x\\nx′ = φ−1 (φ(x))\\nx′′ = φ−1 (φ(x′))\\nFigure 11: We recursively compute HOG and invert it with a\\npaired dictionary. While there is some information loss, our\\nvisualizations still do a good job at accurately representing\\nHOG features. φ(·) is HOG, and φ−1(·) is the inverse.\\nduce blurred visualizations. Direct optimization recovers\\nhigh frequency details at the expense of extra noise. Paired\\ndictionary learning tends to produce the best visualization\\nfor HOG descriptors. By learning a dictionary over the vi-\\nsual world and the correlation between HOG and natural\\nimages, paired dictionary learning recovered high frequen-\\ncies without introducing signiﬁcant noise.\\nWe discovered that the paired dictionary is able to re-\\ncover color from HOG descriptors. Figure 9 shows the re-\\nsult of training a paired dictionary to estimate RGB images\\ninstead of grayscale images. While the paired dictionary\\nassigns arbitrary colors to man-made objects and in-door\\nscenes, it frequently colors natural objects correctly, such as\\ngrass or the sky, likely because those categories are strongly\\ncorrelated to HOG descriptors. We focus on grayscale visu-\\nalizations in this paper because we found those to be more\\nintuitive for humans to understand.\\nWhile our visualizations do a good job at representing\\nHOG features, they have some limitations. Figure 10 com-\\npares our best visualization (paired dictionary) against a\\ngreedy algorithm that draws triangles of random rotation,\\n40 × 40\\n20 × 20\\n10 × 10\\n5 × 5\\nFigure 12: Our inversion algorithms are sensitive to the\\nHOG template size. We show how performance degrades\\nas the template becomes smaller.\\nscale, position, and intensity, and only accepts the triangle\\nif it improves the reconstruction. If we allow the greedy al-\\ngorithm to execute for an extremely long time (a few days),\\nthe visualization better shows higher frequency detail. This\\nreveals that there exists a visualization better than paired\\ndictionary learning, although it may not be tractable. In a\\nrelated experiment, Figure 11 recursively computes HOG\\non the inverse and inverts it again. This recursion shows\\nthat there is some loss between iterations, although it is mi-\\nnor and appears to discard high frequency details. More-\\nover, Figure 12 indicates that our inversions are sensitive to\\nthe dimensionality of the HOG template. Despite these lim-\\nitations, our visualizations are, as we will now show, still\\nperceptually intuitive for humans to understand.\\nWe quantitatively evaluate our algorithms under two\\nbenchmarks. Firstly, we use an automatic inversion metric\\nthat measures how well our inversions reconstruct original\\nimages. Secondly, we conducted a large visualization chal-\\nlenge with human subjects on Amazon Mechanical Turk\\n(MTurk), which is designed to determine how well people\\ncan infer high level semantics from our visualizations.\\n4.1. Inversion Benchmark\\nWe consider the inversion performance of our algorithm:\\ngiven a HOG feature y, how well does our inverse φ−1(y)\\nreconstruct the original pixels x for each algorithm? Since\\nHOG is invariant up to a constant shift and scale, we score\\neach inversion against the original image with normalized\\ncross correlation. Our results are shown in Table 1. Overall,\\nexemplar LDA does the best at pixel level reconstruction.\\n4.2. Visualization Benchmark\\nWhile the inversion benchmark evaluates how well the\\ninversions reconstruct the original image, it does not cap-\\nture the high level content of the inverse: is the inverse of a\\nsheep still a sheep? To evaluate this, we conducted a study\\non MTurk. We sampled 2,000 windows corresponding to\\nobjects in PASCAL VOC 2011. We then showed partic-\\nipants an inversion from one of our algorithms and asked\\nusers to classify it into one of the 20 categories. Each win-\\ndow was shown to three different users. Users were required\\nto pass a training course and qualiﬁcation exam before par-\\nticipating in order to guarantee users understood the task.\\nUsers could optionally select that they were not conﬁdent in\\n5\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 12,2024 at 09:00:20 UTC from IEEE Xplore.  Restrictions apply. \\nCategory\\nELDA\\nRidge\\nDirect\\nPairDict\\nbicycle\\n0.452\\n0.577\\n0.513\\n0.561\\nbottle\\n0.697\\n0.683\\n0.660\\n0.671\\ncar\\n0.668\\n0.677\\n0.652\\n0.639\\ncat\\n0.749\\n0.712\\n0.687\\n0.705\\nchair\\n0.660\\n0.621\\n0.604\\n0.617\\ntable\\n0.656\\n0.617\\n0.582\\n0.614\\nmotorbike\\n0.573\\n0.617\\n0.549\\n0.592\\nperson\\n0.696\\n0.667\\n0.646\\n0.646\\nMean\\n0.671\\n0.656\\n0.620\\n0.637\\nTable 1: We evaluate the performance of our inversion al-\\ngorithm by comparing the inverse to the ground truth image\\nusing the mean normalized cross correlation. Higher is bet-\\nter; a score of 1 is perfect. See supplemental for full table.\\nCategory\\nELDA Ridge Direct PairDict Glyph Expert\\nbicycle\\n0.327 0.127 0.362\\n0.307\\n0.405 0.438\\nbottle\\n0.269 0.282 0.283\\n0.446\\n0.312 0.222\\ncar\\n0.397 0.457 0.617\\n0.585\\n0.359 0.389\\ncat\\n0.219 0.178 0.381\\n0.199\\n0.139 0.286\\nchair\\n0.099 0.239 0.223\\n0.386\\n0.119 0.167\\ntable\\n0.152 0.064 0.162\\n0.237\\n0.071 0.125\\nmotorbike 0.221 0.232 0.396\\n0.224\\n0.298 0.350\\nperson\\n0.458 0.546 0.502\\n0.676\\n0.301 0.375\\nMean\\n0.282 0.258 0.355\\n0.383\\n0.191 0.233\\nTable 2:\\nWe evaluate visualization performance across\\ntwenty PASCAL VOC categories by asking MTurk work-\\ners to classify our inversions. Numbers are percent classi-\\nﬁed correctly; higher is better. Chance is 0.05. Glyph refers\\nto the standard black-and-white HOG diagram popularized\\nby [3]. Paired dictionary learning provides the best visu-\\nalizations for humans. Expert refers to MIT PhD students\\nin computer vision performing the same visualization chal-\\nlenge with HOG glyphs. See supplemental for full table.\\ntheir answer. We also compared our algorithms against the\\nstandard black-and-white HOG glyph popularized by [3].\\nOur results in Table 2 show that paired dictionary learn-\\ning and direct optimization provide the best visualization\\nof HOG descriptors for humans. Ridge regression and ex-\\nemplar LDA performs better than the glyph, but they suf-\\nfer from blurred inversions. Human performance on the\\nHOG glyph is generally poor, and participants were even\\nthe slowest at completing that study. Interestingly, the glyph\\ndoes the best job at visualizing bicycles, likely due to their\\nunique circular gradients. Our results overall suggest that\\nvisualizing HOG with the glyph is misleading, and richer\\nvisualizations from our paired dictionary are useful for in-\\nterpreting HOG vectors.\\nOur experiments suggest that humans can predict the\\nperformance of object detectors by only looking at HOG\\nvisualizations. Human accuracy on inversions and state-of-\\nthe-art object detection AP scores from [7] are correlated\\n(a) Human Vision\\n(b) HOG Vision\\nFigure 13: HOG inversion reveals the world that object de-\\ntectors see. The left shows a man standing in a dark room.\\nIf we compute HOG on this image and invert it, the previ-\\nously dark scene behind the man emerges. Notice the wall\\nstructure, the lamp post, and the chair in the bottom right\\nhand corner.\\nwith a Spearman’s rank correlation coefﬁcient of 0.77.\\nWe also asked computer vision PhD students at MIT to\\nclassify HOG glyphs in order to compare MTurk workers\\nwith experts in HOG. Our results are summarized in the\\nlast column of Table 2. HOG experts performed slightly\\nbetter than non-experts on the glyph challenge, but experts\\non glyphs did not beat non-experts on other visualizations.\\nThis result suggests that our algorithms produce more intu-\\nitive visualizations even for object detection researchers.\\n5. Understanding Object Detectors\\nWe have so far presented four algorithms to visualize ob-\\nject detection features. We evaluated the visualizations with\\na large human study, and we found that paired dictionary\\nlearning provides the most intuitive visualization of HOG\\nfeatures. In this section, we will use this visualization to\\ninspect the behavior of object detection systems.\\n5.1. HOG Goggles\\nOur visualizations reveal that the world that features see\\nis slightly different from the world that the human eye per-\\nceives. Figure 13a shows a normal photograph of a man\\nstanding in a dark room, but Figure 13b shows how HOG\\nfeatures see the same man. Since HOG is invariant to illu-\\nmination changes and ampliﬁes gradients, the background\\nof the scene, normally invisible to the human eye, material-\\nizes in our visualization.\\nIn order to understand how this clutter affects object de-\\ntection, we visualized the features of some of the top false\\nalarms from the Felzenszwalb et al. object detection sys-\\ntem [8] when applied to the PASCAL VOC 2007 test set.\\n6\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 12,2024 at 09:00:20 UTC from IEEE Xplore.  Restrictions apply. \\nFigure 3 shows our visualizations of the features of the top\\nfalse alarms. Notice how the false alarms look very simi-\\nlar to true positives. While there are many different types\\nof detector errors, this result suggests that these particular\\nfailures are due to limitations of HOG, and consequently,\\neven if we develop better learning algorithms or use larger\\ndatasets, these will false alarms will likely persist.\\nFigure 16 shows the corresponding RGB image patches\\nfor the false positives discussed above. Notice how when\\nwe view these detections in image space, all of the false\\nalarms are difﬁcult to explain. Why do chair detectors ﬁre\\non buses, or people detectors on cherries? By visualizing\\nthe detections in feature space, we discovered that the learn-\\ning algorithm made reasonable failures since the features\\nare deceptively similar to true positives.\\n5.2. Human+HOG Detectors\\nAlthough HOG features are designed for machines, how\\nwell do humans see in HOG space? If we could quantify hu-\\nman vision on the HOG feature space, we could get insights\\ninto the performance of HOG with a perfect learning algo-\\nrithm (people). Inspired by Parikh and Zitnick’s methodol-\\nogy [18, 19], we conducted a large human study where we\\nhad Amazon Mechanical Turk workers act as sliding win-\\ndow HOG based object detectors.\\nWe built an online interface for humans to look at HOG\\nvisualizations of window patches at the same resolution as\\nDPM. We instructed workers to either classify a HOG vi-\\nsualization as a positive example or a negative example for\\na category. By averaging over multiple people (we used 25\\npeople per window), we obtain a real value score for a HOG\\npatch. To build our dataset, we sampled top detections from\\nDPM on the PASCAL VOC 2007 dataset for a few cate-\\ngories. Our dataset consisted of around 5, 000 windows per\\ncategory and around 20% were true positives.\\nFigure 14 shows precision recall curves for the Hu-\\nman+HOG based object detector. In most cases, human\\nsubjects classifying HOG visualizations were able to rank\\nsliding windows with either the same accuracy or better\\nthan DPM. Humans tied DPM for recognizing cars, sug-\\ngesting that performance may be saturated for car detection\\non HOG. Humans were slightly superior to DPM for chairs,\\nalthough performance might be nearing saturation soon.\\nThere appears to be the most potential for improvement for\\ndetecting cats with HOG. Subjects performed slightly worst\\nthan DPM for detecting people, but we believe this is the\\ncase because humans tend to be good at fabricating people\\nin abstract drawings.\\nWe then repeated the same experiment as above on chairs\\nexcept we instructed users to classify the original RGB\\npatch instead of the HOG visualization. As expected, hu-\\nmans achieved near perfect accuracy at detecting chairs\\nwith RGB sliding windows.\\nThe performance gap be-\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nRecall\\nPrecision\\nChair\\n \\n \\nHOG+Human AP = 0.63\\nRGB+Human AP = 0.96\\nHOG+DPM AP = 0.51\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nRecall\\nPrecision\\nCat\\n \\n \\nHOG+Human AP = 0.78\\nHOG+DPM AP = 0.58\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nRecall\\nPrecision\\nCar\\n \\n \\nHOG+Human AP = 0.83\\nHOG+DPM AP = 0.87\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nRecall\\nPrecision\\nPerson\\n \\n \\nHOG+Human AP = 0.69\\nHOG+DPM AP = 0.79\\nFigure 14: By instructing multiple human subjects to clas-\\nsify the visualizations, we show performance results with an\\nideal learning algorithm (i.e., humans) on the HOG feature\\nspace. Please see text for details.\\ntween the Human+HOG detector and Human+RGB detec-\\ntor demonstrates the amount of information that HOG fea-\\ntures discard.\\nOur experiments suggest that there is still some perfor-\\nmance left to be squeezed out of HOG. However, DPM\\nis likely operating very close to the performance limit of\\nHOG. Since humans are the ideal learning agent and they\\nstill had trouble detecting objects in HOG space, HOG may\\nbe too lossy of a descriptor for high performance object de-\\ntection. If we wish to signiﬁcantly advance the state-of-the-\\nart in recognition, we suspect focusing effort on building\\nbetter features that capture ﬁner details as well as higher\\nlevel information will lead to substantial performance im-\\nprovements in object detection.\\n5.3. Model Visualization\\nWe found our algorithms are also useful for visualizing\\nthe learned models of an object detector. Figure 15 visu-\\nalizes the root templates and the parts from [8] by invert-\\ning the positive components of the learned weights. These\\nvisualizations provide hints on which gradients the learn-\\ning found discriminative. Notice the detailed structure that\\nemerges from our visualization that is not apparent in the\\nHOG glyph. In most cases, one can recognize the category\\nof the detector by only looking at the visualizations.\\n6. Conclusion\\nWe believe visualizations can be a powerful tool for\\nunderstanding object detection systems and advancing re-\\nsearch in computer vision. To this end, this paper presented\\nand evaluated four algorithms to visualize object detection\\nfeatures. Since object detection researchers analyze HOG\\nglyphs everyday and nearly every recent object detection\\n7\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 12,2024 at 09:00:20 UTC from IEEE Xplore.  Restrictions apply. \\nFigure 15: We visualize a few deformable parts models trained with [8]. Notice the structure that emerges with our visual-\\nization. First row: car, person, bottle, bicycle, motorbike, potted plant. Second row: train, bus, horse, television, chair. For\\nthe right most visualizations, we also included the HOG glyph. Our visualizations tend to reveal more detail than the glyph.\\nFigure 16: We show the original RGB patches that correspond to the visualizations from Figure 3. We print the original\\npatches on a separate page to highlight how the inverses of false positives look like true positives. We recommend comparing\\nthis ﬁgure side-by-side with Figure 3.\\npaper includes HOG visualizations, we hope more intuitive\\nvisualizations will prove useful for the community.\\nAcknowledgments:\\nWe thank Hamed Pirsiavash, Joseph Lim, MIT\\nCSAIL Vision Group, and reviewers. Funding was provided by a NSF\\nGRFP to CV, a Facebook fellowship to AK, and a Google research award,\\nONR MURI N000141010933 and NSF Career Award No. 0747120 to AT.\\nReferences\\n[1] A. Alahi, R. Ortiz, and P. Vandergheynst. Freak: Fast retina keypoint.\\nIn CVPR, 2012. 2\\n[2] M. Calonder, V. Lepetit, C. Strecha, and P. Fua. Brief: Binary robust\\nindependent elementary features. ECCV, 2010. 2\\n[3] N. Dalal and B. Triggs. Histograms of oriented gradients for human\\ndetection. In CVPR, 2005. 6\\n[4] E. d’Angelo, A. Alahi, and P. Vandergheynst. Beyond bits: Recon-\\nstructing images from local binary descriptors. ICPR, 2012. 2\\n[5] S. Divvala, A. Efros, and M. Hebert. How important are deformable\\nparts in the deformable parts model? Technical Report, 2012. 2\\n[6] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zis-\\nserman. The pascal visual object classes challenge. IJCV, 2010. 4\\n[7] P. Felzenszwalb, R. Girshick, and D. McAllester. Cascade object\\ndetection with deformable part models. In CVPR, 2010. 6\\n[8] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Ob-\\nject detection with discriminatively trained part-based models. PAMI,\\n2010. 1, 2, 6, 7, 8\\n[9] B. Hariharan, J. Malik, and D. Ramanan. Discriminative decorrela-\\ntion for clustering and classiﬁcation. ECCV, 2012. 3\\n[10] D. Hoiem, Y. Chodpathumwan, and Q. Dai.\\nDiagnosing error in\\nobject detectors. ECCV, 2012. 2\\n[11] H. Lee, A. Battle, R. Raina, and A. Ng. Efﬁcient sparse coding algo-\\nrithms. NIPS, 2007. 4\\n[12] L. Liu and L. Wang. What has my classiﬁer learned? visualizing the\\nclassiﬁcation rules of bag-of-feature model by support region detec-\\ntion. In CVPR, 2012. 2\\n[13] D. Lowe. Object recognition from local scale-invariant features. In\\nICCV, 1999. 2\\n[14] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online dictionary learn-\\ning for sparse coding. In ICML, 2009. 4\\n[15] T. Malisiewicz, A. Gupta, and A. Efros. Ensemble of exemplar-svms\\nfor object detection and beyond. In ICCV, 2011. 3\\n[16] S. Nishimoto, A. Vu, T. Naselaris, Y. Benjamini, B. Yu, and J. Gal-\\nlant. Reconstructing visual experiences from brain activity evoked\\nby natural movies. Current Biology, 2011. 3\\n[17] A. Oliva and A. Torralba. Modeling the shape of the scene: A holistic\\nrepresentation of the spatial envelope. IJCV, 2001. 2\\n[18] D. Parikh and C. Zitnick. Human-debugging of machines. In NIPS\\nWCSSWC, 2011. 2, 7\\n[19] D. Parikh and C. L. Zitnick. The role of features, algorithms and data\\nin visual recognition. In CVPR, 2010. 2, 7\\n[20] A. Tatu, F. Lauze, M. Nielsen, and B. Kimia. Exploring the repre-\\nsentation capabilities of hog descriptors. In ICCV WIT, 2011. 2\\n[21] S. Wang, L. Zhang, Y. Liang, and Q. Pan.\\nSemi-coupled dictio-\\nnary learning with applications to image super-resolution and photo-\\nsketch synthesis. In CVPR, 2012. 4\\n[22] P. Weinzaepfel, H. J´\\negou, and P. P´\\nerez. Reconstructing an image\\nfrom its local descriptors. In CVPR, 2011. 2\\n[23] J. Yang, J. Wright, T. Huang, and Y. Ma. Image super-resolution via\\nsparse representation. Transactions on Image Processing, 2010. 4\\n[24] X. Zhu, C. Vondrick, D. Ramanan, and C. Fowlkes. Do we need\\nmore training data or better models for object detection?\\nBMVC,\\n2012. 2\\n8\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 12,2024 at 09:00:20 UTC from IEEE Xplore.  Restrictions apply. \\n'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[247]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f4072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e9c8c893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Unsupervised representation learning by discovering reliable image relations',\n",
       " 'doi': '10.1016/j.patcog.2019.107107',\n",
       " 'openalex_id': 'https://openalex.org/W3000050604',\n",
       " 'authors': ['Timo Milbich', 'Omair Ghori', 'Ferran Diego', 'Björn Ommer'],\n",
       " 'publication_date': '2020-01-17',\n",
       " 'publish_year': 2020,\n",
       " 'keywords': ['Artificial intelligence',\n",
       "  'Pairwise comparison',\n",
       "  'Pattern recognition (psychology)',\n",
       "  'Computer science',\n",
       "  'Inference',\n",
       "  'Representation (politics)',\n",
       "  'Divide and conquer algorithms',\n",
       "  'Unsupervised learning',\n",
       "  'Machine learning',\n",
       "  'Feature learning',\n",
       "  'Pascal (unit)',\n",
       "  'Transitive relation',\n",
       "  'Feature (linguistics)',\n",
       "  'Mathematics',\n",
       "  'Algorithm',\n",
       "  'Politics',\n",
       "  'Political science',\n",
       "  'Law',\n",
       "  'Linguistics',\n",
       "  'Philosophy',\n",
       "  'Combinatorics',\n",
       "  'Programming language'],\n",
       " 'abstract': 'Learning robust representations that allow to reliably establish relations between images is of paramount importance for virtually all of computer vision. Annotating the quadratic number of pairwise relations between training images is simply not feasible, while unsupervised inference is prone to noise, thus leaving the vast majority of these relations to be unreliable. To nevertheless find those relations which can be reliably utilized for learning, we follow a divide-and-conquer strategy: We find reliable similarities by extracting compact groups of images and reliable dissimilarities by partitioning these groups into subsets, converting the complicated overall problem into few reliable local subproblems. For each of the subsets we obtain a representation by learning a mapping to a target feature space so that their reliable relations are kept. Transitivity relations between the subsets are then exploited to consolidate the local solutions into a concerted global representation. While iterating between grouping, partitioning, and learning, we can successively use more and more reliable relations which, in turn, improves our image representation. In experiments, our approach shows state-of-the-art performance on unsupervised classification on ImageNet with 46.0% and competes favorably on different transfer learning tasks on PASCAL VOC.',\n",
       " 'global_link_openable': 'https://openalex.org/W3000050604',\n",
       " 'citation_count': 11,\n",
       " 'publication': [{'venue_name': 'Pattern Recognition',\n",
       "   'publisher': 'https://openalex.org/P4310320990',\n",
       "   'pdf_url': None},\n",
       "  {'venue_name': 'arXiv (Cornell University)',\n",
       "   'publisher': 'https://openalex.org/I205783295',\n",
       "   'pdf_url': 'http://arxiv.org/pdf/1911.07808'}],\n",
       " 'references': [{'openalex_id': 'https://openalex.org/W1154498256'},\n",
       "  {'openalex_id': 'https://openalex.org/W2099471712'},\n",
       "  {'openalex_id': 'https://openalex.org/W2108598243'},\n",
       "  {'openalex_id': 'https://openalex.org/W2118858186'},\n",
       "  {'openalex_id': 'https://openalex.org/W2144796873'},\n",
       "  {'openalex_id': 'https://openalex.org/W2151103935'},\n",
       "  {'openalex_id': 'https://openalex.org/W2163605009'},\n",
       "  {'openalex_id': 'https://openalex.org/W2326925005'},\n",
       "  {'openalex_id': 'https://openalex.org/W2395611524'},\n",
       "  {'openalex_id': 'https://openalex.org/W2412320034'},\n",
       "  {'openalex_id': 'https://openalex.org/W2511374729'},\n",
       "  {'openalex_id': 'https://openalex.org/W2519998487'},\n",
       "  {'openalex_id': 'https://openalex.org/W2555897561'},\n",
       "  {'openalex_id': 'https://openalex.org/W2558661413'},\n",
       "  {'openalex_id': 'https://openalex.org/W2559655401'},\n",
       "  {'openalex_id': 'https://openalex.org/W2595840341'},\n",
       "  {'openalex_id': 'https://openalex.org/W2607510315'},\n",
       "  {'openalex_id': 'https://openalex.org/W269471817'},\n",
       "  {'openalex_id': 'https://openalex.org/W2732398094'},\n",
       "  {'openalex_id': 'https://openalex.org/W2743157634'},\n",
       "  {'openalex_id': 'https://openalex.org/W2744910778'},\n",
       "  {'openalex_id': 'https://openalex.org/W2765550594'},\n",
       "  {'openalex_id': 'https://openalex.org/W2785325870'},\n",
       "  {'openalex_id': 'https://openalex.org/W2788970402'},\n",
       "  {'openalex_id': 'https://openalex.org/W2871888018'},\n",
       "  {'openalex_id': 'https://openalex.org/W2912042463'},\n",
       "  {'openalex_id': 'https://openalex.org/W2913668833'},\n",
       "  {'openalex_id': 'https://openalex.org/W2914393402'},\n",
       "  {'openalex_id': 'https://openalex.org/W2916022481'},\n",
       "  {'openalex_id': 'https://openalex.org/W2929226556'},\n",
       "  {'openalex_id': 'https://openalex.org/W2944195984'},\n",
       "  {'openalex_id': 'https://openalex.org/W2949891561'},\n",
       "  {'openalex_id': 'https://openalex.org/W2950180292'},\n",
       "  {'openalex_id': 'https://openalex.org/W2951004968'},\n",
       "  {'openalex_id': 'https://openalex.org/W2951548327'},\n",
       "  {'openalex_id': 'https://openalex.org/W2952863374'},\n",
       "  {'openalex_id': 'https://openalex.org/W2953259386'},\n",
       "  {'openalex_id': 'https://openalex.org/W2963192800'},\n",
       "  {'openalex_id': 'https://openalex.org/W2963350250'},\n",
       "  {'openalex_id': 'https://openalex.org/W2963765995'},\n",
       "  {'openalex_id': 'https://openalex.org/W3157598734'},\n",
       "  {'openalex_id': 'https://openalex.org/W3177525997'},\n",
       "  {'openalex_id': 'https://openalex.org/W4245551996'},\n",
       "  {'openalex_id': 'https://openalex.org/W4251247712'}],\n",
       " 'text': 'Pattern Recognition 102 (2020) 107107 \\nContents lists available at ScienceDirect \\nPattern Recognition \\njournal homepage: www.elsevier.com/locate/patcog \\nUnsupervised representation learning by discovering reliable image \\nrelations \\nTimo Milbich \\na \\n, \\n1 \\n, \\n∗, Omair Ghori \\nb \\n, \\n1 \\n, Ferran Diego \\nc \\n, Björn Ommer \\na \\na \\nHeidelberg Collaboratory for Image Processing and Interdisciplinary Center for Scientiﬁc Computing (HCI), Heidelberg University, Germany \\nb \\nRobert Bosch GmbH, Hildesheim, Germany \\nc \\nTelefonica R&D, Barcelona, Spain \\na r t i c l e \\ni n f o \\nArticle history: \\nReceived 13 May 2019 \\nRevised 16 September 2019 \\nAccepted 10 November 2019 \\nAvailable online 17 January 2020 \\nKeywords: \\nUnsupervised learning \\nVisual representation learning \\nUnsupervised image classiﬁcation \\nMining reliable relations \\nDivide-and-conquer \\na b s t r a c t \\nLearning robust representations that allow to reliably establish relations between images is of paramount \\nimportance for virtually all of computer vision. Annotating the quadratic number of pairwise relations \\nbetween training images is simply not feasible, while unsupervised inference is prone to noise, thus leav- \\ning the vast majority of these relations to be unreliable. To nevertheless ﬁnd those relations which can \\nbe reliably utilized for learning, we follow a divide-and-conquer strategy: We ﬁnd reliable similarities by \\nextracting compact groups of images and reliable dissimilarities by partitioning these groups into subsets, \\nconverting the complicated overall problem into few reliable local subproblems. For each of the subsets \\nwe obtain a representation by learning a mapping to a target feature space so that their reliable relations \\nare kept. Transitivity relations between the subsets are then exploited to consolidate the local solutions \\ninto a concerted global representation. While iterating between grouping, partitioning, and learning, we \\ncan successively use more and more reliable relations which, in turn, improves our image representa- \\ntion. In experiments, our approach shows state-of-the-art performance on unsupervised classiﬁcation on \\nImageNet with 46.0% and competes favorably on different transfer learning tasks on PASCAL VOC. \\n© 2020 Elsevier Ltd. All rights reserved. \\n1. Introduction \\nThe driving force of deep learning has been supervised train- \\ning using vast amounts of tediously labeled training samples, such \\nas object bounding boxes for visual recognition. Since easily ac- \\ncessible visual data is growing exponentially, manual labeling of \\ntraining samples constitutes a bottleneck to utilizing all this valu- \\nable data. Consequently, there has recently been great interest \\nin weakly supervised [1] \\n, self-supervised [2] \\n, and unsupervised \\n[3,4] approaches to representation learning. Fundamental computer \\nvision problems like classiﬁcation [5,6] \\n, object detection [7] and \\nimage segmentation [8] all directly depend on such learned rep- \\nresentations to ﬁnd similar objects or group related image areas. \\nTo learn a characteristic representation of images and the dis- \\ntances between them, different degrees of supervision can be con- \\nsidered: (i) Supervised learning using samples with class labels \\n[9] \\n, (ii) user feedback providing weakly supervised side informa- \\ntion in terms of pairwise constraints [10] \\n, (iii) problem speciﬁc sur- \\n∗Corresponding author. \\nE-mail \\naddresses: \\ntimo.milbich@iwr.uni-heidelberg.de \\n(T. \\nMilbich), \\nbjoern.ommer@iwr.uni-heidelberg.de (B. Ommer). \\n1 Authors contributed equally to this work. \\nrogate tasks such as colorization [11] \\n, permutations [2] \\n, or transi- \\ntivity [12] \\n, and (iv) unsupervised feature learning [13] \\n. Regardless \\nof the training signal, be it unaries such as class labels [14] \\n, bi- \\nnary similarity constraints between samples [10] or sample order- \\ning constraints [15] \\n, a dataset of N training samples gives rise to \\nN \\n2 pairwise relations, exploitable for learning our representation. \\nIn the absence of supervisory information, these relations need to \\nbe automatically inferred during training. However, the vast major- \\nity of these inferred pairwise relations turn out to be unreliable as \\ndiscussed in Section 3, Figs. 2 \\n, and 3 \\n. Despite the danger of dimin- \\nished performance due to learning from spurious relations, recent \\napproaches on unsupervised representation learning [13,16] \\n, nev- \\nertheless, do not question the reliability of these relations. Now, \\nassuming that only a small fraction of correct relations per sample \\ncan be identiﬁed reliably (i.e. we are left with at most O \\n( \\nN \\n) class \\nlabels or pairwise link constraints), how can we discover those \\nfew reliable relations, when no label or guiding side information \\nis available? \\nIn this work we propose a novel approach to visual representa- \\ntion learning that explicitly identiﬁes and leverages reliable image \\nrelations without the need for annotations, supervision, problem- \\nspeciﬁc surrogate tasks for self-supervision, or pre-training. By ex- \\ntracting compact groups of images we are able to harness reliable \\nhttps://doi.org/10.1016/j.patcog.2019.107107 \\n0031-3203/© 2020 Elsevier Ltd. All rights reserved. \\n'}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ef017bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'A new approach to cross-modal multimedia retrieval',\n",
       " 'doi': '10.1145/1873951.1873987',\n",
       " 'openalex_id': 'https://openalex.org/W2106277773',\n",
       " 'authors': ['Nikhil Rasiwasia',\n",
       "  'José Costa Pereira',\n",
       "  'Emanuele Coviello',\n",
       "  'Gabriel Doyle',\n",
       "  'Gert Lanckriet',\n",
       "  'Roger Lévy',\n",
       "  'Nuno Vasconcelos'],\n",
       " 'publication_date': '2010-10-25',\n",
       " 'publish_year': 2010,\n",
       " 'keywords': ['Computer science',\n",
       "  'Latent Dirichlet allocation',\n",
       "  'Abstraction',\n",
       "  'Information retrieval',\n",
       "  'Modal',\n",
       "  'Visual Word',\n",
       "  'Canonical correlation',\n",
       "  'Context (archaeology)',\n",
       "  'Image retrieval',\n",
       "  'Feature (linguistics)',\n",
       "  'Task (project management)',\n",
       "  'Artificial intelligence',\n",
       "  'Scale-invariant feature transform',\n",
       "  'Component (thermodynamics)',\n",
       "  'Explicit semantic analysis',\n",
       "  'Topic model',\n",
       "  'Natural language processing',\n",
       "  'Pattern recognition (psychology)',\n",
       "  'Feature extraction',\n",
       "  'Image (mathematics)',\n",
       "  'Semantic computing',\n",
       "  'Philosophy',\n",
       "  'Linguistics',\n",
       "  'Chemistry',\n",
       "  'Biology',\n",
       "  'Paleontology',\n",
       "  'Management',\n",
       "  'Epistemology',\n",
       "  'Polymer chemistry',\n",
       "  'Economics',\n",
       "  'Semantic Web',\n",
       "  'Thermodynamics',\n",
       "  'Semantic technology',\n",
       "  'Physics'],\n",
       " 'abstract': 'The problem of joint modeling the text and image components of multimedia documents is studied. The text component is represented as a sample from a hidden topic model, learned with latent Dirichlet allocation, and images are represented as bags of visual (SIFT) features. Two hypotheses are investigated: that 1) there is a benefit to explicitly modeling correlations between the two components, and 2) this modeling is more effective in feature spaces with higher levels of abstraction. Correlations between the two components are learned with canonical correlation analysis. Abstraction is achieved by representing text and images at a more general, semantic level. The two hypotheses are studied in the context of the task of cross-modal document retrieval. This includes retrieving the text that most closely matches a query image, or retrieving the images that most closely match a query text. It is shown that accounting for cross-modal correlations and semantic abstraction both improve retrieval accuracy. The cross-modal model is also shown to outperform state-of-the-art image retrieval systems on a unimodal retrieval task.',\n",
       " 'global_link_openable': 'https://openalex.org/W2106277773',\n",
       " 'citation_count': 1291,\n",
       " 'publication': [],\n",
       " 'references': [{'openalex_id': 'https://openalex.org/W1514337951'},\n",
       "  {'openalex_id': 'https://openalex.org/W1534408902'},\n",
       "  {'openalex_id': 'https://openalex.org/W1584938917'},\n",
       "  {'openalex_id': 'https://openalex.org/W1625255723'},\n",
       "  {'openalex_id': 'https://openalex.org/W1746620543'},\n",
       "  {'openalex_id': 'https://openalex.org/W1880262756'},\n",
       "  {'openalex_id': 'https://openalex.org/W1973948212'},\n",
       "  {'openalex_id': 'https://openalex.org/W1994203157'},\n",
       "  {'openalex_id': 'https://openalex.org/W2020842694'},\n",
       "  {'openalex_id': 'https://openalex.org/W2025341678'},\n",
       "  {'openalex_id': 'https://openalex.org/W2041658416'},\n",
       "  {'openalex_id': 'https://openalex.org/W2054543422'},\n",
       "  {'openalex_id': 'https://openalex.org/W2062903088'},\n",
       "  {'openalex_id': 'https://openalex.org/W2107463194'},\n",
       "  {'openalex_id': 'https://openalex.org/W2111993661'},\n",
       "  {'openalex_id': 'https://openalex.org/W2119288237'},\n",
       "  {'openalex_id': 'https://openalex.org/W2120750550'},\n",
       "  {'openalex_id': 'https://openalex.org/W2122695597'},\n",
       "  {'openalex_id': 'https://openalex.org/W2125238156'},\n",
       "  {'openalex_id': 'https://openalex.org/W2125263373'},\n",
       "  {'openalex_id': 'https://openalex.org/W2127411609'},\n",
       "  {'openalex_id': 'https://openalex.org/W2130660124'},\n",
       "  {'openalex_id': 'https://openalex.org/W2137918516'},\n",
       "  {'openalex_id': 'https://openalex.org/W2137993786'},\n",
       "  {'openalex_id': 'https://openalex.org/W2147152072'},\n",
       "  {'openalex_id': 'https://openalex.org/W2147952323'},\n",
       "  {'openalex_id': 'https://openalex.org/W2153675946'},\n",
       "  {'openalex_id': 'https://openalex.org/W2155723311'},\n",
       "  {'openalex_id': 'https://openalex.org/W2156336347'},\n",
       "  {'openalex_id': 'https://openalex.org/W2157487986'},\n",
       "  {'openalex_id': 'https://openalex.org/W2542843105'},\n",
       "  {'openalex_id': 'https://openalex.org/W2569586013'},\n",
       "  {'openalex_id': 'https://openalex.org/W3099514962'},\n",
       "  {'openalex_id': 'https://openalex.org/W4285719527'},\n",
       "  {'openalex_id': 'https://openalex.org/W4293747882'},\n",
       "  {'openalex_id': 'https://openalex.org/W4299689471'}],\n",
       " 'text': 'A New Approach to Cross-Modal Multimedia Retrieval\\nNikhil Rasiwasia1, Jose Costa Pereira1, Emanuele Coviello1, Gabriel Doyle2,\\nGert R.G. Lanckriet1, Roger Levy2, Nuno Vasconcelos1\\n1Dept. of Electrical and Computer Engineering,\\n2Dept. of Linguistics,\\nUniversity of California, San Diego\\n{nikux,josecp,ecoviell,gdoyle,rlevy}@ucsd.edu,{gert,nuno}@ece.ucsd.edu\\nABSTRACT\\nThe problem of joint modeling the text and image compo-\\nnents of multimedia documents is studied. The text compo-\\nnent is represented as a sample from a hidden topic model,\\nlearned with latent Dirichlet allocation, and images are rep-\\nresented as bags of visual (SIFT) features. Two hypotheses\\nare investigated: that 1) there is a beneﬁt to explicitly mod-\\neling correlations between the two components, and 2) this\\nmodeling is more eﬀective in feature spaces with higher lev-\\nels of abstraction. Correlations between the two components\\nare learned with canonical correlation analysis.\\nAbstrac-\\ntion is achieved by representing text and images at a more\\ngeneral, semantic level. The two hypotheses are studied in\\nthe context of the task of cross-modal document retrieval.\\nThis includes retrieving the text that most closely matches\\na query image, or retrieving the images that most closely\\nmatch a query text. It is shown that accounting for cross-\\nmodal correlations and semantic abstraction both improve\\nretrieval accuracy.\\nThe cross-modal model is also shown\\nto outperform state-of-the-art image retrieval systems on a\\nunimodal retrieval task.\\nCategories and Subject Descriptors\\nH.3.3 [Information Search and Retrieval]:\\nRetrieval\\nModels\\nGeneral Terms\\nAlgorithms, Design\\nKeywords\\nCross-media, cross-modal, retrieval, document processing,\\nimage and text, multimedia\\n1.\\nINTRODUCTION\\nOver the last decade there has been a massive explosion\\nof multimedia content on the web. This explosion has not\\nPermission to make digital or hard copies of all or part of this work for\\npersonal or classroom use is granted without fee provided that copies are\\nnot made or distributed for proﬁt or commercial advantage and that copies\\nbear this notice and the full citation on the ﬁrst page. To copy otherwise, to\\nrepublish, to post on servers or to redistribute to lists, requires prior speciﬁc\\npermission and/or a fee.\\nMM’10, October 25–29, 2010, Firenze, Italy.\\nCopyright 2010 ACM 978-1-60558-933-6/10/10 ...$10.00.\\nbeen matched by an equivalent increase in the sophistication\\nof multimedia content modeling technology. Today, the pre-\\nvailing tools for searching multimedia repositories are still\\ntext-based, e.g. search engines such as Google or Bing. To\\naddress this problem, the academic community has devoted\\nitself to the design of models that can account for multiple\\ncontent modalities.\\nIn computer vision, substantial eﬀort\\nhas been devoted to the problem of image annotation [4,\\n12, 9, 16, 19, 1]. The multimedia community has started a\\nnumber of large-scale research and evaluation eﬀorts, such\\nas TRECVID [26] and imageCLEF [21, 29], involving image\\nor video data complemented with annotations, close-caption\\ninformation, or speech recognition transcripts. Many tech-\\nniques have been proposed in this literature to automati-\\ncally augment images with captions or labels and to retrieve\\nand classify imagery augmented with information from these\\nmodalities [26, 21, 29, 28, 6, 31, 14, 8, 32].\\nAn important requirement for further progress in these\\nareas is the development of sophisticated joint models for\\nmultiple content modalities. Particularly important is the\\ndevelopment of models that support inference with respect\\nto content that is rich in multiple modalities. These include\\nmodels that do not simply consider the text accompanying\\nan image as a source of keywords for image classiﬁcation, but\\nmake use of the full structure of documents that pair a body\\nof text with a number of images or video-clips. The avail-\\nability of such documents, which include web-pages, news-\\npaper articles, and technical articles, has blossomed with\\nthe explosion of internet-based information. In this work,\\nwe consider the design of these multimedia models. We con-\\ncentrate on documents containing text and images, although\\nmany of the ideas would be applicable to other modalities.\\nWe start from the extensive literature available on text and\\nimage analysis, including the representation of documents\\nas bags of features (word histograms for text, SIFT his-\\ntograms [5] for images), and the use of topic models (such as\\nlatent Dirichlet allocation [3]) to extract low-dimensionality\\ngeneralizations from document corpora. We build on these\\nrepresentations to design a joint model for images and text.\\nThe performance of this model is evaluated on a cross-\\nmodal retrieval problem that includes two tasks: 1) the re-\\ntrieval of text documents in response to a query image, and\\n2) the retrieval of images in response to a query text. These\\ntasks are central to many applications of practical interest,\\nsuch as ﬁnding on the web the picture that best illustrates a\\ngiven text (e.g., to illustrate a page of a story book), ﬁnding\\nthe texts that best match a given picture (e.g., a set of va-\\ncation accounts about a given landmark), or searching using\\n251\\nMartin Luther King’s presence in Birmingham was not welcomed by all in the black community. A black attorney\\nwas quoted in ”Time” magazine as saying, ”The new administration should have been given a chance to confer\\nwith the various groups interested in change.” Black hotel owner A. G. Gaston stated, ”I regret the absence of\\ncontinued communication between white and Negro leadership in our city.” A white Jesuit priest assisting in\\ndesegregation negotiations attested, ”These demonstrations are poorly timed and misdirected.” Protest organizers\\nknew they would meet with violence from the Birmingham Police Department but chose a confrontational approach\\nto get the attention of the federal government.\\nReverend Wyatt Tee Walker, one of the SCLC founders and the\\nexecutive director from 19601964, planned the tactics of the direct action protests, speci↓cally targeting Bull\\nConnor’s tendency to react to demonstrations with violence. ”My theory was that if we mounted a strong nonviolent\\nmovement, the opposition would surely do something to attract the media, and in turn induce national sympathy\\nand attention to the everyday segregated circumstance of a person living in the Deep South,” Walker said.\\nHe\\nheaded the planning of what he called Project C, which stood for ”confrontation”.\\nAccording to this historians\\nIsserman and Kazin, the demands on the city authorities were straightforward:\\ndesegregate the economic life\\nof Birmingham its restaurants, hotels, public toilets, and the unwritten policy of hiring blacks for menial jobs\\nonly Maurice Isserman and Michael Kazin, America Divided: The Civil War of the 1960s, (Oxford, 2008), p.90.\\nOrganizers believed their phones were tapped, so to prevent their plans from being leaked and perhaps in\\'uencing\\nthe mayoral election, they used code words for demonstrations.\\nThe plan called for direct nonviolent action to\\nattract media attention to ”the biggest and baddest city of the South”.Hampton, p.\\n126.\\nIn preparation for\\nthe protests, Walker timed the walking distance from the Sixteenth Street Baptist Church, headquarters for the\\ncampaign, to the downtown area.\\nHe surveyed the segregated lunch counters of department stores, and listed\\nfederal buildings as secondary targets should police block the protesters’ entrance into primary targets such as\\nstores, libraries, and all-white churches.\\nFigure 1:\\nA section from Wikipedia article on the Birmingham campaign http://en.wikipedia.org/wiki/\\nBirmingham_campaign\\na combination of text and images. We only brieﬂy discuss\\nthese applications, concentrating in this study on the prob-\\nlem of model design. We use performance on the retrieval\\ntasks as an indirect measure of the model quality, under the\\nintuition that the best model should produce the highest\\nretrieval accuracies.\\nWith regards to model design, we investigate two hy-\\npotheses. The ﬁrst is that explicit modeling of correlations\\nbetween images and text is important. We propose mod-\\nels that explicitly account for cross-modal correlations us-\\ning canonical correlation analysis (CCA), and compare their\\nperformance to models where the two modalities are mod-\\neled independently.\\nThe second is that a useful role can\\nbe played by abstraction—deﬁned here as hierarchical infer-\\nence across layers of increasingly general semantics. Various\\nresults have shown that such representations improve per-\\nformance in multimedia tasks, e.g. the use of hierarchical\\ntopic models for text clustering [3] or hierarchical semantic\\nrepresentations for image retrieval [25]. The retrieval prob-\\nlems we consider here are amenable to the design of such\\nabstraction hierarchies:\\nfor example, features group into\\ndocuments, which themselves group into classes or topics,\\nwhich form corpora. Abstract representations are proposed\\nfor both vision and text, by modeling images and documents\\nas vectors of posterior probabilities with respect to a set of\\npre-deﬁned document classes, computed with logistic regres-\\nsion [10].\\nWe investigate the retrieval performance of various com-\\nbinations of image and text representations, which cover all\\npossibilities in terms of the two guiding hypotheses. Our\\nresults show that there is a beneﬁt to both abstraction and\\ncross-modal correlation modeling.\\nIn particular, our best\\nresults are obtained by a model that combines semantic ab-\\nstraction for both images and text with explicit modeling of\\ncross-correlations in a joint space. We also demonstrate the\\nbeneﬁts of joint text and image modeling by comparing the\\nperformance of a state-of-the-art image retrieval system to\\nan image retrieval system that accounts for the text which\\naccompanies each image, using the proposed joint model. It\\nis shown that the latter has substantially higher retrieval\\naccuracy.\\n2.\\nPREVIOUS WORK\\nThe problems of image and text retrieval have been the\\nsubject of extensive research in areas such as information\\nretrieval, computer vision, and multimedia [6, 27, 26, 21,\\n18]. In all these areas, the main emphasis of the retrieval\\nliterature has been on unimodal approaches, where query\\nand retrieved documents share a single modality [30, 6, 27].\\nMore recently, there has been some interest in image re-\\ntrieval systems that rely on collateral text metadata. The\\nlatter is usually provided by human annotators, typically\\nin the form of a few keywords, a small caption or a brief\\nimage description [21, 29, 26]. We do not refer to such sys-\\ntems as cross-modal since the retrieval operation itself is\\nunimodal, simply matching a text query to available text\\nmetadata.\\nHowever, because manual image labeling is a\\nlabor intensive process, these systems inspired research on\\nthe problem of automatic extraction of semantic descriptors\\nfrom images [4, 12, 9, 16, 19, 1]. This enabled a ﬁrst genera-\\ntion of truly cross-modal systems, which support text-based\\nqueries of image databases that do not contain text meta-\\ndata. However, these models are limited by their inherently\\nimpoverished textual information. Images are simply asso-\\nciated with keywords, or class labels, and there is no explicit\\nmodeling of free-form text. Two notable exceptions are the\\nworks of [2, 19], where separate “latent-space” models are\\nlearned for images and text, in a form suitable for cross-\\nmedia image annotation and retrieval.\\nIn parallel, advances have been reported in the area of\\nmulti-modal retrieval systems [21, 29, 26, 28, 6]. These are\\nextensions of the classic unimodal systems, where a single re-\\ntrieval model is applied to information from various modal-\\nities.\\nThis can be done by fusing features from diﬀerent\\nmodalities into a single vector [32, 22], or by learning dif-\\nferent models for diﬀerent modalities and fusing their out-\\nputs [31, 14]. An excellent overview of these approaches is\\ngiven in [8], which also presents an approach to further com-\\nbine unimodal and multimodal retrieval systems. However,\\nmost of these approaches require multimodal queries, queries\\ncomposed of both image and text features. An alternative\\nparadigm is to improve the models of one modality (say\\nimage) using information from other modalities (e.g., image\\ncaptions) [23, 20]. Lastly, it is possible to design multimodal\\nsystems by using text annotations to construct a semantic\\nspace [25]. Images are also represented on this space, and\\nsimilarity measured in this space can be used for retrieval.\\nThis representation uses text to perform image retrieval at\\na higher level of abstraction than simple image matching.\\nIn spite of the these developments in cross- and multi-\\nmodal retrieval, current systems rely on a limited textual\\nrepresentation, in the form of keywords, captions, or small\\n252\\nT\\nt S\\nI\\nS\\nCanonical\\nText\\x03Space\\nLike most of the UK, the Manchester area\\nmobilised extensively during World War II. For\\nexample, casting and machining expertise at\\nBeyer,\\nPeacock\\nand\\nCompany\\'s\\nlocomotive\\nworks in Gorton was switched to bomb making;\\nD\\nl\\n\\'\\nbb\\nk\\ni\\nCh\\nlt\\nM dl\\nk\\nImage\\x03Space\\nCanonical\\nCorrelation\\x03Analysis\\nDunlop\\'s rubber works in ChorltonͲonͲMedlock\\nmade barrage balloons;\\nMartin Luther King\\'s presence in Birmingham\\nwas\\nnot\\nwelcomed\\nby\\nall\\nin\\nthe\\nblack\\ncommunity. A black attorney was quoted in\\n\\'\\'Time\\'\\'\\nmagazine\\nas\\nsaying,\\n\"The\\nnew\\nadministration should have been given a chance\\nto confer with the various groups interested in\\nchange. …\\nIn 1920, at the age of 20, Coward starred in his\\nown play, the light comedy \\'\\'I\\'ll Leave It to You\\'\\'.\\nf\\nh\\nd\\nMultimodal\\x03Documents\\nAfter a tryout in Manchester, it opened in\\nLondon at the New Theatre (renamed the Noël\\nCoward Theatre in 2006), his first fullͲlength play\\nin the West End.Thaxter, John. British Theatre\\nGuide, 2009 Neville Cardus\\'s praise in \\'\\'The\\nManchester Guardian\\'\\'\\nSemantic\\x03\\nConcept\\x031\\nSemantic\\x03\\nConcept\\x031\\nImage\\x03\\nClassifiers\\nText\\x03\\nClassifiers\\nSemantic\\x03\\nConcept\\x03V\\nSemantic\\x03\\nConcept\\x03V\\nSemantic\\x03Space\\n(using\\x03original\\x03features)\\nSemantic\\x03\\nConcept\\x032\\nSemantic\\x03Space\\n(using\\x03CCA\\x03representation)\\nSemantic\\x03\\nConcept\\x032\\nLike most of the UK, the Manchester area\\nbili\\nd\\ni\\nl\\nd\\ni\\nld\\nClosest\\x03Text\\x03to\\x03the\\x03\\x03\\nQuery\\x03Image\\nIMAGE\\x03QUERY\\nSemantic\\x03\\nConcept\\x031\\nmobilised extensively during World War II. For\\nexample, casting and machining expertise at\\nBeyer,\\nPeacock\\nand\\nCompany\\'s\\nlocomotive\\nworks in Gorton was switched to bomb making;\\nDunlop\\'s rubber works in ChorltonͲonͲMedlock\\nmade barrage balloons;\\nLike most of the UK, the Manchester area\\nmobilised extensively during World War II. For\\nexample, casting and machining expertise at\\nBeyer,\\nPeacock\\nand\\nCompany\\'s\\nlocomotive\\nworks in Gorton was switched to bomb making;\\nDunlop\\'s rubber works in ChorltonͲonͲMedlock\\nmade barrage balloons;\\nLike most of the UK, the Manchester area\\nmobilised extensively during World War II. For\\nexample, casting and machining expertise at\\nBeyer,\\nPeacock\\nand\\nCompany\\'s\\nlocomotive\\nworks in Gorton was switched to bomb making;\\nDunlop\\'s rubber works in ChorltonͲonͲMedlock\\nmade barrage balloons;\\nLike most of the UK, the Manchester area\\nmobilised extensively during World War II. For\\nexample, casting and machining expertise at\\nBeyer,\\nPeacock\\nand\\nCompany\\'s\\nlocomotive\\nworks in Gorton was switched to bomb making;\\nDunlop\\'s rubber works in ChorltonͲonͲMedlock\\nmade barrage balloons;\\nLike most of the UK, the Manchester area\\nmobilised extensively during World War II. For\\nexample, casting and machining expertise at\\nBeyer,\\nPeacock\\nand\\nCompany\\'s\\nlocomotive\\nworks in Gorton was switched to bomb making;\\nDunlop\\'s rubber works in ChorltonͲonͲMedlock\\nSemantic\\x03\\nSemantic\\x03\\nConcept\\x03V\\nmade barrage balloons;\\nSemantic\\x03Space\\nConcept\\x032\\nQ\\nTEXT\\x03QUERY\\nLike most of the UK, the Manchester area\\nmobilised extensively during World War II. For\\nexample, casting and machining expertise at\\nBeyer\\nPeacock\\nand\\nCompany\\'s\\nlocomotive\\nClosest\\x03Image\\x03To\\x03\\nCCA\\x03Space\\nBeyer,\\nPeacock\\nand\\nCompany s\\nlocomotive\\nworks in Gorton was switched to bomb making;\\nDunlop\\'s rubber works in ChorltonͲonͲMedlock\\nmade barrage balloons;\\ng\\nthe\\x03Query\\x03Text\\nFigure 2: Schematic of the proposed cross-modal retrieval system.\\nLeft) Mapping of the text and image\\nfrom their respective natural spaces to a CCA space, Semantic Space and a Semantic space learned using\\nCCA representation. Right) Example of cross-modal query in the proposed system. At the top is shown\\nan example of retrieving text in response to an image query, where both text and images are mapped to\\na common semantic space. At the bottom is shown an example of retrieving images in response to a text\\nquery, where both text and images are mapped to a common subspace using CCA.\\ntext snippets. On the other hand, with the ongoing explo-\\nsion of Web-based multimedia content, it is now possible to\\ncollect large datasets of more richly annotated data.\\nEx-\\namples include news archives, or Wikipedia pages, where\\npictures are related to complete text articles, not just a few\\nkeywords. In these datasets, the connection between images\\nand text is much less direct than that provided by light anno-\\ntation, weakening the one-to-one mapping between textual\\nwords and class labels. For example, Fig 1 shows a section of\\nthe Wikipedia article on the “Birmingham campaign”, along\\nwith the associated image. Notice that, although related to\\nthe text, the image is clearly not representative of all the\\nwords in the article. A major long-term goal of modeling\\nin this domain should be to recover this latent relationship\\nbetween the text and image components of a document, and\\nput it to practical use in applications.\\n3.\\nCROSS-MODAL RETRIEVAL\\nIn this section, we present a novel approach to cross-modal\\nretrieval.\\nAlthough the fundamental ideas are applicable\\nto any combination of content modalities, we restrict the\\ndiscussion to documents containing images and text. The\\ngoal is to support truly cross-modal queries: to retrieve text\\narticles in response to query images and vice-versa.\\n3.1\\nThe problem\\nWe consider the problem of information retrieval from a\\ndatabase D = {D1, . . . , D|D|} of documents which contain\\ncomponents of images and text. In practice, these compo-\\nnents can be quite diverse: from documents where a single\\ntext is complemented by one or more images (e.g. a newspa-\\nper article) to documents containing multiple pictures and\\ntext sections (e.g.\\na Wikipedia page). For simplicity, we\\nconsider the case where each document consists of an image\\nand its accompanying text, i.e. Di = (Ii, Ti). Images and\\ntext are represented as vectors on feature spaces ℜI and\\nℜT , respectively. In this way, each document establishes a\\none-to-one mapping between points in the text and image\\nspaces. Given a text (image) query Tq ∈ℜT (Iq ∈ℜI), the\\ngoal of cross-modal retrieval is to return the closest match\\nin the image (text) space ℜI (ℜT ).\\n3.2\\nMatching images and text\\nWhenever the image and text spaces have a natural cor-\\nrespondence, cross-modal retrieval reduces to a classical re-\\ntrieval problem. Let\\nM : ℜT →ℜI\\nbe an invertible mapping between the two spaces. Given a\\nquery Tq in ℜT , it suﬃces to ﬁnd the nearest neighbor to\\nM(Tq) in ℜI. Similarly, given a query a Iq in ℜI, it suﬃces\\nto ﬁnd the nearest neighbor to M−1(Iq). In this case, the\\ndesign of a cross-modal retrieval system reduces to the de-\\nsign of an eﬀective similarity function for the determination\\nof nearest neighbors.\\nSince diﬀerent representations tend to be adopted for im-\\nages and text, there is typically no natural correspondence\\nbetween ℜI and ℜT . In this case, the mapping M has to\\nbe learned from examples. One possibility, that we pursue\\nin this work, is to map the two representations into two\\nintermediate spaces UI and UT that have a natural corre-\\nspondence. Let\\nMI : ℜI →UI\\n253\\nand\\nMT : ℜT →UT\\nbe invertible mappings from each of the image and text\\nspaces to two isomorphic spaces UI and UT such there is\\nan invertible mapping\\nM : UT →UI.\\nGiven a query Tq in ℜT the cross-modal retrieval operation\\nreduces to ﬁnding the nearest neighbor of M−1\\nI ◦M◦MT (Tq)\\nin ℜI. Similarly, given a query Iq in ℜI the goal is to ﬁnd\\nthe nearest neighbor of M−1\\nT\\n◦M−1 ◦MI(Iq) in ℜT .\\nUnder this approach, the main problem in the design of\\na cross-modal retrieval system is to learn the intermediate\\nspaces UI and UT . In this work, we consider three possi-\\nbilities that result from the combination of two main proce-\\ndures. In the ﬁrst case, two linear projections\\nPT : ℜT →UT\\nand\\nPI : ℜT →UI\\nare learned to map ℜI respectively ℜT onto correlated d-\\ndimensional subspaces UI and UT . This maintains the level\\nof abstraction of the representation. In the second case, a\\npair of non-linear transformations\\nLT : ℜT →ST\\nand\\nLI : ℜI →SI\\nare used to map the image and text spaces into a pair of\\nsemantic spaces ST , SI such that ST = SI. This increases\\nthe semantic abstraction of the representation.\\nWe next\\ndescribe the two approaches in greater detail.\\n3.3\\nCorrelation matching\\nLearning UT , UI requires some notion of an optimal corre-\\nspondence between the representations in the text and image\\nspaces. One possibility is to rely on subspace learning. This\\nis a learning framework that underlies some extremely pop-\\nular dimensionality reduction approaches in both the text\\nand vision literatures, such as latent semantic indexing [7] or\\nprincipal component analysis (PCA) [13]. Subspace learning\\nmethods are typically eﬃcient from a computational point\\nof view, and produce linear transformations which are easy\\nto conceptualize, implement, and deploy. In this case, a nat-\\nural measure of correspondence between the image and text\\nsubspaces is their correlation. This suggests canonical corre-\\nlation analysis (CCA) as a natural subspace representation\\nfor cross-modal modeling.\\nCanonical correlation analysis (CCA) [11] is a data anal-\\nysis and dimensionality reduction method similar to PCA.\\nWhile PCA deals with only one data space, CCA is a tech-\\nnique for joint dimensionality reduction across two (or more)\\nspaces that provide heterogeneous representations of the same\\ndata. The assumption is that the representations in these\\ntwo spaces contain some joint information that is reﬂected in\\ncorrelations between them. CCA learns d-dimensional sub-\\nspaces UI ⊂ℜI and UT ⊂ℜT that maximize the correlation\\nbetween the two modalities.\\nSimilar to principal components in PCA, CCA learns a\\nbasis of canonical components, i.e., directions wi ∈ℜI and\\nwt ∈ℜT along which the data is maximally correlated, i.e.,\\nmax\\nwi̸=0, wt̸=0\\nwT\\ni ΣIT wt\\np\\nwT\\ni ΣIIwi\\np\\nwT\\nt ΣT T wt\\n,\\n(1)\\nwhere ΣII and ΣT T represent the empirical covariance ma-\\ntrices for images {I1, . . . , I|D|} and text {T1, . . . , T|D|} re-\\nspectively, while ΣIT = ΣT\\nT I represents the cross-covariance\\nmatrix between them. The optimization of (1) can be solved\\nas a generalized eigenvalue problem (GEV) [24]\\n„\\n0\\nΣIT\\nΣT I\\n0\\n« „\\nwi\\nwt\\n«\\n= λ\\n„\\nΣII\\n0\\n0\\nΣT T\\n« „\\nwi\\nwt\\n«\\n.\\nThe generalized eigenvectors determine a set of uncorrelated\\ncanonical components, with the corresponding generalized\\neigenvalues indicating the explained correlation. GEVs can\\nbe solved as eﬃciently as regular eigenvalue problems [15].\\nThe ﬁrst d canonical components {wi,k}d\\nk=1 and {wt,k}d\\nk=1\\ndeﬁne a basis for projecting ℜI respectively ℜT on a sub-\\nspace UI respectively UT .\\nA natural invertible mapping\\nbetween these two projections follows from the correspon-\\ndence between the d-dimensional bases of maximal cross-\\nmodal correlation, as wi,1 ↔wt,1, ..., wi,d ↔wt,d.\\nFor\\ncross-modal retrieval, every text T ∈ℜT is mapped into\\nits projection pT = PT (T) onto {wt,k}d\\nk=1, and every im-\\nage into its projection pI = PI(I) onto {wi,k}d\\nk=1.\\nThis\\nresults in a compact, eﬃcient representation of both modal-\\nities.\\nSince the vectors pT and pI are coordinates in two\\nisometric d dimensional subspaces UT respectively UI, they\\ncan be thought as belonging to a single space U, obtained by\\noverlaying UI and UT . This leads to the schematic represen-\\ntation of Figure 2, where CCA deﬁnes a common subspace\\n(U) for cross-modal retrieval.\\nGiven an image query Iq with projection pI = P(Iq), the\\ntext T ∈ℜT that most closely matches it is that for which\\npT = P(T) minimizes\\nD(I, T ) = d(pI, pT )\\n(2)\\nfor some suitable measure of distance d(·, ·) in a d-dimensional\\nvector space.\\nSimilarly, given a query text Tq with pro-\\njection pT = P(Tq), the image I ∈ℜI that most closely\\nmatches it is that for which pI = PI(I) minimizes d(pI, pT ).\\nWe refer to this type of retrieval as correlation matching.\\n3.4\\nSemantic matching\\nAn alternative to subspace learning is to represent docu-\\nments at a higher level of abstraction, so that there is a natu-\\nral correspondence between the text and image spaces. This\\nis accomplished by augmenting the database D with a vo-\\ncabulary V = {v1, . . . , vK} of semantic concepts. These are\\nbroad document classes, such as “History” or “Biology”, into\\nwhich individual documents are grouped.\\nTwo mappings\\nLT and LI are then implemented with recourse to two clas-\\nsiﬁers of text and images, respectively. LT maps a text T ∈\\nℜT into a vector of posterior probabilities PV |T (vi|T ), i ∈\\n{1, . . . , K} with respect to each of the classes in V.\\nThe\\nspace ST of these posterior vectors is referred to as the se-\\nmantic space for text, and the probabilities PV |T (vi|T ) as\\nsemantic text features. Similarly, LI maps an image I into a\\nvector of semantic image features PV |I(vi|I), i ∈{1, . . . , K}\\nin a semantic image space SI.\\nOne possibility to compute posterior probability distribu-\\ntions is through multi-class logistic regression.\\nThis pro-\\nduces a linear classiﬁer with a probabilistic interpretation.\\n254\\nLogistic regression computes the posterior probability of class\\nj, by ﬁtting data to a logistic function,\\nPV |X(j|x; w) =\\n1\\nZ(x, w) exp (wT\\nj x)\\n(3)\\nwhere Z(x, w) = P\\nj exp (wT\\nj x) is a normalization constant,\\nV the class label, X the vector of features in the input space,\\nand w = {w1, . . . , wK}, with wj a vector of parameters for\\nclass j. A multi-class logistic regression is learned for the\\ntext and image modalities, by making X the image and text\\nrepresentation I ∈ℜI and T ∈ℜT respectively.\\nSemantic modeling has two advantages for cross-modal re-\\ntrieval. First, it provides a higher level of abstraction. While\\nstandard features in ℜT and ℜI are the result of unsuper-\\nvised learning, and frequently have no obvious interpreta-\\ntion (e.g. image features tend to be edges, edge orientations\\nor frequency bases), the features in SI and ST are seman-\\ntic concept probabilities (e.g. the probability that the im-\\nage belongs to the “History” or “Biology” document classes).\\nPrevious work has shown that this increased abstraction can\\nlead to substantially better generalization for tasks such as\\nimage retrieval [25].\\nSecond, the semantic spaces SI and\\nST are isomorphic: in both cases, images and text are rep-\\nresented as vectors of posterior probabilities with respect\\nto the same document classes.\\nHence, the spaces can be\\nthought as the same, i.e. ST = SI, leading to the schematic\\nrepresentation of Figure 2.\\nGiven a query image Iq, represented by a probability vec-\\ntor πI ∈SI, retrieval consists of ﬁnding the text T, repre-\\nsented by a probability vector πT ∈ST, that minimizes\\nD(I, T ) = d(πI, πT ),\\n(4)\\nfor some suitable measure of distance d between probability\\ndistributions. We refer to this type of retrieval as semantic\\nmatching.\\n3.5\\nSemantic correlation matching\\nIt is also possible to combine subspace and semantic mod-\\neling. In this case, logistic regression is performed within\\ntwo maximally correlated subspaces.\\nThe CCA modeling\\nof Section 3.3 is ﬁrst applied to learn the maximally corre-\\nlated subspaces UI ⊂ℜI and UT ⊂ℜT . Logistic regressors\\nLI and LT are then learned in each of these subspaces to\\nproduce the semantic spaces SI and ST , respectively. Re-\\ntrieval is ﬁnally based on the image-text distance D(I, T )\\nof (4), based on the semantic mappings πI = LI(PI(I))\\nand πT = LT (PT (T )) after projections onto UI and UT re-\\nspectively.\\nWe refer to this type of retrieval as semantic\\ncorrelation matching.\\n3.6\\nText and Image Representation\\nIn this work, the representation of text on ℜT is derived\\nfrom a latent Dirichlet allocation (LDA) model [3].\\nLDA\\nis a generative model for a text corpus, where the semantic\\ncontent or “gist” of a text is summarized as a mixture of\\ntopics. More precisely, a text is modeled as a multinomial\\ndistribution over K topics, each of which is in turn modeled\\nas a multinomial distribution over words. Each word in a\\ntext Di is generated by ﬁrst sampling a topic z from the\\ntext-speciﬁc topic distribution, and then sampling a word\\nfrom that topic’s multinomial. In ℜT text documents are\\nrepresented by their topic assignment probability distribu-\\ntions.\\nTable 1: Summary of the Wikipedia dataset.\\nCategory\\nTraining\\nQuery/\\nTotal\\nRetrieval\\ndocuments\\nArt & architecture\\n138\\n34\\n172\\nBiology\\n272\\n88\\n360\\nGeography & places\\n244\\n96\\n340\\nHistory\\n248\\n85\\n333\\nLiterature & theatre\\n202\\n65\\n267\\nMedia\\n178\\n58\\n236\\nMusic\\n186\\n51\\n237\\nRoyalty & nobility\\n144\\n41\\n185\\nSport & recreation\\n214\\n71\\n285\\nWarfare\\n347\\n104\\n451\\nIn ℜI, image representation is based on the popular scale\\ninvariant feature transformation (SIFT) [17]. A bag of SIFT\\ndescriptors is ﬁrst extracted from each image in the training\\nset (using the SIFT implementation of LEAR 1). A code-\\nbook, or dictionary, of visual words is then learned with\\nthe k-means clustering algorithm.\\nThe SIFT descriptors\\nextracted from each image are vector quantized with this\\ncodebook, and images are represented by the SIFT descrip-\\ntor histograms that result from this quantization [5].\\n4.\\nEXPERIMENTS\\nIn this section, we describe an extensive experimental eval-\\nuation of the proposed framework for cross-modal retrieval.\\n4.1\\nDataset\\nThe evaluation of a cross-modal retrieval system requires\\na document corpus with paired texts and images. To de-\\nsign such a corpus we relied on Wikipedia’s “featured arti-\\ncles.” This is a continually updated collection of 2700 ar-\\nticles that have been selected and reviewed by Wikipedia’s\\neditors, since 2009. The articles are accompanied by one or\\nmore pictures from the Wikimedia Commons, supplying a\\npairing of the desired kind. In addition, each featured article\\nis categorized by Wikipedia into one of 29 categories. These\\ncategory labels were assigned to both the text and image\\ncomponents of each article. Since some of the categories are\\nvery scarce, we considered only the 10 most populated ones.\\nEach article was split into sections, based on its section\\nheadings, and each image in the article assigned to the sec-\\ntion in which it was placed by the article author(s). This\\nproduced a set of short and focused articles, usually con-\\ntaining a single image. The dataset was ﬁnally pruned by\\nremoving the sections without any image. The ﬁnal corpus\\ncontains a total of 2866 documents. These are text - im-\\nage pairs, annotated with a label from the vocabulary of\\n10 semantic classes. A random split was used to produce a\\ntraining set of 2173 documents, and a test set of 693 docu-\\nments, as summarized in Table 1.\\n4.2\\nThe fundamental hypotheses\\nIn Section 3, we have introduced three approaches to cross-\\nmodal retrieval, which we denoted by correlation matching\\n(CM), semantic matching (SM), and semantic correlation\\n1https://lear.inrialpes.fr/people/dorko/downloads.html\\n255\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nArt & architecture\\nBiology\\nGeography & places\\nHistory\\nLiterature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\n \\n \\nRandom\\nCM\\nSM\\nSCM\\n(a) Text Query\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nArt & architecture\\nBiology\\nGeography & places\\nHistory\\nLiterature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\n \\n \\nRandom\\nCM\\nSM\\nSCM\\n(b) Image Query\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nArt & architecture\\nBiology\\nGeography & places\\nHistory\\nLiterature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\n \\n \\nRandom\\nCM\\nSM\\nSCM\\n% Training Data\\n(c) Average Performance\\nFigure 3: MAP performance for each category\\nTable 2: Taxonomy of the proposed approaches to\\ncross-modal retrieval.\\ncorrelation hypothesis\\nabstraction hypothesis\\nCM\\n√\\nSM\\n√\\nSCM\\n√\\n√\\nmatching (SCM). These approaches are illustrated in Fig-\\nure 2. They represent diﬀerent choices with respect to two\\nfundamental hypothesis in the modeling of joint images and\\ntext:\\n• H1 (correlation hypothesis): that explicit modelling\\nof correlations between the two modalities is important\\n• H2 (abstraction hypothesis): that the use of an ab-\\nstract representation for images and text is desirable.\\nTable 2 summarizes which hypotheses hold for each of the\\nthree approaches. The comparative evaluation of the per-\\nformance of the latter on cross-modal retrieval experiments\\nprovides indirect evidence for the importance of the former\\nto the joint modelling of images and text.\\nThe intuition\\nis that when important hypotheses are met, the resulting\\nmodels are more eﬀective, and cross-modal retrieval perfor-\\nmance improves.\\nTo evaluate the importance of the two\\nhypotheses, we conducted a number of experiments which\\nare discussed in the remainder of this section.\\n4.3\\nExperimental Protocol\\nThe training set of Table 1 was used to learn all mappings\\nof Section 3. The performance of CM, SM, and SCM, was\\nevaluated on the test set. Two tasks were considered: text\\nretrieval using an image query, and image retrieval using a\\nquery text. In the ﬁrst case, each image was used as a query,\\nproducing a ranking of all texts. In the second, the roles of\\nimages and text were reversed.\\nIn all cases, performance\\nwas measured with precision-recall (PR) curves and mean\\naverage precision (MAP). The MAP score is the average\\nprecision at the ranks where recall changes.\\nIt is widely\\nused in the image retrieval literature [25].\\nAn LDA text model of 10 topics, and a SIFT codebook\\nof 128 codewords were learned in an unsupervised manner,\\nfrom a large corpus of images and texts2. They were used to\\n2The performance of the system was not very sensitive to\\nthe choice of these parameters\\nTable 3: Diﬀerent Distance Metric (MAP Scores)\\nExperiment\\nDistance\\nImage\\nText\\nMetric\\nQuery\\nQuery\\nCM\\nNC\\n0.249\\n0.196\\nCM\\nL2\\n0.235\\n0.181\\nCM\\nL1\\n0.226\\n0.179\\nSM\\nNC\\n0.225\\n0.223\\nSM\\nL1\\n0.165\\n0.221\\nSM\\nKL\\n0.177\\n0.221\\nSCM\\nNC\\n0.277\\n0.226\\nSCM\\nKL\\n0.241\\n0.226\\nSCM\\nL1\\n0.228\\n0.226\\nTable 4: Retrieval Performance (MAP Scores)\\nExperiment\\nImage\\nText\\nAverage\\nQuery\\nQuery\\nRandom\\n0.118\\n0.118\\n0.118\\nCM\\n0.249\\n0.196\\n0.223\\nSM\\n0.225\\n0.223\\n0.224\\nSCM\\n0.277\\n0.226\\n0.252\\ncompute a topic or SIFT histogram for each text or image,\\nrespectively, i.e. the document representation in the spaces\\nℜT and ℜI. For CM, the projection these spaces onto max-\\nimally correlated subspaces was learned with a CCA of 9\\ncomponents.\\nFor SM and SCM, the semantic spaces ST\\nand SI produced by logistic regression were 10-dimensional\\nprobability simplexes.\\nSince, in these spaces, each point\\nrepresents a multinomial distribution over the Wikipedia\\ncategories, we refer these points as semantic multinomials\\n(SMN). Each image and text is thus represented as a SMN.\\nVarious distance functions were used in (2) and (4). Ta-\\nble 3 presents the MAP scores obtained with the L1 dis-\\ntance, normalized correlation (NC), Kullback-Leibler diver-\\ngence (KL), - for SM and SCM - and L2 distance, NC, KL\\n- for CM. Since, in all cases, NC has the best performance,\\nit was adopted in the remaining experiments.\\n4.4\\nCross-modal Retrieval\\nTable 4 summarizes the MAP scores obtained for the three\\ncross-modal retrieval approaches, as well the chance-level\\nperformance.\\nThe table includes scores for both text re-\\ntrieval from an image query, and image retrieval from a text\\n256\\nThe pre-collegiate medium of instruction in schools is predominantly Kannada, while English and Kannada are predominant\\nlanguages in private schools.\\nAdditionally, other media of instruction exist in Mangalore.\\nThe medium of instruction in\\neducational institutions after matriculation in colleges is English. Recently, a committee of experts constituted by the Tulu\\nSahitya Academy recommended the inclusion of Tulu (in Kannada script) as a medium of instruction in education.\\nSchools and colleges in Mangalore are either government-run or run by private trusts and individuals.\\nThe schools are\\na¡liated with either the Karnataka State Board, Indian Certi↓cate of Secondary Education (ICSE), or the Central Board\\nfor Secondary Education (CBSE) boards. After completing 10 years of schooling in secondary education, students enroll in\\nHigher Secondary School, specializing in one of the three streams ˆ\\na Arts, Commerce or Science. Since the 1980s, there have\\nbeen a large number of professional institutions established in a variety of ↓elds including engineering, medicine, dentistry,\\nbusiness management and hotel management.\\nThe earliest schools established in Mangalore were the Basel Evangelical\\nSchool (1838) and Milagres School (1848).\\nThe Kasturba Medical College established in 1953, was India’s ↓rst private\\nmedical college.\\nPopular educational institutions in the city are National Institute of Technology (Karnataka), KS Hegde\\nMedical Academy,Father Muller Medical College, St. Aloysius College, Canara College,Canara Engineering College, S.D.M.\\nCollege and St.\\nJoseph Engineering College.\\nThe Bibliophile’s Paradise, a hi-tech public library run by the Corporation\\nBank, is located at Mannagudda in Mangalore. Mangalore University was established on September 10, 1980. It caters to the\\nhigher educational needs of Dakshina Kannada, Udupi and Kodagu districts and is a National Assessment and Accreditation\\nCouncil (NAAC) accredited four-star level institution.\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nArt & architecture\\nBiology\\nGeography & places\\nHistory\\nLiterature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\nSemantic Concepts\\nProbabilities\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\nArt & architecture\\nBiology\\nHistory\\nLiterature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\nSemantic Concepts\\nProbabilities\\nGeography & places\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nArt & architecture\\nBiology\\nGeography & places\\nHistory\\nLiterature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\nSemantic Concepts\\nProbabilities\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nArt & architecture\\nBiology\\nGeography & places\\nHistory\\nLiterature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\nSemantic Concepts\\nProbabilities\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1\\nArt & architecture\\nBiology\\nGeography & places\\nHistory\\nLiterature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\nSemantic Concepts\\nProbabilities\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nArt & architecture\\nBiology\\nGeography & places\\nHistory\\nLiterature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\nSemantic Concepts\\nProbabilities\\nFrank J. Selke served as Chairman of the selection committee from 1960 until 1971, when he resigned because of the induction\\nof Harvey ”Busher” Jackson. Jackson, known for his o↑-ice lifestyle, had died in 1966 of liver failure. Selke would not condone\\nthe induction and even tried to block it because he considered Jackson a poor role model.”Honoured members: the Hockey\\nHall of Fame”, p. 91\\nOn March 30, 1993, it was announced that Gil Stein, who at the time was the president of the National Hockey League,\\nwould be inducted into the Hall of Fame.\\nThere were immediate allegations that he had engineered his election through\\nmanipulation of the hall’s board of directors.\\nDue to these allegations, NHL commissioner Gary Bettman hired two in-\\ndependent lawyers, Arnold Burns and Yves Fortier, to lead an investigation.\\nThey concluded that Stein had ”improperly\\nmanipulated the process” and ”created the false appearance and illusion” that his nomination was the idea of Bruce McNall.\\nThey concluded that Stein pressured McNall to nominate him and had refused to withdraw his nomination when asked to\\ndo so by Bettman.\\nThere was a dispute over McNall’s role and Stein was ”categorical in stating that the idea was Mr.\\nMcNall’s.” They recommended that Stein’s selection be overturned, but it was revealed Stein had decided to turn down the\\ninduction before their announcement.\\nIn 1989, Alan Eagleson, a long time executive director of the National Hockey League Players Association, was inducted as a\\nbuilder. He resigned nine years later from the Hall after pleading guilty to mail fraud and embezzling hundreds of thousands\\nof dollars from the NHL Players Association pension funds.”Honoured members:\\nthe Hockey Hall of Fame”, p.\\n167 His\\nresignation came six days before a vote was scheduled to determine if he should be expelled from the Hall. Originally, the\\nHall of Fame was not going to become involved in the issue, but was forced to act when dozens of inductees, including Bobby\\nOrr, Ted Lindsay and Brad Park, campaigned for Eagleson’s expulsion, even threatening to renounce their membership if\\nhe was not removed. He became the ↓rst member of a sports hall of fame in North America to resign.\\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\nArt & architecture\\nBiology\\nGeography & places\\nHistory\\nLiterature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\nSemantic Concepts\\nProbabilities\\n0\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\nArt & architecture\\nBiology\\nGeography & places\\nHistory\\nLiterature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\nSemantic Concepts\\nProbabilities\\n0\\n0.02\\n0.04\\n0.06\\n0.08\\n0.1\\n0.12\\n0.14\\n0.16\\n0.18\\n0.2\\nArt & architecture\\nBiology\\nGeography & places\\nHistory\\nLiterature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\nSemantic Concepts\\nProbabilities\\n0\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\nArt & architecture\\nBiology\\nGeography & places\\nHistory\\nLiterature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\nSemantic Concepts\\nProbabilities\\n0\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\n0.3\\n0.35\\nArt & architecture\\nBiology\\nGeography & places\\nHistory\\nLiterature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\nSemantic Concepts\\nProbabilities\\n0\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\n0.3\\n0.35\\nArt & architecture\\nBiology\\nGeography & places\\nHistory\\nLiterature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\nSemantic Concepts\\nProbabilities\\nFigure 4: Two examples of text queries and the top images retrieved by SCM.\\n257\\n0  \\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1  \\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nRecall\\nPrecision\\n \\n \\nRandom\\nCM\\nSM\\nSCM\\n(a) Text Query\\n0  \\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1  \\n0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nRecall\\nPrecision\\n \\n \\nRandom\\nCM\\nSM\\nSCM\\n(b) Image Query\\nFigure 5: Precision recall curves\\nquery, and their average.\\nThe table provides evidence in\\nsupport of the two fundamental hypotheses.\\nFirst, both\\nmodeling of correlations (CM) and abstraction (SM) lead to\\na non-trivial improvement over chance-level performance. In\\nboth cases, the average MAP score, ∼0.22, is close to dou-\\nble that of random retrieval (∼.12). Second, the combina-\\ntion of correlation modeling and abstraction (SCM) further\\nimproves the MAP score to 0.252. This suggests that the\\nhypothesis are complementary: not only there is a beneﬁt\\nto correlation modeling and abstraction, but the best perfor-\\nmance is achieved when the two are combined.\\nTable 4 also shows that the gain of SCM holds for both\\nforms of cross-modal retrieval, i.e. image and text query.\\nFigure 3 shows the MAP scores achieved per category by all\\napproaches. Notice that, for most categories, the average\\nMAP of SCM is at least competitive with those of CM and\\nSM. Figure 4 shows two examples of text queries and the top\\nimages retrieved by SCM. In each case, the query text and\\ngroundtruth image are shown at the top, together with the\\nsemantic multinomial computed from the query. The top\\nﬁve retrieved images are shown at the bottom, along with\\ntheir semantic multinomials. Note that SCM retrieves these\\nimages because they are perceived by the retrieval system\\nas belonging to the category of the query text (“Geography\\n& Places” at the top, “Sports” at the bottom). This can be\\nseen from the semantic multinomials, which SCM uses to\\nﬁnd the closest matches.\\nFurther analysis of the results is presented in Figure 5,\\nwhich shows the PR of all approaches for both image and\\ntext queries. It is clear that SM, CM, and SCM all con-\\nsistently improve on random retrieval at all levels of recall.\\nThese plots also provide some insight into the improvements\\ndue to the semantic representation.\\nComparing the PR\\ncurves of SCM and CM, shows that SCM attains higher\\nprecision at all levels of recall for text queries (left). How-\\never, for image queries (right), the improvement is only sub-\\nstantial at higher levels of recall. It appears that, for the\\nlatter, abstraction eﬀectively widens the scope of matches.\\nInitially, both methods tend to retrieve texts that are re-\\nlated speciﬁcally to the query image. However, whereas CM\\nfails to generalize this to the query category, SCM is able to\\nalso retrieve texts whose relation to the query is only cate-\\ngorical. This seems to indicate that abstraction is especially\\nimportant for exploratory tasks.\\nFigure 6 presents confusion matrices for illustrative pur-\\nposes.\\nThe queries are classiﬁed into the class of highest\\nMAP3 as computed by SCM. Rows refer to true categories,\\nand columns to category predictions. Text queries are con-\\nsidered on the left and image queries on the right. Note that\\nthe confusion matrix for the former is cleaner than that of\\nthe latter. This is likely due to the fact that category labels\\napply more directly to text than images. Consider the exam-\\nple article of Figure 1, where the text suggests a clear ﬁt to\\nthe “History” category, but the image of a church could also\\nﬁt into the “Art & Architecture” or “Geography & Places”\\ncategories. Note that for many of the misclassiﬁed queries,\\nthe misclassiﬁcation is reasonable.\\nThis is especially true\\nfor text queries. For instance, the non-visual arts of “Liter-\\nature”, “Media”, and “Music” tend to be confused with each\\nother, but not to unrelated categories, like “Sports”. Sim-\\nilarly, the fact that “History” and “Royalty” are commonly\\nmisclassiﬁed as “Warfare” is not surprising, since these three\\ncategories share similar words and imagery.\\n4.5\\nComparison with image retrieval systems\\nSince the comparison to chance-level retrieval is somewhat\\nunsatisfactory, the performance of SCM-based cross-modal\\nretrieval was also compared to familiar benchmarks on the\\nproblem of unimodal image retrieval (where both query and\\nretrieved documents are images). To perform this compar-\\nison, we note that a cross-modal retrieval system can be\\nadapted to perform unimodal retrieval in two ways. In the\\nﬁrst, a query image is complemented with a text article,\\nand the latter is used to rank the images in the retrieval set.\\nNote that the query image is not used, and the query text\\nserves as its proxy. In the second, images in the retrieval\\nset are complemented by text articles, which are ranked by\\nsimilarity to the query image. In this case, the images in\\nthe retrieval set are not used, and the text articles serves as\\na proxy for them.\\nTable 5 compares the retrieval performances of these two\\ncross-modal approaches with a number of state-of-art uni-\\nmodal image retrieval methods. These include the method\\nof [30], which represents images as distributions of SIFT fea-\\ntures, and that of [25], where the images are projected on\\na semantic space similar to that used in this work. In both\\ncases, images are retrieved by similarity of the associated\\nprobability distributions. Note that, as noted in [25], the\\n3Note that this is not the ideal choice for classiﬁcation, infact\\nthe MAP is computed over a ranking of the test set.\\n258\\nText vs Image\\nArt & architecture\\nBiology\\nGeography & places\\nHistory\\nLiterature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\nArt & architecture\\nBiology\\nography & places\\nHistory\\nerature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n(a) Text Query\\nImage vs Text\\nArt & architecture\\nBiology\\nGeography & places\\nHistory\\nLiterature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\nArt & architecture\\nBiology\\nography & places\\nHistory\\nerature & theatre\\nMedia\\nMusic\\nRoyalty & nobility\\nSport\\nWarfare \\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\n0.3\\n0.35\\n0.4\\n0.45\\n(b) Image Query\\nFigure 6: Category-level confusion matrices as computed by classifying query to the class of highest MAP.\\nTable 5: Content based image retrieval.\\nExperiment\\nModel\\nMAP Score\\nCCA + SMN\\nLog. Regression\\n0.277\\n(Proxy Text Ranking )\\nCCA + SMN\\nLog. Regression\\n0.226\\n(Proxy Text Query)\\nImage SMN [25]\\nGauss Mixture\\n0.161\\nImage SMN\\nLog. Regression\\n0.152\\nImage SIFT Features [30]\\nGauss Mixture\\n0.135\\nImage SIFT Features\\nHistogram\\n0.140\\nRandom\\n-\\n0.117\\nmore abstract semantic representation also improves perfor-\\nmance on the unimodal image matching task: using SMNs\\nover SIFT features improves the MAP score from 0.140 to\\n0.161. However, all unimodal retrieval approaches have very\\nlimited gains over chance-level performance. This illustrates\\nthe diﬃculty of image matching on the dataset used in this\\nwork, where there is a wide variety of images in each class.\\nThe two cross-modal approaches signiﬁcantly improve the\\nretrieval performance, achieving MAP scores of 0.226 and\\n0.277 for the proxy text query and proxy text ranking re-\\nspectively4. This indicates that there is a signiﬁcant beneﬁt\\nin approaching the classical image retrieval problem from\\na cross-modal point of view, whenever images are part of\\nlarger documents (e.g. web pages). Figure 7 presents some\\nexamples, for the case where the query image was used to\\nrank the text articles in the retrieval set. Shown in the ﬁg-\\nure is the query image and the images corresponding to the\\ntop retrieved text articles.\\n5.\\nREFERENCES\\n[1] K. Barnard, P. Duygulu, D. Forsyth, N. De Freitas,\\n4These numbers are slightly diﬀerent from those of the previ-\\nous section, as the retrieval set no longer includes the query\\nimage itself or the corresponding text article.\\nD. Blei, and M. Jordan. Matching words and pictures.\\nJMLR, 3:1107–1135, 2003.\\n[2] D. Blei and M. Jordan. Modeling annotated data. In\\nProceedings of the 26th annual international ACM\\nSIGIR conference, pages 127–134. ACM, 2003.\\n[3] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet\\nallocation. JMLR, 3:993–1022, 2003.\\n[4] G. Carneiro, A. Chan, P. Moreno, and N. Vasconcelos.\\nSupervised learning of semantic classes for image\\nannotation and retrieval. IEEE Transactions on\\nPattern Analysis and Machine Intelligence,\\n29(3):394–410, 2007.\\n[5] G. Csurka, C. Dance, L. Fan, J. Willamowski, and\\nC. Bray. Visual categorization with bags of keypoints.\\nIn Workshop on Statistical Learning in Computer\\nVision, ECCV, volume 1, page 22. Citeseer, 2004.\\n[6] R. Datta, D. Joshi, J. Li, and J. Wang. Image\\nretrieval: Ideas, inﬂuences, and trends of the new age.\\nACM Computing Surveys (CSUR), 40(2):1–60, 2008.\\n[7] S. Deerwester, S. Dumais, G. Furnas, T. Landauer,\\nand R. Harshman. Indexing by latent semantic\\nanalysis. Journal of the American society for\\ninformation science, 41(6):391–407, 1990.\\n[8] H. Escalante, C. H´\\nernadez, L. Sucar, and M. Montes.\\nLate fusion of heterogeneous methods for multimedia\\nimage retrieval. In Proceeding of the 1st ACM\\ninternational conference on Multimedia information\\nretrieval, pages 172–179. ACM, 2008.\\n[9] S. Feng, R. Manmatha, and V. Lavrenko. Multiple\\nbernoulli relevance models for image and video\\nannotation. In CVPR, volume 2, 2004.\\n[10] D. Hosmer and S. Lemeshow. Applied logistic\\nregression. Wiley-Interscience, 2000.\\n[11] H. Hotelling. Relations between two sets of variates.\\nBiometrika, 28:321–377, 1936.\\n[12] J. Jeon, V. Lavrenko, and R. Manmatha. Automatic\\nimage annotation and retrieval using cross-media\\nrelevance models. In Proceedings of the 26th annual\\ninternational ACM SIGIR conference, page 126.\\nACM, 2003.\\n259\\nQuery Image\\nImages corresponding to the top retrieved text.\\nFigure 7: Some examples of image queries (framed image on the far-left column is the query object) and the\\ncorresponding top retrieved images (as ranked by text similarity).\\n[13] I. Jolliﬀe. Principal component analysis. Springer\\nverlag, 2002.\\n[14] T. Kliegr, K. Chandramouli, J. Nemrava, V. Svatek,\\nand E. Izquierdo. Combining image captions and\\nvisual analysis for image concept classiﬁcation. In\\nProceedings of the 9th International Workshop on\\nMultimedia Data Mining at ACM SIGKDD 2008,\\npages 8–17. ACM New York, NY, USA, 2008.\\n[15] A. Laub. Matrix analysis for scientists and engineers.\\nSiam, 2005.\\n[16] V. Lavrenko, R. Manmatha, and J. Jeon. A model for\\nlearning the semantics of pictures. In NIPS, 2003.\\n[17] D. Lowe. Distinctive image features from\\nscale-invariant keypoints. International journal of\\ncomputer vision, 60(2):91–110, 2004.\\n[18] C. Meadow, B. Boyce, D. Kraft, and C. Barry. Text\\ninformation retrieval systems. Emerald Group Pub\\nLtd, 2007.\\n[19] F. Monay and D. Gatica-Perez. Modeling semantic\\naspects for cross-media image indexing. IEEE\\nTransactions on Pattern Analysis and Machine\\nIntelligence, 29(10):1802–1817, 2007.\\n[20] A. Nakagawa, A. Kutics, K. Tanaka, and\\nM. Nakajima. Combining words and object-based\\nvisual features in image retrieval. In Proceedings 12th\\nInternational Conference on Image Analysis and\\nProcessing, pages 354–359, 2003.\\n[21] M. Paramita, M. Sanderson, and P. Clough. Diversity\\nin photo retrieval: overview of the ImageCLEFPhoto\\ntask 2009. CLEF working notes, 2009.\\n[22] T. Pham, N. Maillot, J. Lim, and J. Chevallet. Latent\\nsemantic fusion model for image retrieval and\\nannotation. In Proceedings of the sixteenth ACM\\nconference on Conference on information and\\nknowledge management, pages 439–444. ACM, 2007.\\n[23] A. Quattoni, M. Collins, T. Darrell, and C. MIT.\\nLearning visual representations using images with\\ncaptions. In IEEE Conference on Computer Vision\\nand Pattern Recognition, pages 1–8, 2007.\\n[24] J. Ramsay and B. Silverman. Functional Data\\nAnalysis. Springer, 1997.\\n[25] N. Rasiwasia, P. Moreno, and N. Vasconcelos.\\nBridging the gap: Query by semantic example. IEEE\\nTransactions on Multimedia, 9(5):923–938, 2007.\\n[26] A. F. Smeaton, P. Over, and W. Kraaij. Evaluation\\ncampaigns and trecvid. In MIR ’06: Proceedings of the\\n8th ACM International Workshop on Multimedia\\nInformation Retrieval, pages 321–330, New York, NY,\\nUSA, 2006. ACM Press.\\n[27] A. Smeulders, M. Worring, S. Santini, A. Gupta, and\\nR. Jain. Content-based image retrieval at the end of\\nthe early years. IEEE Transactions on pattern analysis\\nand machine intelligence, 22(12):1349–1380, 2000.\\n[28] C. Snoek and M. Worring. Multimodal video indexing:\\nA review of the state-of-the-art. Multimedia Tools and\\nApplications, 25(1):5–35, 2005.\\n[29] T. Tsikrika and J. Kludas. Overview of the\\nwikipediaMM task at ImageCLEF 2009. In Working\\nNotes for the CLEF 2009 Workshop, 2009.\\n[30] N. Vasconcelos. Minimum probability of error image\\nretrieval. IEEE Transactions on Signal Processing,\\n52(8):2322–2336, 2004.\\n[31] G. Wang, D. Hoiem, and D. Forsyth. Building text\\nfeatures for object image classiﬁcation. In Proceedings\\nof 19th international conference on pattern\\nrecognition, 2009.\\n[32] T. Westerveld. Probabilistic multimedia retrieval. In\\nProceedings of the 25th annual international ACM\\nSIGIR conference, page 438. ACM, 2002.\\n260\\n'}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[797]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550ebce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d630c581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2549\n",
      "['10.48550/arxiv.1706.03762', '10.1109/cvpr.2016.90', '10.48550/arxiv.1406.2661', '10.48550/arxiv.1412.6980', '10.48550/arxiv.1804.02767', '10.48550/arxiv.1611.01578', '10.48550/arxiv.1803.03635', '10.1016/j.patcog.2019.107107', '10.1016/j.ijforecast.2016.01.001', '10.1145/3236009', '10.1145/3285029', '10.1109/cvpr.2017.690', '10.48550/arxiv.1905.11946', '10.48550/arxiv.1906.08237', '10.48550/arxiv.1907.11692', '10.48550/arxiv.2005.14165', '10.48550/arxiv.2103.14030', '10.48550/arxiv.2102.12092', '10.1038/s41586-021-03819-2', '10.48550/arxiv.1806.07366', '10.1109/cvpr.2019.00453', '10.1109/cvpr.2018.00727', '10.1109/cvpr.2017.424', '10.1109/cvpr.2019.00020', '10.1109/iccv.2015.169', '10.1109/iccv.2015.123', '10.1007/978-3-642-35289-8_14', '10.48550/arxiv.1502.03167', '10.1007/978-3-319-10602-1_48', '10.1109/cvpr.2015.7298965', '10.48550/arxiv.1207.0580', '10.1109/iccv.2015.135', '10.1109/cvpr.2015.7299173', '10.1109/tpami.2011.235', '10.1145/1141911.1142005', '10.1016/0893-6080(95)90002-0', '10.1007/s11263-009-0275-4', '10.1162/neco.1997.9.8.1735', '10.1145/1873951.1874249', '10.1109/cvpr.2015.7298594', '10.1109/cvpr.2014.81', '10.1109/72.279181', '10.1007/s11263-015-0816-y', '10.1109/tpami.2010.57', '10.48550/arxiv.1312.6120', '10.3929/ethz-a-004263473', '10.1109/cvpr.2007.383266', '10.1162/neco.1989.1.4.541', '10.1145/2647868.2654889', '10.1109/34.56188', '10.48550/arxiv.1402.1869', '10.48550/arxiv.1409.5185', '10.1007/978-3-319-10578-9_23', '10.48550/arxiv.1408.5093', '10.48550/arxiv.1504.06066', '10.48550/arxiv.1506.01497', '10.48550/arxiv.1409.1556', '10.1007/978-3-642-42054-2_55', '10.48550/arxiv.1412.6550', '10.1007/3-540-49430-8_11', '10.1145/1179352.1142005', '10.1007/3-540-49430-8_2', '10.1007/978-1-4757-3121-7', '10.48550/arxiv.1409.2329', '10.48550/arxiv.1312.6026', '10.1109/tsmc.1978.4310035', '10.1145/321992.322002', '10.1109/5.726791', '10.1007/bf00992696', '10.1162/artl.2009.15.2.15202', '10.1109/iccv.1999.790410', '10.1145/1068009.1068315', '10.1109/msp.2012.2205597', '10.1109/cvpr.2005.177', '10.48550/arxiv.1511.06732', '10.1109/cvpr.2016.90', '10.48550/arxiv.1512.05287', '10.48550/arxiv.1601.01705', '10.48550/arxiv.1605.07146', '10.48550/arxiv.1605.07648', '10.48550/arxiv.1608.06993', '10.1109/iccv.2009.5459469', '10.48550/arxiv.1611.01462', '10.48550/arxiv.1603.05027', '10.48550/arxiv.1603.09382', '10.48550/arxiv.1412.6980', '10.1007/978-1-4615-5529-2', '10.48550/arxiv.1503.02531', '10.48550/arxiv.1312.6184', '10.1109/icnn.1993.298572', '10.14288/1.0165555', '10.48550/arxiv.1607.03250', '10.48550/arxiv.1607.05423', '10.1109/cvprw.2017.61', '10.1109/cvpr.2017.643', '10.48550/arxiv.1705.08665', '10.48550/arxiv.1611.06440', '10.48550/arxiv.1810.05270', '10.48550/arxiv.1701.05369', '10.48550/arxiv.1802.05296', '10.1109/iccv.2017.155', '10.48550/arxiv.1705.07565', '10.48550/arxiv.1804.08838', '10.48550/arxiv.1712.01312', '10.1109/sp.2017.49', '10.48550/arxiv.1608.04493', '10.48550/arxiv.1506.02142', '10.1109/sp.2016.41', '10.1109/iccv.2017.541', '10.1016/j.patcog.2015.06.013', '10.1109/cvpr.2009.5206848', '10.1109/tpami.2015.2496141', '10.1023/b:visi.0000029664.99615.94', '10.1007/978-3-319-46487-9_40', '10.1109/tpami.2016.2572683', '10.1016/j.patcog.2016.08.006', '10.1109/cvpr.2016.559', '10.1109/cvpr.2017.76', '10.48550/arxiv.1704.05310', '10.1016/j.patcog.2018.07.003', '10.48550/arxiv.1708.02901', '10.1016/j.patcog.2017.10.022', '10.48550/arxiv.1803.07728', '10.1016/j.patcog.2018.01.036', '10.1016/j.patcog.2018.07.005', '10.1016/j.patcog.2019.01.037', '10.1016/j.patcog.2019.02.024', '10.1016/j.patcog.2019.03.022', '10.1016/j.patcog.2019.05.012', '10.48550/arxiv.1505.00687', '10.48550/arxiv.1511.06856', '10.1109/iccv.2017.309', '10.1016/j.patcog.2016.06.001', '10.1016/j.csda.2015.11.016', '10.1016/j.eneco.2014.10.012', '10.1109/tsg.2013.2274373', '10.1016/j.ijforecast.2014.07.001', '10.1109/tpwrs.2011.2162082', '10.1016/j.ijforecast.2013.07.001', '10.1111/j.2517-6161.1996.tb02080.x', '10.1016/j.ijforecast.2016.02.001', '10.1016/j.ijforecast.2015.09.006', '10.1109/tsg.2015.2447007', '10.1007/3-540-56602-3_124', '10.1007/978-3-642-04747-3_8', '10.1371/journal.pone.0130140', '10.1109/cvpr.2015.7299155', '10.1109/cvpr.2015.7298640', '10.1109/cvpr.2011.5995616', '10.1109/iccv.2013.8', '10.1016/j.dss.2011.01.013', '10.1007/s11063-011-9207-8', '10.1145/2783258.2788613', '10.1145/2463372.2463382', '10.1109/cvpr.2014.127', '10.1016/j.neuroimage.2013.10.067', '10.1016/j.dss.2010.12.003', '10.1145/1401890.1401959', '10.1145/2594473.2594475', '10.1093/bioinformatics/btn170', '10.1145/775047.775113', '10.1145/2487575.2487579', '10.1136/bmj.296.6623.657', '10.1016/j.eswa.2010.08.023', '10.1145/2783258.2783407', '10.4088/jcp.12m08338', '10.1016/0950-7051(96)81920-4', '10.1007/s10618-014-0368-8', '10.1145/1014052.1014122', '10.1016/s0031-3203(98)00181-2', '10.1007/978-3-642-04174-7_45', '10.1145/1081870.1081878', '10.1016/s0304-3800(02)00064-9', '10.1109/cidm.2009.4938655', '10.1093/bioinformatics/bti1102', '10.1016/s0020-7373(87)80053-6', '10.1109/titb.2006.880553', '10.1145/2339530.2339556', '10.1016/j.ins.2012.10.039', '10.1017/s0269888913000039', '10.1186/s40064-015-1481-x', '10.1007/s11263-016-0911-8', '10.1016/j.patcog.2016.11.008', '10.1007/978-1-4899-7637-6_10', '10.1109/tnnls.2016.2599820', '10.1145/2939672.2939778', '10.1109/cvpr.2016.319', '10.1145/2939672.2939874', '10.1145/2858036.2858529', '10.1109/sp.2016.42', '10.1007/978-3-319-47175-4_20', '10.1109/tcds.2016.2628365', '10.1109/mlsp.2016.7738872', '10.1109/icdm.2016.0171', '10.1109/mis.2017.1', '10.48550/arxiv.1702.08608', '10.1007/978-3-319-54024-5', '10.1145/3077257.3077271', '10.1145/3097983.3098066', '10.1007/978-3-540-73078-1_67', '10.1109/icdm.2016.0011', '10.1016/j.jneumeth.2016.10.008', '10.1145/3097983.3098039', '10.1016/j.ejor.2006.04.051', '10.1093/idpl/ipx005', '10.1145/2740908.2742726', '10.1145/2684822.2697044', '10.1016/0893-6080(91)90009-t', '10.1145/1401890.1401944', '10.1145/2556270', '10.1145/2766462.2767755', '10.1145/2806416.2806527', '10.1145/1864708.1864770', '10.1145/2647868.2654940', '10.1145/1721654.1721677', '10.1145/2792838.2796546', '10.1145/2623372', '10.1145/1273496.1273596', '10.1109/icdm.2008.22', '10.1145/1772690.1772758', '10.1145/2736277.2741667', '10.1145/1390156.1390177', '10.1186/s40537-014-0007-7', '10.1145/2505515.2505665', '10.1016/0893-6080(89)90020-8', '10.48550/arxiv.1205.2618', '10.1038/nature14236', '10.1145/2783258.2783273', '10.1109/tkde.2005.99', '10.48550/arxiv.1511.06443', '10.1007/978-1-4899-7637-6_1', '10.1145/2843948', '10.1109/bigdata.2015.7363830', '10.3115/v1/d14-1002', '10.1145/2835776.2835837', '10.1109/icdm.2010.127', '10.1016/j.neucom.2015.10.134', '10.1145/2837024', '10.1145/2872427.2883006', '10.1016/j.knosys.2016.04.020', '10.1109/tnnls.2016.2514368', '10.1007/978-3-319-26535-3_69', '10.48550/arxiv.1605.09477', '10.1109/icde.2016.7498326', '10.1145/2911451.2914734', '10.1145/2988450.2988452', '10.1016/j.knosys.2016.06.028', '10.1145/2911451.2914726', '10.1145/2988450.2988454', '10.1007/978-3-319-31750-2_44', '10.1145/2983323.2983874', '10.1145/2939672.2939673', '10.1145/2959100.2959167', '10.1145/2959100.2959190', '10.1145/2959100.2959162', '10.1145/2959100.2959165', '10.1145/2939672.2939704', '10.48550/arxiv.1608.07400', '10.1145/2964284.2984068', '10.1109/dasc-picom-datacom-cyberscitec.2016.149', '10.1145/2988450.2988451', '10.1016/j.eswa.2016.09.040', '10.1145/2983323.2983788', '10.1109/iset.2016.12', '10.48550/arxiv.1611.01604', '10.1007/s11280-017-0437-1', '10.1145/3018661.3018665', '10.1109/access.2017.2655150', '10.1145/3018661.3018689', '10.48550/arxiv.1612.04609', '10.1145/3018661.3018719', '10.1145/3018661.3018720', '10.1145/3025171.3025207', '10.48550/arxiv.1609.03675', '10.1007/978-981-10-4154-9_52', '10.1016/j.neunet.2017.03.009', '10.1145/3038912.3052639', '10.1145/3038912.3052638', '10.1145/3038912.3052569', '10.1007/978-3-319-57454-7_15', '10.1145/3109859.3109878', '10.1109/ithings-greencom-cpscom-smartdata.2016.109', '10.1109/ithings-greencom-cpscom-smartdata.2016.120', '10.1109/skima.2016.7916236', '10.1145/3041021.3054207', '10.1145/3041021.3054227', '10.1145/3077136.3080689', '10.1109/waina.2017.72', '10.24963/ijcai.2017/446', '10.1145/3077136.3080786', '10.48550/arxiv.1706.02263', '10.1145/3109859.3109876', '10.1145/3269206.3271761', '10.1145/3125486.3125488', '10.1016/j.ins.2017.06.026', '10.1145/3097983.3098077', '10.1145/3073565', '10.1109/icis.2017.7960088', '10.1145/3079628.3079684', '10.1145/3077136.3080822', '10.1145/3077136.3080776', '10.48550/arxiv.1708.04617', '10.1145/3077136.3080730', '10.24963/ijcai.2017/447', '10.1145/3077136.3080797', '10.1145/3077136.3080658', '10.1145/3097983.3098108', '10.1145/3097983.3098094', '10.1145/3097983.3098096', '10.1145/3109859.3109900', '10.1145/3109859.3109872', '10.1145/3109859.3109917', '10.1145/3109859.3109890', '10.1145/3109859.3109877', '10.1145/3132847.3132892', '10.48550/arxiv.1711.06632', '10.1145/3159652.3159656', '10.1145/3159652.3159728', '10.48550/arxiv.1801.05532', '10.48550/arxiv.1801.10288', '10.1145/3178876.3185994', '10.1145/3219819.3219886', '10.48550/arxiv.1803.00144', '10.1007/s11257-018-9209-6', '10.1145/3209978.3209981', '10.1109/icoiact.2018.8350761', '10.1145/3240323.3240374', '10.48550/arxiv.1805.07037', '10.1145/3219819.3220021', '10.1145/3219819.3219856', '10.1145/3219819.3220014', '10.1145/3219819.3220122', '10.1145/3219819.3220004', '10.1145/3219819.3219950', '10.1145/3219819.3219894', '10.1145/3270323.3270326', '10.1145/3219819.3219965', '10.1145/3269206.3271715', '10.1145/3234944.3234956', '10.1007/978-1-4899-7637-6', '10.48550/arxiv.1311.6355', '10.48550/arxiv.1703.04247', '10.48550/arxiv.1809.07428', '10.24963/ijcai.2018/308', '10.48550/arxiv.1605.02226', '10.1109/tkde.2016.2606428', '10.1145/3178876.3186150', '10.1145/3077136.3080777', '10.1109/cvpr.2017.243', '10.1145/3219819.3219961', '10.48550/arxiv.1706.01427', '10.48550/arxiv.1609.02907', '10.24963/ijcai.2018/510', '10.1145/3109859.3109896', '10.1145/2988450.2988453', '10.1145/2959100.2959180', '10.1145/3178876.3186154', '10.1145/3219819.3219890', '10.1145/3219819.3220086', '10.1145/3077136.3080786', '10.1145/3178876.3186146', '10.1109/cvpr.2016.279', '10.1145/3219819.3219886', '10.1145/2988450.2988456', '10.1145/3240323.3240374', '10.1145/3109859.3109878', '10.1007/978-3-319-12643-2_35', '10.1561/2000000039', '10.1109/asonam.2016.7752265', '10.48550/arxiv.1808.06414', '10.48550/arxiv.1803.03067', '10.1145/2872427.2883037', '10.48550/arxiv.1511.06939', '10.48550/arxiv.1508.04025', '10.1145/2988450.2988457', '10.1145/3077136.3080771', '10.1007/978-3-658-40442-0_9', '10.1093/ijl/3.4.235', '10.1145/2812802', '10.1109/cvpr.2016.314', '10.1109/cvpr.2016.91', '10.1007/978-3-319-46448-0_2', '10.1109/tpami.2016.2577031', '10.1109/iccv.2015.11', '10.18653/v1/d15-1075', '10.1007/11736790_9', '10.1007/11736790_9', '10.48550/arxiv.1806.02847', '10.48550/arxiv.1606.08415', '10.48550/arxiv.1901.07291', '10.48550/arxiv.1901.11504', '10.18653/v1/n19-4009', '10.48550/arxiv.1904.09482', '10.48550/arxiv.1904.09223', '10.48550/arxiv.1905.02450', '10.18653/v1/p19-1478', '10.48550/arxiv.1906.01604', '10.48550/arxiv.1704.04683', '10.18653/v1/p16-1162', '10.18653/v1/p18-1031', '10.18653/v1/p18-2124', '10.18653/v1/d16-1264', '10.18653/v1/w18-6301', '10.18653/v1/n18-1101', '10.48550/arxiv.1906.08237', '10.1162/tacl_a_00290', '10.1016/0004-3702(90)90007-m', '10.1109/cvpr.2013.474', '10.48550/arxiv.1308.3432', '10.1007/978-3-319-46493-0_38', '10.48550/arxiv.1605.05396', '10.48550/arxiv.1611.01144', '10.1109/cvpr.2017.374', '10.48550/arxiv.1711.02213', '10.48550/arxiv.1710.03740', '10.18653/v1/p18-1238', '10.48550/arxiv.1905.13727', '10.1109/iccv.2017.97', '10.48550/arxiv.1401.4082', '10.1109/tpami.2018.2856256', '10.18653/v1/n18-1197', '10.48550/arxiv.1606.03498', '10.48550/arxiv.1711.00937', '10.1109/cvpr.2018.00143', '10.1109/iccv.2017.629', '10.48550/arxiv.1701.05517', '10.1109/cvpr.2019.01245', '10.48550/arxiv.2004.08249', '10.48550/arxiv.1910.02054', '10.18653/v1/2020.acl-main.170', '10.48550/arxiv.2009.11278', '10.1109/wacv48630.2021.00028', '10.1016/s0022-2836(05)80269-4', '10.1038/nsb1101-923', '10.1016/0022-2836(87)90352-4', '10.1038/nbt.2419', '10.1093/protein/7.3.349', '10.1038/nprot.2010.5', '10.1038/nmeth.1818', '10.1371/journal.pone.0028766', '10.1186/1471-2105-11-431', '10.1016/j.tibs.2014.10.005', '10.1093/bioinformatics/btu739', '10.1093/nar/gkg571', '10.1093/protein/14.11.835', '10.1016/0022-2836(88)90564-5', '10.1146/annurev.biophys.37.092707.153558', '10.1109/tpami.2009.186', '10.1371/journal.pcbi.1002195', '10.1093/bioinformatics/btt473', '10.1093/bioinformatics/btr638', '10.48550/arxiv.1603.04467', '10.1093/nar/gkw1081', '10.1093/nar/gky949', '10.1371/journal.pcbi.1005659', '10.1038/s41592-019-0437-4', '10.1371/journal.pcbi.1005324', '10.1038/nbt.3988', '10.1016/j.cels.2019.03.006', '10.1038/s41467-018-04964-5', '10.1109/cvpr.2016.512', '10.1038/s41580-019-0163-x', '10.1371/journal.pone.0220182', '10.1093/bioinformatics/btz679', '10.1186/s12859-019-3019-7', '10.1109/iccv.2019.00069', '10.1093/nar/gkz1035', '10.1186/s12859-019-3220-8', '10.1038/s41586-019-1923-7', '10.1109/cvpr42600.2020.01070', '10.12688/f1000research.25097.1', '10.1038/s41594-020-0464-y', '10.1371/journal.ppat.1008707', '10.1007/978-3-030-58548-8_7', '10.1038/s41586-020-2921-5', '10.1038/s41586-020-2649-2', '10.1038/srep33964', '10.1093/nar/gkaa1100', '10.1016/j.sbi.2021.01.007', '10.1371/journal.pcbi.1008865', '10.1038/s42256-021-00348-5', '10.21203/rs.3.rs-515215/v1', '10.1038/s41586-021-03828-1', '10.1007/978-1-4842-4470-8_7', '10.48550/arxiv.1210.5474', '10.1109/cvpr.2015.7298761', '10.1162/neco.1992.4.6.863', '10.1109/cvpr.2014.241', '10.1109/72.165600', '10.1109/cvpr.2016.265', '10.48550/arxiv.1610.07629', '10.24963/ijcai.2017/310', '10.1109/iccv.2017.167', '10.48550/arxiv.1705.08086', '10.48550/arxiv.1802.05983', '10.48550/arxiv.1802.05957', '10.48550/arxiv.1802.05637', '10.48550/arxiv.1801.04406', '10.48550/arxiv.1805.08318', '10.23915/distill.00011', '10.48550/arxiv.1807.11346', '10.48550/arxiv.1807.09295', '10.48550/arxiv.1807.06650', '10.48550/arxiv.1808.10356', '10.48550/arxiv.1810.01365', '10.48550/arxiv.1904.11486', '10.48550/arxiv.1611.01673', '10.1109/cvpr.2018.00068', '10.48550/arxiv.1606.00704', '10.24963/ijcai.2018/306', '10.48550/arxiv.1606.03657', '10.1109/cvpr.2018.00917', '10.48550/arxiv.1711.10337', '10.1007/978-3-030-01219-9_11', '10.48550/arxiv.1811.10597', '10.48550/arxiv.1802.04942', '10.1145/325334.325242', '10.48550/arxiv.1605.09782', '10.48550/arxiv.1710.10196', '10.48550/arxiv.1706.00082', '10.48550/arxiv.1704.00028', '10.48550/arxiv.1406.2661', '10.48550/arxiv.1706.08500', '10.48550/arxiv.1612.05299', '10.1109/iccv.2015.178', '10.1109/cvpr.2015.7298935', '10.1109/cvpr.2015.7298788', '10.1109/cvpr.2015.7298932', '10.1109/cvpr.2011.5995711', '10.1109/cvpr.2009.5206757', '10.1093/cercor/1.1.1', '10.1109/cvpr.2014.254', '10.1109/cvpr.2014.214', '10.1146/annurev.psych.031809.130730', '10.1109/cvpr.2008.4587597', '10.1109/72.265960', '10.1109/cvpr.2016.413', '10.1007/978-3-319-46448-0_51', '10.1109/iccv.2009.5459303', '10.1007/978-3-319-75304-1_5', '10.48550/arxiv.1406.2984', '10.48550/arxiv.1604.03692', '10.48550/arxiv.1310.4546', '10.1007/978-3-642-16108-7_31', '10.1007/978-3-642-21557-5_17', '10.1109/cvpr.2015.7298966', '10.1109/tpami.2013.149', '10.1109/icdm.2014.65', '10.1006/jcss.1997.1504', '10.1007/s11263-014-0783-8', '10.1145/1015330.1015376', '10.1145/1646396.1646452', '10.1109/tmm.2015.2390499', '10.1007/s10791-009-9117-9', '10.1109/cvpr.2012.6247933', '10.1109/cvpr.2012.6247923', '10.1162/0899766042321814', '10.1007/978-3-642-02172-5_2', '10.1145/1873951.1873987', '10.1007/11752790_2', '10.1109/tpami.2007.70791', '10.1109/iccv.2013.261', '10.1109/iccv.2015.466', '10.1109/iccv.2015.12', '10.1109/tmm.2016.2558463', '10.1109/cvpr.2016.466', '10.48550/arxiv.1502.03044', '10.1109/cvpr.2016.541', '10.1007/978-3-642-77281-8_26', '10.48550/arxiv.1312.5402', '10.48550/arxiv.1306.5151', '10.1007/978-3-642-46466-9_18', '10.1109/cvpr.2012.6248110', '10.1109/icdar.2003.1227801', '10.1016/j.cviu.2005.09.012', '10.48550/arxiv.1606.04586', '10.1109/icvgip.2008.47', '10.1109/cvpr.2017.634', '10.48550/arxiv.1611.01578', '10.1109/access.2017.2696121', '10.48550/arxiv.1704.03976', '10.48550/arxiv.1705.07485', '10.48550/arxiv.1709.07417', '10.48550/arxiv.1505.03229', '10.48550/arxiv.1707.06347', '10.48550/arxiv.1708.04552', '10.48550/arxiv.1708.04896', '10.48550/arxiv.1708.05344', '10.48550/arxiv.1710.10564', '10.48550/arxiv.1709.01643', '10.1109/cvpr.2018.00745', '10.48550/arxiv.1710.09412', '10.48550/arxiv.1711.00648', '10.48550/arxiv.1711.04528', '10.48550/arxiv.1711.04340', '10.48550/arxiv.1712.04621', '10.48550/arxiv.1801.02929', '10.1109/cvpr.2019.00277', '10.48550/arxiv.1703.01041', '10.48550/arxiv.1702.05538', '10.48550/arxiv.1610.02242', '10.48550/arxiv.1703.01780', '10.48550/arxiv.1605.08695', '10.1109/cvpr.2017.668', '10.48550/arxiv.1608.03983', '10.1109/cvpr.2018.00255', '10.3389/frobt.2018.00066', '10.1109/tpami.2019.2913372', '10.1007/978-3-030-01216-8_12', '10.48550/arxiv.1711.00436', '10.1007/978-3-030-01246-5_2', '10.1109/icme.2017.8019371', '10.48550/arxiv.1804.09170', '10.1109/cvpr.2018.00907', '10.1109/tpami.2018.2858821', '10.48550/arxiv.1803.07055', '10.48550/arxiv.1611.02167', '10.48550/arxiv.1711.02846', '10.48550/arxiv.1805.10255', '10.48550/arxiv.1611.01331', '10.48550/arxiv.1802.03268', '10.1109/tpami.2008.67', '10.1109/cvpr.2015.7299102', '10.1109/tpami.2015.2465908', '10.1109/cvpr.2014.276', '10.1109/cvpr.2006.68', '10.1109/cvpr.2001.990517', '10.48550/arxiv.1404.0736', '10.1109/tpami.2009.167', '10.1109/tpami.2015.2437384', '10.1007/978-3-642-33786-4_32', '10.1007/978-3-642-12304-7_9', '10.1007/978-94-011-5014-9_17', '10.1109/icassp.2010.5495651', '10.1109/cvpr.2004.1315150', '10.1109/cvpr.2005.202', '10.1109/iccv.2009.5459250', '10.48550/arxiv.1404.5997', '10.1109/icassp.2013.6638312', '10.1109/cvpr.2014.220', '10.1007/978-3-319-10578-9_23', '10.48550/arxiv.1411.6836', '10.48550/arxiv.1411.2539', '10.1007/978-3-642-15561-1_11', '10.48550/arxiv.1312.6082', '10.1109/cvpr.2004.383', '10.1109/cvpr.2014.222', '10.48550/arxiv.1412.2306', '10.48550/arxiv.1409.0575', '10.48550/arxiv.1605.06211', '10.48550/arxiv.1312.6034', '10.48550/arxiv.1412.6115', '10.1109/iccvw.2011.6130513', '10.3390/s101211259', '10.48550/arxiv.1406.2572', '10.1109/tpami.2013.50', '10.1007/978-3-642-40728-4_14', '10.48550/arxiv.1301.3516', '10.48550/arxiv.1301.3557', '10.48550/arxiv.1301.5088', '10.1109/tpami.2012.231', '10.48550/arxiv.1206.2944', '10.1007/978-3-642-35289-8_13', '10.1007/978-3-642-35289-8_5', '10.1007/978-3-642-35289-8_23', '10.48550/arxiv.cs/0212041', '10.1007/3-540-56602-3_158', '10.1523/jneurosci.02-01-00032.1982', '10.1016/0167-2789(86)90244-7', '10.1016/s0893-6080(05)80139-x', '10.1109/34.506411', '10.1007/978-94-010-1223-2_23', '10.1007/978-3-642-35289-8_11', '10.1016/0004-3702(92)90079-d', '10.1016/s0031-8914(56)80033-5', '10.1109/proc.1976.10286', '10.48550/arxiv.1211.5063', '10.48550/arxiv.1507.00210', '10.1016/s0378-3758(00)00115-4', '10.1109/icassp.2014.6853582', '10.1016/s0893-6080(00)00026-5', '10.1109/cvpr.2008.4587821', '10.1007/978-3-642-33715-4_54', '10.1007/978-3-540-88682-2_4', '10.1007/978-3-642-33712-3_25', '10.1109/iccv.2013.258', '10.1109/cvpr.2010.5539970', '10.1109/cvpr.2011.5995347', '10.1109/tpami.2011.155', '10.1145/2461912.2462002', '10.1109/cvpr.2011.5995659', '10.1109/cvpr.2012.6248070', '10.1109/cvpr.2012.6247998', '10.1109/tpami.2011.208', '10.1109/cvpr.2009.5206772', '10.1109/tpami.2010.161', '10.1109/cvprw.2009.5206594', '10.1109/iccv.2013.344', '10.1109/tpami.2008.128', '10.1007/s11263-010-0390-2', '10.1109/cvprw.2009.5204174', '10.1109/cvpr.2006.19', '10.1109/cvpr.2007.383271', '10.1006/cviu.2001.0921', '10.1016/j.patrec.2008.04.005', '10.1109/cvpr.2009.5204174', '10.1007/978-3-319-10584-0_20', '10.1007/978-3-642-15555-0_26', '10.1007/978-3-319-10584-0_23', '10.1007/978-3-319-10590-1_53', '10.1109/cvpr.2015.7299025', '10.1109/cvpr.2015.7298642', '10.1109/cvpr.2013.79', '10.1109/tpami.2010.147', '10.1109/icip.2013.6738831', '10.1109/cvpr.2014.119', '10.1109/iccv.2011.6126343', '10.1109/cvpr.2013.386', '10.1109/iccv.2013.84', '10.1109/tip.2005.852470', '10.48550/arxiv.1406.2283', '10.48550/arxiv.1310.1531', '10.1007/978-3-319-10590-1_54', '10.1007/978-1-4612-0745-0', '10.1109/tasl.2011.2109382', '10.1109/cvpr.2011.5995504', '10.1109/tasl.2011.2134090', '10.48550/arxiv.1411.7714', '10.1109/cvpr.2015.7298621', '10.1109/tpami.2015.2389824', '10.1109/iccv.2011.6126456', '10.1109/iccv.2009.5459183', '10.1145/3065386', '10.48550/arxiv.1502.04275', '10.1007/978-3-319-10602-1_20', '10.48550/arxiv.1405.3531', '10.1109/cvprw.2014.131', '10.48550/arxiv.1312.1847', '10.1007/978-3-642-33266-1_8', '10.1007/978-3-642-15555-0_11', '10.1109/cvpr.2010.5539958', '10.1109/cvpr.2010.5539949', '10.1109/cvpr.2010.5540018', '10.1109/cvpr.2010.5539963', '10.1109/iccv.2005.239', '10.1109/iccv.2003.1238663', '10.1109/cvpr.2004.1315206', '10.1007/978-3-540-88690-7_52', '10.1109/cvpr.2008.4587635', '10.1109/tpami.2008.138', '10.1109/iccv.2003.1238476', '10.1007/978-3-540-88682-2_24', '10.1109/cvpr.2010.5540039', '10.1109/cvpr.2010.5539914', '10.1109/cvpr.2009.5206609', '10.1109/cvpr.2010.5540009', '10.1109/cvpr.2006.264', '10.1109/iccv.2005.171', '10.1109/cvpr.2007.383172', '10.1109/cvpr.2009.5206582', '10.1109/cvpr.2009.5206566', '10.1109/cvpr.2008.4587633', '10.1109/tpami.2009.132', '10.1109/cvpr.2007.382971', '10.1109/cvpr.2009.5206839', '10.1109/cvpr.2009.5206531', '10.1109/iccv.2009.5459466', '10.1109/tpami.2005.188', '10.1109/iccv.2009.5459419', '10.1109/cvprw.2009.5206566', '10.1109/iccv.2009.5459354', '10.1007/bfb0014884', '10.1007/978-3-540-24673-2_31', '10.1007/978-3-540-24673-2_3', '10.1145/1015706.1015718', '10.1145/218380.218439', '10.1016/0734-189x(83)90020-8', '10.1109/34.297948', '10.1109/tpami.1984.4767504', '10.1016/s0734-189x(85)80003-7', '10.1109/38.20317', '10.1145/293145.293146', '10.1145/1201775.882269', '10.1016/0734-189x(86)90114-3', '10.1109/34.277594', '10.1109/cvpr.2001.990638', '10.1109/cvpr.2003.1211356', '10.1109/34.3908', '10.1109/tpami.1986.4767807', '10.1145/199404.199410', '10.1109/34.601247', '10.1109/tip.2005.843754', '10.1109/cvpr.2006.18', '10.1145/566570.566573', '10.1145/566654.566573', '10.1145/882262.882269', '10.1007/11744023_45', '10.1016/0893-6080(94)00099-8', '10.1016/s0734-189x(87)80014-2', '10.1007/978-3-540-74198-5_14', '10.1007/11744023_1', '10.1007/978-3-540-85891-1_26', '10.1007/11744047_26', '10.1007/3-540-47979-1_7', '10.1007/11861898_28', '10.1007/11788034_28', '10.1007/978-3-540-88682-2_40', '10.1371/journal.pcbi.0040027', '10.1109/cvpr.2008.4587658', '10.1109/cvpr.2007.383272', '10.1109/cvpr.2008.4587803', '10.1109/cvpr.2008.4587586', '10.1109/tpami.2006.79', '10.1109/cvpr.2007.383050', '10.1109/cvprw.2008.4562953', '10.1109/icip.2007.4378918', '10.1109/tpami.2007.1055', '10.1109/tpami.2007.1144', '10.1109/cvprw.2006.177', '10.1007/11736790_8', '10.1109/iccv.2005.142', '10.1109/cvpr.2006.232', '10.1109/cvpr.2008.4587417', '10.1016/0893-6080(88)90007-x', '10.1016/0885-064x(88)90021-0', '10.1016/0893-6080(90)90044-l', '10.1016/0893-6080(89)90022-1', '10.1109/72.548162', '10.1109/72.279191', '10.1109/iscas.1992.230622', '10.1109/72.410363', '10.1007/978-3-540-88693-8_52', '10.1109/iccv.2009.5459169', '10.1016/j.imavis.2004.02.006', '10.1109/iccv.2007.4409066', '10.48550/arxiv.1310.6343', '10.1109/tpami.2007.56', '10.1007/978-3-642-15555-0_42', '10.1109/cvpr.2013.423', '10.1109/cvpr.2014.49', '10.1109/cvpr.2011.5995678', '10.1109/tpami.2011.231', '10.1109/tpami.2012.28', '10.1109/cvpr.2009.5206727', '10.1109/iccv.2013.10', '10.1109/cvpr.2012.6248077', '10.1109/cvpr.2013.237', '10.1109/iccv.2011.6126474', '10.1109/cvpr.2012.6248069', '10.1109/cvpr.2013.406', '10.1109/34.655648', '10.1109/cvpr.2013.417', '10.1109/cvpr.2013.465', '10.1007/978-3-642-40763-5_51', '10.1109/34.655647', '10.21236/ada164453', '10.1109/icnn.1993.298832', '10.1109/ijcnn.1989.118276', '10.1016/0167-2789(91)90236-3', '10.1145/29380.29864', '10.1109/69.382304', '10.1109/72.125861', '10.1109/icnn.1993.298725', '10.1109/72.125866', '10.1007/3-540-52255-7_31', '10.1109/nnsp.1992.253712', '10.1007/978-3-642-82657-3_24', '10.1007/978-3-642-33709-3_35', '10.48550/arxiv.1404.1869', '10.1007/978-3-642-33712-3_22', '10.1109/cvpr.2014.304', '10.1109/cvpr.2012.6248018', '10.1109/cvpr.2011.5995368', '10.1109/cvpr.2014.264', '10.1109/cvpr.2012.6248021', '10.1016/j.patrec.2012.07.019', '10.1109/tpami.2014.2343217', '10.1109/icra.2014.6907300', '10.1145/219717.219748', '10.1109/cvpr.2009.5206693', '10.1007/978-3-642-33786-4_34', '10.1109/cvpr.2005.16', '10.1109/tmm.2010.2091400', '10.1177/0278364913491297', '10.1109/iccv.2013.315', '10.48550/arxiv.1409.3505', '10.1109/cvpr.2012.6248090', '10.1109/cvpr.2011.5995477', '10.1109/iccv.2003.1238311', '10.48550/arxiv.1308.0371', '10.1109/iccv.2013.257', '10.1109/tpami.2011.131', '10.1109/tpami.2009.154', '10.1109/tpami.2006.244', '10.1109/iccv.2009.5459211', '10.1007/978-3-0348-8609-3', '10.1016/b978-1-55860-377-6.50038-4', '10.1016/b978-1-55860-377-6.50012-8', '10.1007/978-1-4612-0493-0', '10.1007/978-3-642-83069-3', '10.1007/3-540-59119-2_189', '10.1007/978-3-642-75894-2', '10.1016/b978-1-55860-377-6.50060-8', '10.1007/bfb0017012', '10.1007/978-1-4471-3087-1_36', '10.1016/b978-1-55860-377-6.50032-3', '10.1007/978-3-642-76153-9_28', '10.1109/icdar.1995.598994', '10.1016/0893-6080(90)90049-q', '10.1145/225058.225118', '10.1145/203330.203338', '10.1145/203330.203341', '10.1145/258128.258179', '10.1109/mex.1987.4307093', '10.1109/5.58325', '10.1145/203330.203340', '10.1016/0031-3203(94)90006-x', '10.1145/219717.219768', '10.1145/219717.219771', '10.1145/167088.167198', '10.1109/tpami.1984.4767523', '10.1109/pgec.1965.263958', '10.1016/0016-0032(61)90702-5', '10.1016/0022-0000(79)90045-x', '10.1016/0166-218x(93)90126-9', '10.1016/0031-3203(90)90098-6', '10.1109/tit.1981.1056403', '10.1145/238061.238163', '10.1109/72.317728', '10.1007/978-3-642-79119-2_5', '10.1109/isit.1993.748374', '10.1145/225298.225305', '10.1007/978-3-642-56927-2_6', '10.1109/5.58326', '10.1109/robot.1991.131713', '10.1007/978-1-4757-2440-0', '10.1109/18.382012', '10.1109/21.155943', '10.1109/72.80210', '10.1145/203330.203336', '10.1111/j.2517-6161.1976.tb01573.x', '10.1007/3-540-49257-7_15', '10.1109/cvpr.2009.5206529', '10.1109/tnn.2008.917504', '10.1145/355744.355745', '10.1109/cvpr.2008.4587638', '10.1109/tpami.2006.148', '10.1109/cvpr.2007.382970', '10.1109/icassp.2010.5495403', '10.1109/18.720541', '10.1016/0893-6080(89)90014-2', '10.1007/bfb0020229', '10.1007/978-3-642-35289-8', '10.1007/11752790_7', '10.1007/11744085_36', '10.1109/tsa.2004.841042', '10.1109/tpami.2005.127', '10.1109/iccv.2005.66', '10.1109/icassp.2000.859329', '10.1109/iccv.2005.56', '10.1109/ijcnn.1989.118274', '10.1016/b978-0-08-051584-7.50038-3', '10.1109/icassp.1989.266376', '10.48550/arxiv.1104.3250', '10.1007/978-1-4471-1599-1_78', '10.15607/rss.2014.x.041', '10.1109/cvpr.2014.212', '10.1109/ccv.1988.590038', '10.1016/0004-3702(81)90024-2', '10.1007/978-3-642-51590-3', '10.1109/cvpr.1989.37871', '10.1016/0004-3702(81)90023-0', '10.1016/0167-8191(88)90015-4', '10.1109/tpami.1986.4767767', '10.1109/mcg.1987.277064', '10.1145/359842.359846', '10.1109/tcom.1983.1095851', '10.1016/0734-189x(83)90095-6', '10.1016/0898-1221(77)90079-7', '10.1007/978-3-662-02427-0', '10.1007/978-1-4613-1637-4', '10.1007/978-3-642-23968-7', '10.1016/j.neunet.2012.02.023', '10.1090/pcms/013/08', '10.1109/iscas.2010.5537907', '10.48550/arxiv.1102.0183', '10.1371/journal.pcbi.1000579', '10.48550/arxiv.1306.0239', '10.1007/bf00114844', '10.1109/cvpr.2006.164', '10.48550/arxiv.1305.2436', '10.1007/978-3-319-10584-0_26', '10.48550/arxiv.1312.5851', '10.1109/cvpr.2014.414', '10.48550/arxiv.1404.4316', '10.1007/978-3-319-10602-1_26', '10.48550/arxiv.1412.1441', '10.48550/arxiv.1311.5591', '10.48550/arxiv.1412.6558', '10.48550/arxiv.1507.01526', '10.48550/arxiv.1308.0850', '10.48550/arxiv.1409.6070', '10.1109/tnnls.2013.2293637', '10.1007/978-3-540-24670-1_36', '10.1109/tpami.2015.2491929', '10.48550/arxiv.1411.2861', '10.1109/cvpr.2015.7298686', '10.1109/cvpr.2015.7298641', '10.1007/978-3-319-10584-0_22', '10.48550/arxiv.1405.0312', '10.48550/arxiv.1412.7062', '10.48550/arxiv.1301.3583', '10.1007/11503415_37', '10.1016/s0893-6080(98)00012-4', '10.1109/tcsi.2004.834521', '10.48550/arxiv.1301.3568', '10.5591/978-1-57735-516-8/ijcai11-210', '10.1016/s0898-1221(96)90463-0', '10.1016/s0893-6080(05)80056-5', '10.1109/72.286919', '10.1016/0893-6080(88)90003-2', '10.1016/0925-2312(94)90071-x', '10.1109/tassp.1976.1162867', '10.6083/m40p0wz1', '10.1016/0041-5553(64)90137-5', '10.1007/978-1-4419-8853-9', '10.1007/978-3-642-35289-8_27', '10.1007/978-1-4613-1367-0_10', '10.21236/ada273556', '10.1109/asru.2011.6163930', '10.1109/slt.2012.6424228', '10.1016/j.neunet.2007.04.016', '10.1109/tpami.2008.137', '10.48550/arxiv.1309.4168', '10.48550/arxiv.1409.3215', '10.1109/icassp.1987.1169748', '10.1109/icassp.2013.6638947', '10.3115/v1/d14-1179', '10.3115/v1/p14-1129', '10.1109/icfhr.2014.55', '10.48550/arxiv.1207.4404', '10.48550/arxiv.1301.3584', '10.48550/arxiv.1311.6091', '10.1109/icassp.2011.5947611', '10.1016/s0065-2458(08)60519-7', '10.21236/ad0406138', '10.1016/s0019-9958(67)91165-5', '10.1016/s0019-9958(75)90261-2', '10.1109/tse.1976.233812', '10.1109/t-c.1975.224180', '10.1016/s0019-9958(72)90424-x', '10.1145/321864.321875', '10.1145/1045231.1045233', '10.1007/bfb0031388', '10.1109/icnn.1988.23829', '10.1109/icassp.1986.1169179', '10.1016/s0925-2312(97)90018-7', '10.1109/tit.1967.1054010', '10.1109/2.144441', '10.1109/4.104196', '10.1016/0167-6393(90)90049-f', '10.1109/pgec.1967.264666', '10.1109/5.156477', '10.1016/0885-2308(87)90010-6', '10.1109/tac.1974.1100577', '10.1109/34.57669', '10.3115/1075812.1075870', '10.1016/0031-3203(91)90081-f', '10.1109/tc.1978.1675179', '10.1109/78.175747', '10.1145/130385.130401', '10.1016/0031-3203(82)90024-3', '10.1109/12.210173', '10.1109/5.156474', '10.1109/89.260364', '10.1109/icpr.1994.576966', '10.1109/29.21701', '10.1109/cvpr.1997.609310', '10.1109/5.18626', '10.1109/icassp.1990.115733', '10.1109/icdar.1995.598933', '10.1109/72.129422', '10.1109/ijcnn.1992.227175', '10.1109/icassp.1993.319196', '10.1109/icassp.1990.115724', '10.1109/89.568733', '10.1109/icnn.1993.298793', '10.1109/34.62605', '10.1109/72.554195', '10.1109/icassp.1994.389576', '10.1109/72.363436', '10.1109/icassp.1989.266355', '10.1016/0893-6080(90)90028-j', '10.1109/icsmc.1995.538133', '10.1109/72.536317', '10.1109/icpr.1994.576889', '10.1109/cvpr.1996.517075', '10.1007/bf00116037', '10.1007/978-3-642-76153-9_35', '10.1007/978-1-4615-3210-1', '10.1109/inpar.2012.6339587', '10.48550/arxiv.1201.0490', '10.48550/arxiv.1209.5111', '10.1109/fg.2011.5771385', '10.1109/cvprw.2011.5981788', '10.14288/1.0051652', '10.1007/978-3-642-25566-3_40', '10.1016/0005-1098(85)90014-7', '10.1145/1056441.1056445', '10.1109/tsmc.1985.6313371', '10.1109/tsmc.1983.6313193', '10.1016/0893-6080(90)90056-q', '10.1109/tsmc.1983.6313077', '10.1109/tac.1986.1104342', '10.1109/tsmc.1985.6313407', '10.1109/icnn.1988.23856', '10.1016/0004-3702(80)90026-0', '10.1016/b978-1-55860-356-1.50019-4', '10.1109/cec.2001.934446', '10.1007/978-3-540-24854-5_12', '10.1007/3-540-06867-8_3', '10.1109/cec.2002.1004528', '10.1007/978-94-009-2649-3_12', '10.1109/cec.2010.5585994', '10.1007/978-1-4613-8476-2', '10.1016/0022-5193(68)90079-9', '10.1109/64.393139', '10.1016/s0022-5193(05)80391-1', '10.1016/0022-5193(68)90080-5', '10.1007/978-3-540-24855-2_130', '10.1109/isads.2005.1452025', '10.1109/tnn.2004.842673', '10.1109/5.784219', '10.1109/4235.996015', '10.1109/icec.1994.350019', '10.1109/tevc.2005.856210', '10.1109/4235.585893', '10.1016/b978-012428765-5/50054-2', '10.1016/b978-012428765-5/50053-0', '10.1007/978-3-662-44848-9_34', '10.1007/bfb0015571', '10.1016/0004-3702(95)00022-4', '10.1016/s0959-4388(98)80143-8', '10.1016/s0959-4388(97)80032-3', '10.1109/tpami.1987.4767935', '10.1109/cvpr.1997.609451', '10.1109/34.134043', '10.1016/s0042-6989(98)00030-3', '10.1109/34.589215', '10.1016/0004-3702(87)90070-1', '10.1109/icpr.1996.546722', '10.1515/znc-1998-7-807', '10.1109/tpami.1984.4767500', '10.1109/iccv.1995.466931', '10.1109/34.615453', '10.1016/0031-3203(81)90009-1', '10.1007/bfb0056901', '10.1109/cec.1999.781936', '10.1017/s0305004100030401', '10.1109/72.963769', '10.1109/icnn.1997.614247', '10.1109/72.572107', '10.1109/72.508930', '10.1109/72.788663', '10.1109/5.58337', '10.1007/bf00114722', '10.1007/bfb0014548', '10.1007/978-1-4419-9017-4_6', '10.1109/icassp.1998.674454', '10.1109/tasl.2011.2165280', '10.1109/icassp.2010.5495222', '10.1006/csla.2001.0182', '10.1109/icassp.2007.367023', '10.1109/icassp.2007.366945', '10.1109/tasl.2011.2116010', '10.1109/msp.2008.926652', '10.1109/tasl.2006.878265', '10.1109/79.536824', '10.1109/tassp.1981.1163530', '10.1109/msp.2005.1511826', '10.1109/msp.2009.932166', '10.1109/tasl.2011.2129510', '10.1109/icassp.2012.6288994', '10.1109/icassp.2012.6288833', '10.1109/icassp.2012.6288836', '10.1109/icassp.2011.5947494', '10.1109/72.279192', '10.1109/icassp.2009.4960445', '10.1109/tasl.2011.2155060', '10.1109/icassp.2011.5947378', '10.1109/icassp.2008.4518545', '10.1109/tasl.2008.2010286', '10.1109/icassp.2012.6288837', '10.1109/icassp.2012.6288333', '10.1109/icassp.2011.5947490', '10.1109/icassp.2012.6288864', '10.1109/asru.2011.6163899', '10.1109/asru.2009.5373263', '10.1109/icassp.2000.862024', '10.1109/tit.1986.1057145', '10.1109/icassp.2012.6288863', '10.1109/ijcnn.1991.155435', '10.1016/0167-6393(89)90009-5', '10.1007/978-3-642-60087-6_20', '10.1007/978-3-642-35289-8_32', '10.1109/iccv.1999.791202', '10.1109/cvpr.2000.854739', '10.1109/afgr.1996.557250', '10.1006/cviu.1998.0716', '10.1109/ivs.2004.1336348', '10.1109/34.917571', '10.1007/978-3-540-24670-1_6', '10.1007/3-540-47979-1_47', '10.1109/iccv.2001.937552', '10.1109/iccv.2003.1238422', '10.1109/icassp.2013.6639343', '10.48550/arxiv.1110.4198', '10.1145/1327452.1327492', '10.48550/arxiv.1106.5730', '10.48550/arxiv.cmp-lg/9605002', '10.1109/icassp.1995.479394', '10.3115/1073083.1073135', '10.3115/1220175.1220271', '10.3115/v1/p14-2023', '10.48550/arxiv.1011.0686', '10.48550/arxiv.1412.7755', '10.1016/0010-0277(84)90022-2', '10.1016/0031-3203(76)90020-0', '10.1006/jmps.1997.1154', '10.1016/0885-2014(88)90014-7', '10.1016/j.cogpsych.2012.09.003', '10.1016/0010-0277(81)90013-5', '10.1145/800057.808710', '10.1016/0010-0285(73)90005-4', '10.1016/0031-3203(90)90068-v', '10.1109/34.506410', '10.1016/j.cognition.2011.11.005', '10.1371/journal.pone.0057410', '10.1016/s1053-8119(03)00088-0', '10.1016/j.neuropsychologia.2006.06.026', '10.1109/72.788640', '10.21236/ada238689', '10.1109/tpami.2012.269', '10.1109/34.799914', '10.1109/34.161346', '10.1016/j.bbr.2009.08.031', '10.1145/1968.1972', '10.1145/168304.168306', '10.1109/slt.2014.7078575', '10.1007/s00422-012-0490-x', '10.3115/1219840.1219855', '10.1109/asru.2015.7404775', '10.48550/arxiv.1406.1078', '10.48550/arxiv.1505.05424', '10.48550/arxiv.1506.02158', '10.48550/arxiv.1506.05869', '10.1038/ncomms3528', '10.1523/jneurosci.2076-07.2007', '10.1371/journal.pbio.0040125', '10.48550/arxiv.1508.00305', '10.1109/85.238389', '10.3115/v1/p15-1002', '10.3115/v1/d14-1070', '10.48550/arxiv.1508.05508', '10.1109/72.279194', '10.1186/1744-9081-2-31', '10.48550/arxiv.1512.00965', '10.3115/v1/p15-1129', '10.48550/arxiv.1506.00379', '10.48550/arxiv.1109.6841', '10.1109/cvpr.2016.92', '10.48550/arxiv.1511.05641', '10.48550/arxiv.1504.00941', '10.1109/cvpr.2015.7298958', '10.48550/arxiv.1511.05635', '10.48550/arxiv.1603.08029', '10.48550/arxiv.1511.07289', '10.48550/arxiv.1412.6806', '10.48550/arxiv.1509.08985', '10.48550/arxiv.1012.2599', '10.48550/arxiv.1504.00702', '10.48550/arxiv.1511.08228', '10.48550/arxiv.1511.06279', '10.1109/iros.2015.7354297', '10.48550/arxiv.1602.08671', '10.1109/ecnn.2000.886220', '10.1109/ijcnn.1990.137898', '10.1109/icassp.2013.6639349', '10.1109/72.750553', '10.1109/5.726790', '10.1109/icnn.1993.298591', '10.1007/3-540-44668-0_13', '10.1109/ijcnn.1992.287172', '10.1109/icnn.1993.298623', '10.48550/arxiv.1503.05671', '10.1109/ijcnn.2001.938471', '10.48550/arxiv.1210.0118', '10.1109/tnnls.2016.2582924', '10.1016/j.neunet.2013.01.022', '10.1016/j.neunet.2014.09.003', '10.48550/arxiv.1602.02410', '10.48550/arxiv.1603.08983', '10.1109/icassp.2016.7472780', '10.1109/tnn.2010.2073482', '10.48550/arxiv.1604.03640', '10.48550/arxiv.1511.06430', '10.48550/arxiv.1505.01504', '10.48550/arxiv.1412.7753', '10.48550/arxiv.1402.3722', '10.3115/v1/d14-1034', '10.48550/arxiv.1602.01137', '10.48550/arxiv.1606.01700', '10.48550/arxiv.1610.05256', '10.48550/arxiv.1212.5701', '10.1006/csla.1996.0011', '10.48550/arxiv.1603.01417', '10.48550/arxiv.1603.06744', '10.48550/arxiv.1608.04207', '10.48550/arxiv.1506.07285', '10.1007/978-3-540-88690-7_6', '10.26719/1996.2.2.290', '10.26719/2000.6.5-6.1114', '10.1109/cvpr.2005.254', '10.1016/s0264-410x(99)00456-9', '10.1016/s0264-410x(98)00415-0', '10.1109/cvpr.2008.4587652', '10.1109/cvpr.2006.200', '10.1016/s0042-6989(97)00169-7', '10.1109/iccv.2007.4408875', '10.1109/cvpr.2007.383157', '10.1109/cvpr.2009.5206545', '10.1109/cvpr.2006.301', '10.1109/cvpr.2005.320', '10.1016/s0889-8553(21)00146-1', '10.48550/arxiv.1010.3467', '10.48550/arxiv.1409.1259', '10.3115/v1/d14-1162', '10.48550/arxiv.1507.06228', '10.1109/cvpr.2016.308', '10.1109/cvpr.2016.515', '10.48550/arxiv.1403.5607', '10.1109/tnn.2010.2049859', '10.21236/ada446572', '10.48550/arxiv.1301.3781', '10.48550/arxiv.1502.01710', '10.3115/v1/d14-1181', '10.18653/v1/d15-1041', '10.18653/v1/d15-1176', '10.3115/1073483.1073485', '10.3115/v1/p14-1062', '10.1006/csla.1999.0128', '10.48550/arxiv.1503.05034', '10.3115/v1/p15-2081', '10.48550/arxiv.1508.02096', '10.48550/arxiv.1405.4273', '10.3115/v1/p15-1151', '10.48550/arxiv.1508.04112', '10.48550/arxiv.1508.04271', '10.1007/978-3-540-74690-4_56', '10.1007/978-3-319-24574-4_28', '10.1109/cvpr.2015.7298967', '10.1109/cvpr.2013.263', '10.48550/arxiv.1511.06988', '10.48550/arxiv.1505.02438', '10.48550/arxiv.1511.02583', '10.1109/cvpr.2016.619', '10.1109/iccv.2015.222', '10.1109/asru.2013.6707742', '10.1109/78.650093', '10.3115/1073445.1073462', '10.3115/v1/w14-4012', '10.3115/v1/w14-4009', '10.1109/icassp.2013.6639345', '10.1007/3-540-45014-9_1', '10.48550/arxiv.1305.2982', '10.48550/arxiv.1306.0733', '10.48550/arxiv.1507.05331', '10.48550/arxiv.1412.7003', '10.1093/bioinformatics/btr444', '10.1007/s10994-009-5124-8', '10.48550/arxiv.1206.4683', '10.1109/icassp.1986.1168599', '10.1109/35.41400', '10.1016/0005-1098(78)90005-5', '10.1109/ijcnn.1991.155331', '10.1016/0893-6080(94)90014-0', '10.1109/icassp.2013.6638949', '10.1109/29.46546', '10.48550/arxiv.1006.0448', '10.1007/978-3-642-39593-2_1', '10.1109/tsp.2009.2036477', '10.1109/cvpr.2013.355', '10.1109/ijcnn.2000.861455', '10.1007/978-1-4471-1599-1_27', '10.1109/72.80230', '10.1109/72.572101', '10.1109/78.258082', '10.1109/cvpr.2015.7298754', '10.1109/tnn.2006.881047', '10.48550/arxiv.1512.01274', '10.48550/arxiv.1602.06709', '10.1016/j.cviu.2017.05.007', '10.48550/arxiv.1607.04381', '10.48550/arxiv.1512.03385', '10.1109/cvpr.2016.284', '10.1109/tpami.2016.2644615', '10.1109/fg.2015.7163154', '10.48550/arxiv.1601.00978', '10.1017/s0004972700034894', '10.1090/s0002-9939-1978-0476971-2', '10.1016/j.artint.2014.02.004', '10.7153/mia-05-06', '10.1016/b978-0-12-604550-5.50015-8', '10.1007/3-540-44864-0_11', '10.48550/arxiv.1509.08101', '10.48550/arxiv.1512.03965', '10.48550/arxiv.1610.04167', '10.1007/978-3-642-28027-6', '10.1007/978-3-642-15825-4_9', '10.48550/arxiv.1306.0543', '10.48550/arxiv.1311.5750', '10.48550/arxiv.1511.06530', '10.48550/arxiv.1408.5352', '10.18653/v1/d15-1166', '10.48550/arxiv.1412.5567', '10.48550/arxiv.1112.2679', '10.48550/arxiv.1512.02595', '10.48550/arxiv.0806.4686', '10.1109/access.2015.2494536', '10.1109/cvpr.2016.435', '10.48550/arxiv.1602.02830', '10.1109/isca.2016.30', '10.1109/cvpr.2016.280', '10.1109/iccv.2015.327', '10.1109/iccv.2015.173', '10.1109/cvprw.2012.6238900', '10.48550/arxiv.1509.06569', '10.48550/arxiv.1412.1442', '10.48550/arxiv.1511.05946', '10.48550/arxiv.1506.02626', '10.1109/cvpr.2015.7298681', '10.1109/isscc.2014.6757323', '10.1109/72.248452', '10.1109/cvpr.2016.521', '10.1109/jssc.2016.2616357', '10.1007/978-3-319-46493-0_32', '10.1109/isscc.2016.7418007', '10.1109/isca.2016.32', '10.1109/icassp.2012.6288897', '10.48550/arxiv.1512.04906', '10.48550/arxiv.1609.08144', '10.48550/arxiv.1412.5474', '10.1109/cvpr.2008.4587784', '10.1007/978-3-319-46484-8_3', '10.48550/arxiv.1512.06473', '10.48550/arxiv.1510.01722', '10.48550/arxiv.1511.06789', '10.48550/arxiv.1611.10012', '10.1109/cvpr.2015.7298682', '10.48550/arxiv.1502.02551', '10.1109/icassp.2014.6853595', '10.48550/arxiv.1506.04449', '10.1007/978-1-4471-0219-9_11', '10.1109/tnn.2004.828762', '10.48550/arxiv.1606.01981', '10.1016/j.neucom.2017.02.029', '10.1109/jproc.2017.2761740', '10.48550/arxiv.1704.07724', '10.48550/arxiv.1703.08651', '10.48550/arxiv.1511.06393', '10.1109/tpds.2017.2752706', '10.48550/arxiv.1406.2952', '10.1109/cvpr.2016.456', '10.1007/3-540-44581-1_27', '10.1007/978-3-642-35289-8_30', '10.1109/18.661502', '10.1007/3-540-44581-1_15', '10.48550/arxiv.1602.04485', '10.1109/ijcnn.2001.938745', '10.48550/arxiv.1611.02731', '10.1007/978-3-540-45167-9_16', '10.1145/279943.279989', '10.1016/0020-0190(87)90114-1', '10.1103/physrevlett.115.128101', '10.48550/arxiv.1710.01878', '10.1109/msp.2017.2765695', '10.1109/cvpr.2018.00716', '10.48550/arxiv.1706.08498', '10.48550/arxiv.1611.03530', '10.48550/arxiv.1312.6211', '10.1016/s0893-6080(03)00138-2', '10.48550/arxiv.1609.03541', '10.48550/arxiv.1703.04730', '10.1109/tpami.2015.2502579', '10.1109/cvpr.2017.195', '10.1109/cvpr.2017.98', '10.1109/cvpr.2017.351', '10.48550/arxiv.1608.08710', '10.48550/arxiv.1608.03665', '10.1145/1365490.1365500', '10.48550/arxiv.1610.09639', '10.48550/arxiv.1512.08571', '10.48550/arxiv.1410.0759', '10.48550/arxiv.1611.06321', '10.1038/502172a', '10.1016/j.neunet.2005.06.042', '10.48550/arxiv.1103.0398', '10.1109/icassp.2007.366913', '10.1007/978-1-4612-1694-0_15', '10.1109/icassp.2015.7178304', '10.48550/arxiv.1412.5068', '10.1109/icassp.2013.6638293', '10.1145/2619239.2631434', '10.1109/eurosp.2016.36', '10.48550/arxiv.1511.03034', '10.1109/cvpr.2016.282', '10.1109/lra.2015.2509024', '10.48550/arxiv.1604.07316', '10.48550/arxiv.1605.07277', '10.48550/arxiv.1606.04435', '10.48550/arxiv.1607.05113', '10.1007/978-3-319-63387-9_1', '10.18653/v1/p16-1231', '10.48550/arxiv.1605.07262', '10.48550/arxiv.1312.5602', '10.48550/arxiv.1607.02533', '10.48550/arxiv.1511.06422', '10.1109/cvpr.2015.7298809', '10.1038/494035b', '10.1038/506150a', '10.1038/nmeth.2613', '10.1016/j.foodchem.2014.11.121', '10.48550/arxiv.1206.6430', '10.1109/ijcnn.2009.5178592', '10.1038/ncomms5308', '10.48550/arxiv.1504.04788', '10.1109/ijcb.2011.6117503', '10.48550/arxiv.1206.6389', '10.1109/icassp.2013.6639347', '10.1007/s10994-010-5188-5', '10.1007/978-3-642-21735-7_7', '10.1109/tkde.2013.57', '10.14722/ndss.2016.23115', '10.1007/s10994-017-5663-3', '10.1109/tkde.2013.57', '10.1007/bf02134016', '10.1007/978-3-642-40994-3_25', '10.1109/tit.2008.929958', '10.1109/cvpr.2009.5206537', '10.1109/iccv.2015.277', '10.1109/iccv.2017.74', '10.48550/arxiv.1510.00149', '10.1007/978-3-642-33885-4_4', '10.1007/978-3-642-33709-3_6', '10.1109/iccv.2011.6126229', '10.1109/cvpr.2011.5995492', '10.1109/cvpr.2013.124', '10.1016/j.patrec.2004.02.002', '10.1109/cvpr.2013.421', '10.1016/j.imavis.2012.02.010', '10.1109/cvpr.2012.6248001', '10.1109/iccv.2011.6126383', '10.1109/cvpr.2008.4587721', '10.1109/cvpr.2013.126', '10.1109/cvpr.2001.990477', '10.1109/cvpr.2010.5540096', '10.1109/iccv.2013.58', '10.1109/tpami.2008.277', '10.1007/978-3-642-33783-3_9', '10.48550/arxiv.1306.1091', '10.1109/cvpr.2007.383035', '10.1007/3-540-63576-9_121', '10.1007/978-3-540-88682-2_8', '10.1007/978-3-540-88693-8_35', '10.1016/s0262-8856(97)00070-x', '10.1109/cvpr.2007.383048', '10.1109/iccv.2007.4409064', '10.1109/cvpr.2008.4587598', '10.1016/b978-1-4832-1446-7.50028-5', '10.1109/cvpr.2010.5539962', '10.1109/icicisys.2009.5357786', '10.1016/s0042-6989(97)00121-1', '10.1109/tpami.2008.182', '10.1007/11744023_3', '10.1007/11744085_38', '10.1007/978-3-319-00065-7_27', '10.1109/cvpr.2013.91', '10.1109/iccv.2011.6126555', '10.48550/arxiv.1307.1493', '10.1007/978-3-319-10590-1_36', '10.1007/3-540-47969-4_28', '10.1007/3-540-47969-4_9', '10.1007/3-540-47969-4_19', '10.1007/3-540-45054-8_2', '10.1016/s0262-8856(98)00124-3', '10.1109/robot.2001.932909', '10.1016/b978-0-08-051581-6.50036-2', '10.1109/irds.2002.1041393', '10.1109/cvpr.2003.1211479', '10.1109/34.391390', '10.1109/cvpr.2001.990541', '10.1109/cvpr.2000.855899', '10.1145/293347.293348', '10.1109/iccv.2015.13', '10.1007/978-3-540-88690-7_10', '10.1109/iccv.2015.320', '10.1109/iccv.2015.166', '10.1109/iccv.2015.72', '10.1145/383259.383295', '10.1109/iccv.2015.55', '10.1007/978-3-319-46493-0_35', '10.1109/tpami.2017.2699184', '10.1007/978-3-319-46448-0_48', '10.1109/cvpr.2016.278', '10.48550/arxiv.1511.07122', '10.1109/cvpr.2016.264', '10.1109/iccv.2015.167', '10.1007/978-1-4612-4380-9_41', '10.1109/iccv.2015.191', '10.48550/arxiv.1502.02734', '10.1007/978-3-642-97177-8_28', '10.1109/iccv.2015.209', '10.1109/tpami.2015.2505283', '10.48550/arxiv.1506.04579', '10.48550/arxiv.1412.7062', '10.1109/cvpr.2015.7298959', '10.1109/5.52200', '10.1109/iccv.2015.179', '10.1109/iccv.2015.203', '10.1007/978-3-642-75988-8_28', '10.1109/iccv.2015.316', '10.1109/iccv.2015.164', '10.1007/978-3-319-46466-4_5', '10.48550/arxiv.1605.02688', '10.48550/arxiv.1511.06434', '10.1007/978-3-642-33709-3_1', '10.1109/cvpr.2014.317', '10.1109/tgrs.2014.2340734', '10.1109/tpami.2015.2424863', '10.1016/j.patcog.2014.12.016', '10.1109/cvpr.2011.5995370', '10.1109/tgrs.2013.2241444', '10.1016/j.patcog.2013.09.020', '10.1109/cvpr.2014.476', '10.1109/icip.2006.313136', '10.1109/cvpr.2009.5206577', '10.1109/jstsp.2009.2015374', '10.1109/tmm.2016.2557071', '10.1109/tcyb.2015.2446755', '10.1109/tip.2016.2577886', '10.1109/tgrs.2016.2564639', '10.1007/978-3-642-15549-9_48', '10.1007/978-3-642-15555-0_12', '10.1007/978-3-642-33885-4_7', '10.1109/cvpr.2015.7299024', '10.1109/cvpr.2015.7298962', '10.1109/cvpr.2015.7298763', '10.1109/cvpr.2015.7298947', '10.1007/978-3-642-33783-3_63', '10.1109/cvpr.2015.7298862', '10.1109/cvpr.2010.5540121', '10.1109/tpami.2013.128', '10.1109/cvpr.2010.5539994', '10.1109/cvpr.2011.5995353', '10.1109/cvpr.2011.5995432', '10.1109/cvpr.2011.5995451', '10.1007/978-3-642-15561-1_32', '10.1109/iccv.2013.52', '10.1016/j.ijar.2008.11.006', '10.1109/dcc.2016.23', '10.1109/cvpr.2015.7298699', '10.48550/arxiv.1411.7923', '10.48550/arxiv.1506.07310', '10.1109/cvpr.2015.7298880', '10.1109/tifs.2014.2359577', '10.5591/978-1-57735-516-8/ijcai11-460', '10.1109/iccvw.2013.77', '10.1109/cvpr.2006.100', '10.1109/cvprw.2008.4562973', '10.1109/cvpr.2016.130', '10.1109/cvpr.2016.434', '10.1109/cvpr.2016.126', '10.48550/arxiv.1304.5634', '10.1109/cvpr.2015.7298652', '10.1109/iccv.2015.304', '10.1145/279943.279962', '10.48550/arxiv.1601.06759', '10.1007/978-3-319-46448-0_32', '10.1109/cvpr.2017.638', '10.1109/cvpr.2017.96', '10.48550/arxiv.1606.05328', '10.48550/arxiv.1511.06811', '10.48550/arxiv.1605.08803', '10.1109/cvpr.2016.309', '10.1007/11503415_11', '10.1007/978-3-319-10599-4_7', '10.1109/cvpr.2012.6247963', '10.1109/cvpr.2014.244', '10.1109/iccv.2013.21', '10.48550/arxiv.0902.1284', '10.1007/978-3-642-21569-8_27', '10.1007/11848035_41', '10.1109/tip.2015.2480599', '10.1007/978-3-642-38294-9_8', '10.1007/978-3-642-21569-8_25', '10.1109/tpami.2014.2366145', '10.1016/j.patrec.2015.01.003', '10.1109/tip.2014.2362053', '10.1145/321556.321570', '10.1006/jvci.1999.0441', '10.1109/tip.2014.2329767', '10.1016/j.patrec.2014.05.007', '10.1109/tgrs.2013.2273664', '10.1109/tip.2011.2161322', '10.1109/tgrs.2011.2160647', '10.1145/358150.358158', '10.1016/j.patcog.2011.07.017', '10.1109/lgrs.2012.2188776', '10.1006/cviu.1999.0777', '10.1109/tip.2012.2231687', '10.1016/j.isprsjprs.2013.11.003', '10.1109/icip.2010.5652595', '10.1109/83.403422', '10.1109/34.295913', '10.1109/tip.2008.2002841', '10.1109/34.88566', '10.1016/j.patcog.2011.03.025', '10.1109/tpami.2007.70817', '10.1109/34.87344', '10.1109/tpami.2006.233', '10.1109/34.546254', '10.1109/jproc.2012.2205209', '10.1109/34.969114', '10.1109/83.841934', '10.1109/83.663500', '10.1109/83.841532', '10.1145/361002.361007', '10.1109/tpami.2010.200', '10.1109/tpami.2015.2441070', '10.1109/msp.2009.934154', '10.1109/icip.2017.8297008', '10.1007/978-3-319-10578-9_24', '10.48550/arxiv.1312.4894', '10.1007/11744023_32', '10.1109/iccv.2015.465', '10.1109/cvpr.2012.6247936', '10.1109/iccv.2005.77', '10.1109/iccv.2013.441', '10.1109/cvpr.2012.6247702', '10.1109/cvpr.2006.326', '10.1109/iccv.2013.421', '10.1007/978-3-319-10599-4_44', '10.1109/icra.2016.7487517', '10.48550/arxiv.1701.01821', '10.48550/arxiv.1704.07813', '10.1109/cvpr.2016.179', '10.1007/978-3-319-10602-1_31', '10.1007/978-3-642-15552-9_29', '10.1007/978-3-319-10590-1_3', '10.1109/tpami.2012.137', '10.1109/iccvw.2011.6130384', '10.1109/cvpr.2014.416', '10.1109/cvpr.2012.6247806', '10.1109/cvpr.2014.471', '10.1109/cvpr.2011.5995318', '10.1109/cvpr.2011.5995407', '10.1109/cvpr.2008.4587756', '10.1007/11573548_1', '10.1007/978-3-319-46493-0_13', '10.48550/arxiv.1609.02612', '10.48550/arxiv.1508.07654', '10.1109/iccv.2003.1238420', '10.1007/978-3-540-72348-6_3', '10.1016/j.patcog.2015.07.006', '10.1109/icassp.2014.6853900', '10.1109/79.543975', '10.1016/j.neucom.2015.08.127', '10.1109/tkde.2014.2324592', '10.1016/j.patcog.2014.03.025', '10.1109/tpami.2012.230', '10.48550/arxiv.1509.02634', '10.1109/cvpr.2015.7298854', '10.1109/icdm.2006.160', '10.1109/cvpr.2014.243', '10.48550/arxiv.1502.00873', '10.1016/b978-0-12-802806-3.00008-7', '10.1109/tip.2016.2514498', '10.1016/j.patcog.2016.06.008', '10.48550/arxiv.1511.06241', '10.1016/j.ins.2016.02.044', '10.1109/cvpr.2010.5539957', '10.1016/j.patcog.2016.01.007', '10.1016/j.patcog.2016.06.002', '10.1016/j.patcog.2016.06.013', '10.1016/j.patcog.2016.07.026', '10.1016/j.patcog.2016.10.016', '10.1016/j.patcog.2016.10.019', '10.1016/0893-6080(90)90065-s', '10.48550/arxiv.1506.02351', '10.48550/arxiv.1507.02672', '10.48550/arxiv.1706.02677', '10.48550/arxiv.1611.09842', '10.1109/cvpr.2015.7298878', '10.1109/cvpr.2015.7299064', '10.1109/cvpr.2014.85', '10.1109/cvpr.2014.45', '10.1109/cvpr.2014.459', '10.1109/cvpr.2008.4587727', '10.1109/tmi.2004.834601', '10.1007/978-1-4757-3023-4', '10.1109/cvpr.2009.5206495', '10.1007/978-3-319-10605-2_3', '10.1109/icip.2015.7350766', '10.48550/arxiv.1605.06336', '10.48550/arxiv.1404.4661', '10.1109/cvpr.1994.323798', '10.1109/iccv.1995.466933', '10.1007/978-3-642-33765-9_33', '10.1109/cvpr.2015.7298711', '10.1109/cvpr.2015.7299060', '10.1109/cvpr.2014.309', '10.1016/j.patcog.2015.01.022', '10.1109/tpami.2016.2535231', '10.1109/tpami.2016.2636150', '10.1109/tpami.2014.2356192', '10.1109/cvpr.2015.7298906', '10.1109/cvpr.2013.271', '10.1016/j.patcog.2016.01.015', '10.1109/cvpr.2016.382', '10.1109/cvpr.2016.106', '10.1016/j.patcog.2016.07.009', '10.1007/978-3-319-46487-9_52', '10.1016/j.patcog.2017.08.026', '10.1109/tip.2016.2642781', '10.1109/cvpr.2017.106', '10.1109/cvpr.2017.687', '10.1016/j.patcog.2017.05.001', '10.48550/arxiv.1707.01691', '10.48550/arxiv.1708.02862', '10.48550/arxiv.1612.03144', '10.48550/arxiv.1703.08448', '10.48550/arxiv.1605.06409', '10.48550/arxiv.1505.01554', '10.1109/cvpr.2017.557', '10.1109/tpami.2018.2846566', '10.1109/cvpr.2016.311', '10.1007/978-3-319-10599-4_28', '10.48550/arxiv.1711.02512', '10.48550/arxiv.1208.3422', '10.1109/tip.2012.2192285', '10.1109/91.413225', '10.1016/j.patcog.2008.05.018', '10.1016/j.patcog.2013.02.015', '10.1109/cvpr.2014.242', '10.48550/arxiv.1407.4979', '10.1109/iccv.2007.4408862', '10.1109/tpami.2016.2567386', '10.1109/iccv.2017.94', '10.1016/j.patcog.2017.03.001', '10.1016/j.patcog.2017.02.032', '10.48550/arxiv.1703.07737', '10.1016/j.patcog.2017.03.030', '10.1109/cvpr.2017.237', '10.1016/j.patcog.2017.06.011', '10.1016/j.patcog.2017.09.022', '10.1016/j.patcog.2017.10.020', '10.1109/tip.2017.2782366', '10.1109/iccv.2017.555', '10.48550/arxiv.1611.00137', '10.48550/arxiv.1704.01719', '10.48550/arxiv.1511.05939', '10.1007/978-3-642-15555-0_6', '10.1109/cvpr.2013.114', '10.1109/iccv.2011.6126481', '10.1109/icme.2013.6607437', '10.1016/j.patcog.2014.10.025', '10.1109/mmul.2006.63', '10.1109/cvpr.2013.430', '10.1109/tip.2008.916999', '10.1109/cvpr.2008.4587410', '10.1109/34.868688', '10.1109/cvpr.2008.4587622', '10.1109/cvpr.2008.4587620', '10.1109/cvpr.2004.1315241', '10.1016/j.patcog.2016.06.005', '10.1016/j.patcog.2016.10.032', '10.1016/j.patcog.2016.12.005', '10.1016/j.patcog.2017.02.009', '10.1016/j.patrec.2017.10.020', '10.1109/tip.2017.2696747', '10.1016/j.patcog.2017.06.002', '10.1109/tip.2017.2745109', '10.1016/j.patrec.2018.06.033', '10.1109/cvpr.2017.775', '10.1109/cvpr.1993.341010', '10.1007/978-3-319-10590-1_4', '10.1109/tpami.2012.193', '10.1109/cvpr.2012.6247912', '10.1109/msp.2012.2211477', '10.1016/j.patrec.2011.10.018', '10.1109/tpami.2015.2408359', '10.1109/tpami.2012.48', '10.1016/j.patcog.2013.08.022', '10.1109/tpami.2011.103', '10.1109/iccv.2013.317', '10.1109/cvpr.2014.253', '10.1109/cvpr.2013.69', '10.1109/tpami.2016.2608882', '10.1109/tmm.2016.2620604', '10.1016/j.patcog.2017.02.026', '10.1016/j.patcog.2017.02.034', '10.1109/tip.2017.2678163', '10.1016/j.patcog.2017.03.004', '10.1016/j.patcog.2017.03.021', '10.1109/tits.2017.2749965', '10.1016/j.patcog.2018.01.007', '10.1016/j.neucom.2018.04.034', '10.1109/tnnls.2018.2816743', '10.1109/tnnls.2018.2874657', '10.48550/arxiv.1206.4618', '10.48550/arxiv.1407.1151', '10.1109/tip.2015.2487833', '10.1007/978-3-540-79547-6_7', '10.1109/cvpr.2015.7298606', '10.1109/cvpr.2015.7298798', '10.1109/cvpr.2015.7298731', '10.1109/cvpr.2015.7298938', '10.1109/tip.2013.2282897', '10.1109/cvpr.2014.346', '10.1109/cvpr.2012.6248030', '10.1016/j.neucom.2014.05.089', '10.1109/cvpr.2011.5995344', '10.1109/tip.2014.2383320', '10.1109/cvpr.2013.407', '10.1109/tip.2012.2199502', '10.1016/j.patcog.2008.01.026', '10.1109/cvpr.2014.43', '10.1109/tgrs.2014.2357078', '10.1109/tip.2009.2030969', '10.1109/iccv.2009.5459467', '10.1109/tpami.2007.1106', '10.1109/iccv.2013.370', '10.1109/cvpr.2012.6248093', '10.1016/j.patcog.2015.10.009', '10.1016/j.patcog.2016.01.031', '10.1109/tpami.2015.2465960', '10.1007/978-3-319-46475-6_43', '10.1109/cvpr.2016.78', '10.1016/j.patcog.2016.05.020', '10.1109/cvpr.2016.80', '10.1007/978-3-319-46493-0_50', '10.1016/j.patcog.2016.10.025', '10.1109/iccv.2009.5459296', '10.1016/j.patcog.2017.01.010', '10.1109/tpami.2018.2815688', '10.1016/j.patcog.2017.01.012', '10.1109/cvpr.2017.190', '10.1016/j.patcog.2017.05.018', '10.1016/j.patcog.2017.07.026', '10.1016/j.patcog.2017.07.028', '10.48550/arxiv.1708.00786', '10.1109/cvpr.2017.404', '10.48550/arxiv.1708.02001', '10.48550/arxiv.1708.03474', '10.1016/j.patcog.2017.10.027', '10.1016/j.patcog.2017.11.024', '10.1109/iccv.2017.62', '10.1109/iccv.2017.423', '10.1109/iccv.2017.433', '10.48550/arxiv.1802.06527', '10.1016/j.patcog.2018.03.010', '10.1016/j.patcog.2018.03.005', '10.1016/j.patcog.2018.07.006', '10.48550/arxiv.1603.01976', '10.48550/arxiv.1502.01852', '10.48550/arxiv.1503.08663', '10.48550/arxiv.1504.06375', '10.1109/iccv.2017.32', '10.1007/978-3-540-79547-6', '10.1109/tip.2016.2579306', '10.1109/tpami.2014.2345401', '10.1007/978-3-031-44137-0', '10.1016/j.patcog.2015.07.005', '10.1109/cvpr.2015.7299002', '10.1016/s0893-6080(98)00116-6', '10.1109/cvpr.2016.418', '10.1007/978-3-319-46478-7_34', '10.1007/978-0-387-21606-5', '10.48550/arxiv.1506.06579', '10.1007/978-3-642-35289-8_25', '10.1109/cvpr.1991.139758', '10.1007/978-3-319-46478-7_5', '10.1109/iccv.2013.175', '10.1109/cvpr.2012.6247727', '10.48550/arxiv.1206.6413', '10.48550/arxiv.1511.05879', '10.1109/iccv.2015.120', '10.1109/iccv.2015.19', '10.48550/arxiv.1707.05776', '10.1109/iccv.2017.628', '10.1109/cvpr.2016.320', '10.1109/cvpr.2016.556', '10.48550/arxiv.1511.06335', '10.1016/0370-2693(87)91197-x', '10.1145/318242.318443', '10.1109/cvpr.2015.7298684', '10.1109/cvpr.2015.7298701', '10.1109/cvpr.2015.7299097', '10.1109/cvpr.2008.4587569', '10.1109/tpami.2015.2396051', '10.1007/978-3-319-46478-7_30', '10.1109/iccv.2009.5459191', '10.1109/cvpr.2017.39', '10.1007/978-3-319-46487-9_48', '10.48550/arxiv.1412.6296', '10.1007/978-3-642-15567-3_11', '10.1109/cvpr.2011.5995496', '10.1109/cvpr.2010.5539773', '10.48550/arxiv.1411.4958', '10.48550/arxiv.1504.01220', '10.1109/tip.2015.2467315', '10.1109/cvpr.2015.7298930', '10.1109/iccv.2015.22', '10.1109/cvpr.2015.7298682', '10.1016/0165-1684(94)90029-9', '10.1109/iccv.2015.382', '10.48550/arxiv.1610.08904', '10.1109/allerton.2008.4797684', '10.1007/978-3-319-46454-1_44', '10.1109/cvpr.2016.641', '10.1109/lsp.2016.2603342', '10.1007/bf00994018', '10.48550/arxiv.1605.07270', '10.48550/arxiv.1611.00822', '10.48550/arxiv.1611.04831', '10.1023/b:mach.0000008084.60811.49', '10.1109/tcyb.2013.2283296', '10.1109/tcyb.2013.2285166', '10.1109/icecc.2011.6067662', '10.1109/cvpr.2013.207', '10.1109/tip.2010.2047667', '10.1090/conm/516/10172', '10.1109/tip.2011.2169273', '10.1109/tip.2015.2475625', '10.1007/978-3-642-42051-1_47', '10.1111/j.1467-9868.2012.01045.x', '10.1016/s0167-9473(96)00060-6', '10.1016/j.ijforecast.2013.04.004', '10.1016/j.jmva.2013.01.004', '10.1016/j.csda.2007.12.004', '10.1016/j.jeconom.2006.07.016', '10.1016/j.spl.2010.07.013', '10.1016/j.jmva.2010.10.012', '10.1111/j.1467-9868.2005.00503.x', '10.1111/j.1467-9868.2005.00490.x', '10.1111/j.1467-9868.2007.00577.x', '10.1007/978-3-319-13881-7', '10.1016/j.eneco.2013.09.015', '10.1016/j.ijepes.2013.04.006', '10.1016/j.eneco.2013.02.006', '10.1016/j.enpol.2008.03.035', '10.1016/j.eneco.2011.08.012', '10.1016/j.ijforecast.2008.07.005', '10.1016/j.enpol.2011.03.084', '10.1016/j.epsr.2011.06.002', '10.1016/j.enpol.2013.03.028', '10.1016/j.eneco.2008.06.003', '10.1016/j.eneco.2013.09.011', '10.1016/j.ijforecast.2008.09.008', '10.1016/j.epsr.2013.04.007', '10.1016/j.eneco.2012.04.008', '10.1016/j.ijepes.2008.09.003', '10.1016/j.eneco.2004.11.005', '10.1016/j.ijforecast.2011.02.019', '10.1109/tpwrs.2005.846054', '10.1016/j.enpol.2012.06.028', '10.1016/j.eneco.2011.12.001', '10.1109/59.221222', '10.1016/j.eneco.2011.12.006', '10.1016/j.enpol.2007.05.032', '10.1109/tsg.2010.2078842', '10.1109/tsg.2011.2171046', '10.1016/s0169-2070(00)00065-0', '10.1109/pes.2011.6039768', '10.1109/tpwrs.2009.2036017', '10.1109/pes.2011.6038881', '10.1109/pes.2009.5275308', '10.1109/tia.2009.2023569', '10.1109/tia.2009.2023571', '10.1016/j.ijforecast.2013.07.004', '10.1109/pmaps.2010.5529001', '10.1016/j.ijforecast.2013.07.002', '10.1016/j.ijforecast.2013.07.003', '10.1016/j.ijforecast.2013.07.005', '10.1109/59.910780', '10.1016/0142-0615(95)00019-4', '10.1016/s0169-2070(97)00015-0', '10.1016/s0925-2312(98)00073-3', '10.1109/tpwrs.2002.1007923', '10.1016/s0925-2312(03)00390-4', '10.1109/tpwrs.2006.889130', '10.1109/tpwrs.2005.846071', '10.1016/j.epsr.2004.10.015', '10.1109/tpwrs.2008.922249', '10.1109/tpwrs.2006.873409', '10.1109/tpwrs.2003.811010', '10.1109/tpwrs.2005.860944', '10.1109/tpwrs.2007.907583', '10.1109/tpwrs.2009.2036821', '10.1109/tpwrs.2007.908438', '10.1109/tpwrs.2004.835679', '10.1109/tpwrs.2004.835632', '10.1109/tpwrs.2009.2016470', '10.1109/tpwrs.2007.901117', '10.1109/tpwrs.2010.2065818', '10.1016/j.ijforecast.2010.04.009', '10.1175/1520-0450(1984)023<1184:tsmtsa>2.0.co;2', '10.1016/b978-1-55860-377-6.50048-7', '10.1016/0169-2070(93)90088-5', '10.1016/j.ijforecast.2012.05.001', '10.1111/j.1467-9868.2007.00607.x', '10.1111/j.1467-9868.2007.00627.x', '10.1145/301136.301186', '10.1016/0377-2217(82)90093-5', '10.1016/j.rser.2014.01.033', '10.1016/j.solener.2009.05.016', '10.1016/j.ijforecast.2014.08.008', '10.1016/j.ijforecast.2015.11.011', '10.1109/t-aiee.1944.5058843', '10.1109/tsg.2013.2278475', '10.1109/proc.1987.13927', '10.1109/59.99410', '10.1109/tsg.2015.2437877', '10.1016/j.ijforecast.2013.07.001', '10.1109/tsg.2013.2279836', '10.1016/0142-0615(91)90026-r', '10.1109/tpas.1974.293973', '10.1109/tpas.1981.316721', '10.1109/59.667379', '10.1109/tpwrs.2008.920078', '10.1109/tpwrs.2007.894843', '10.1109/drpt.2008.4523658', '10.1109/tpwrs.2010.2100050', '10.1016/j.ijforecast.2015.11.005', '10.1016/b978-1-55860-200-7.50089-1', '10.1016/0010-4485(86)90018-7', '10.1016/b978-1-55860-200-7.50086-6', '10.1016/b978-1-55860-200-7.50091-x', '10.1016/b978-1-55860-200-7.50093-3', '10.1016/b978-1-55860-200-7.50092-1', '10.1016/0022-2496(87)90031-9', '10.1016/b978-0-934613-64-4.50040-2', '10.1007/978-1-4615-4693-1_35', '10.1007/978-3-642-14400-4_44', '10.1007/3-540-39963-1_45', '10.1016/s0003-2670(03)00301-5', '10.1016/j.dss.2007.10.009', '10.1016/s0925-2312(02)00632-x', '10.1016/j.talanta.2006.10.029', '10.1016/j.ijpe.2007.05.020', '10.1109/72.846738', '10.1016/s0893-6080(03)00169-2', '10.1016/b978-1-55860-377-6.50023-2', '10.1007/bfb0017011', '10.1145/335191.335372', '10.1109/icdm.2001.989541', '10.1016/b978-1-55860-335-6.50013-1', '10.1145/42372.42377', '10.1016/s0004-3702(00)00077-1', '10.1016/s0893-6080(00)00072-1', '10.1109/69.617056', '10.1109/afgr.2000.840642', '10.1016/s0004-3702(02)00190-x', '10.1109/34.825759', '10.1109/icnn.1996.549037', '10.1109/ijcnn.2000.861337', '10.1109/ijcnn.1998.682375', '10.1109/34.58871', '10.1109/78.668782', '10.1016/s0933-3657(01)00094-x', '10.1109/72.623216', '10.1109/3477.678637', '10.1109/72.809084', '10.1109/72.80233', '10.1109/ijcnn.1998.687135', '10.1109/nnsp.1992.253679', '10.1016/s0893-6080(05)80023-1', '10.1007/3-540-48873-1_41', '10.1109/anziis.1994.396977', '10.1007/978-90-481-3751-0', '10.1007/s11263-011-0443-1', '10.1007/s11263-010-0417-8', '10.1109/cvpr.2012.6248103', '10.1016/s0304-3800(02)00257-0', '10.1016/j.cviu.2012.09.006', '10.1371/journal.pone.0038897', '10.1109/iccv.2011.6126534', '10.1016/j.neuroimage.2010.12.035', '10.48550/arxiv.0912.1128', '10.1109/tpami.2007.70822', '10.1109/iccv.2009.5459401', '10.1016/j.ecolmodel.2004.03.013', '10.1007/978-3-642-40991-2_20', '10.15607/rss.2006.ii.005', '10.1109/ijcnn.1989.118277', '10.1109/iccvw.2011.6130416', '10.1109/72.809074', '10.1109/72.286912', '10.1109/fuzzy.2005.1452427', '10.1109/5.784232', '10.48550/arxiv.1401.4112', '10.1109/cec.2010.5586158', '10.1109/tevc.2010.2104157', '10.48550/arxiv.1411.1792', '10.1109/tpami.2002.1033214', '10.1016/0262-8856(92)90015-u', '10.1109/tmm.2010.2046265', '10.1145/1275808.1276382', '10.1007/978-3-642-15561-1_56', '10.3724/sp.j.1004.2009.01257', '10.1109/cvpr.2012.6247715', '10.1109/cvpr.2010.5539920', '10.1109/cvpr.2010.5539906', '10.1109/cvpr.2012.6247930', '10.1109/tip.2010.2050625', '10.1007/11731139_1', '10.1007/3-540-49292-5_20', '10.1007/978-3-540-30116-5_34', '10.1007/3-540-45372-5_42', '10.1007/11564126_45', '10.1007/3-540-45728-3_1', '10.1007/11839088_8', '10.1007/978-3-540-45231-7_13', '10.1109/hicss.1998.648319', '10.1016/j.dss.2005.10.001', '10.1007/978-3-540-45231-7_1', '10.1016/0004-3702(90)90026-v', '10.1016/j.jss.2007.07.034', '10.1016/j.eswa.2008.08.021', '10.1016/j.ijforecast.2007.05.001', '10.1016/s0957-4174(98)00047-5', '10.1016/j.im.2008.05.005', '10.1109/tnn.2007.908641', '10.1109/icdm.2001.989510', '10.1016/s0169-023x(98)80003-7', '10.1109/tkde.2008.131', '10.1016/0004-3702(83)90016-4', '10.1007/s10994-009-5119-5', '10.1016/j.dss.2008.01.003', '10.1109/tevc.2006.890229', '10.1023/b:mach.0000008082.80494.e0', '10.1016/j.patrec.2005.10.010', '10.1109/tsmcc.2005.855493', '10.1111/j.1467-985x.2010.00646_6.x', '10.1007/978-1-4471-0409-4_3', '10.1007/bf00116835', '10.1007/bf00153759', '10.1007/bf00994659', '10.1007/3-540-45164-1_22', '10.1109/ijcnn.2005.1556388', '10.1109/icnn.1988.23855', '10.1007/3-540-52255-7_26', '10.1016/j.ins.2004.12.006', '10.1016/s0925-2312(97)00038-6', '10.1016/j.neucom.2005.04.010', '10.1016/j.neunet.2009.02.001', '10.1016/j.neunet.2006.07.005', '10.1109/52.43044', '10.1016/j.ijar.2006.04.003', '10.1016/j.ejor.2007.09.022', '10.1109/2.485895', '10.1016/j.ins.2007.01.016', '10.1016/j.neucom.2005.12.127', '10.1016/j.eswa.2007.11.024', '10.1016/j.neunet.2008.01.003', '10.1109/72.963775', '10.1007/11550907_9', '10.1109/72.728352', '10.1109/69.824621', '10.1109/tnn.2008.2005604', '10.1109/tnn.2005.863472', '10.48550/arxiv.1004.3272', '10.1109/72.572092', '10.1109/72.839008', '10.1109/tkde.2007.190671', '10.1016/0925-2312(92)90018-k', '10.1016/j.jbi.2005.02.005', '10.1016/s0933-3657(96)00367-3', '10.1007/3-540-45372-5_80', '10.1109/tevc.2012.2185846', '10.1007/s10994-010-5216-5', '10.1016/s0933-3657(98)00062-1', '10.1109/tevc.2002.802452', '10.1109/tsmcb.2011.2157681', '10.1109/ms.1986.233092', '10.1007/978-3-642-32650-9_7', '10.1016/j.cor.2006.05.004', '10.1109/cvpr.2011.5995367', '10.1016/j.ejor.2005.09.032', '10.1109/cvpr.2012.6248035', '10.1109/cvpr.2013.231', '10.1109/cvpr.2012.6247699', '10.1109/cvpr.2010.5540212', '10.1007/978-1-84882-491-1', '10.1016/j.neuroimage.2011.01.061', '10.1016/j.neuroimage.2013.07.079', '10.1016/j.pneurobio.2005.10.003', '10.1016/j.neuroimage.2010.05.051', '10.1016/j.neuroimage.2008.04.246', '10.1016/j.neuroimage.2010.09.003', '10.3389/fnins.2012.00162', '10.1109/79.962275', '10.1016/j.neuroimage.2010.07.033', '10.1007/s10994-009-5153-3', '10.1016/j.neuroimage.2008.08.020', '10.1016/j.neuroimage.2012.04.015', '10.1016/j.neuroimage.2011.01.057', '10.1016/0013-4694(95)00083-b', '10.1016/j.neuroimage.2010.07.073', '10.1109/rbme.2011.2170675', '10.1016/j.neuroimage.2010.11.004', '10.1016/0013-4694(74)90224-7', '10.1016/j.neuroimage.2012.09.036', '10.1016/j.neuroimage.2012.12.051', '10.1109/msp.2008.4408441', '10.1016/j.neuroimage.2010.07.035', '10.1109/msp.2008.4408447', '10.1109/tap.1986.1143830', '10.1016/j.aap.2009.06.007', '10.1109/tbme.2010.2046325', '10.1186/1471-2105-7-s1-s9', '10.1016/s0925-2312(02)00821-4', '10.1109/72.761722', '10.1016/j.neuroimage.2005.05.032', '10.1016/j.neuroimage.2010.06.048', '10.1016/0165-1838(82)90067-4', '10.1016/j.mri.2009.12.016', '10.1016/0013-9351(81)90173-0', '10.1007/11823728_26', '10.1007/3-540-44533-1_51', '10.1007/3-540-59286-5_57', '10.1016/j.ijhcs.2006.06.007', '10.1016/s0377-2217(02)00792-0', '10.1016/1042-8143(92)90004-k', '10.1016/0378-7206(94)90010-8', '10.1016/s0020-7373(84)80037-1', '10.1145/5465.5469', '10.1109/tsmc.1982.4308848', '10.1109/5254.850821', '10.1145/7538.7545', '10.1016/s0004-3702(99)00013-2', '10.1145/62959.62971', '10.1016/0169-023x(94)00020-4', '10.1016/j.dss.2005.05.008', '10.1016/0749-5978(86)90044-0', '10.1145/129617.129621', '10.1016/j.dss.2008.06.013', '10.1145/175276.175283', '10.1145/5465.5470', '10.1016/s0167-9236(99)00014-7', '10.1109/hicss.1993.284327', '10.1109/69.774103', '10.1016/s0378-7206(99)00051-8', '10.1109/icif.2005.1592040', '10.1109/tevc.2002.806857', '10.1016/s0010-4655(98)00168-4', '10.1016/0304-3800(81)90008-9', '10.1016/0951-8320(96)00002-6', '10.1016/s0010-4655(01)00159-x', '10.1016/s0010-4655(98)00152-0', '10.1016/s0306-4379(03)00072-3', '10.1145/335191.335438', '10.1145/342009.335438', '10.1007/3-540-48751-4_3', '10.1016/b978-1-55860-247-2.50066-8', '10.1007/1-4020-2673-0_15', '10.1007/3-540-45153-6_14', '10.1016/b978-1-55860-335-6.50016-7', '10.1007/978-3-540-74976-9_58', '10.1007/3-540-36182-0_16', '10.1007/978-3-540-87479-9_38', '10.1007/978-3-662-04923-5', '10.1007/bfb0095128', '10.1186/1471-2105-6-136', '10.1016/s0933-3657(00)00053-1', '10.1016/s0020-7373(87)80003-2', '10.1016/s0933-3657(00)00110-x', '10.1145/112646.112684', '10.1109/tkde.2005.50', '10.1016/j.ijmedinf.2006.11.006', '10.1007/978-3-540-74958-5_39', '10.1186/1471-2105-3-11', '10.1007/s10994-010-5227-2', '10.1109/tcbb.2008.47', '10.1016/j.ipm.2009.03.002', '10.1016/j.ejor.2011.11.022', '10.1007/11415770_30', '10.1016/j.aca.2007.03.023', '10.1186/1471-2105-8-s10-s7', '10.1186/gb-2002-3-12-research0087', '10.1186/1471-2105-5-169', '10.1016/b978-1-55860-200-7.50040-4', '10.1007/978-3-642-77393-8_43', '10.1007/978-3-540-74958-5_31', '10.1016/s0167-7152(96)00140-x', '10.1136/bmj.294.6568.328', '10.1016/0271-5384(81)90069-7', '10.1007/978-1-4615-5775-3', '10.1007/bfb0055937', '10.1007/978-1-4757-4137-7', '10.1007/978-1-4757-4137-7_33', '10.1016/s0167-739x(00)00043-1', '10.1016/j.eswa.2006.09.038', '10.1016/0022-4359(93)90003-2', '10.1016/s0377-2217(03)00069-9', '10.1016/s0148-2963(98)00060-5', '10.1016/j.cor.2003.11.018', '10.1016/j.asoc.2006.01.005', '10.1016/j.eswa.2008.05.027', '10.1016/s0957-4174(02)00030-1', '10.1016/s0957-4174(03)00133-7', '10.1016/j.eswa.2007.12.049', '10.1109/3477.484436', '10.1016/s0167-6245(99)00015-3', '10.1016/j.ejor.2003.12.010', '10.1109/tevc.2003.819264', '10.1016/j.eswa.2005.04.043', '10.1109/72.846740', '10.1016/j.asoc.2009.02.014', '10.1007/978-1-4615-5775-3_20', '10.1109/iat.2003.1241052', '10.1016/j.eswa.2005.09.080', '10.1016/j.asoc.2008.03.002', '10.1007/bf00993277', '10.1109/tevc.2016.2612650', '10.1007/0-387-27255-0_9', '10.1109/tit.2007.909108', '10.48550/arxiv.1306.5860', '10.1016/j.psychres.2010.12.001', '10.1016/b978-1-55860-200-7.50134-3', '10.1016/b978-1-55860-200-7.50097-0', '10.1109/64.248354', '10.1016/0165-0114(94)90242-9', '10.1016/0004-3702(77)90003-0', '10.1109/21.299696', '10.1016/0165-0114(94)90025-6', '10.1109/72.159069', '10.1109/icnn.1993.298658', '10.1109/ijcnn.1992.287212', '10.1007/bf00993103', '10.1016/j.engappai.2008.05.012', '10.1016/s0167-7152(02)00243-2', '10.1109/tit.1968.1054155', '10.1007/978-3-0348-8930-8_16', '10.1109/tpami.2009.164', '10.1109/tit.1967.1053964', '10.1109/tevc.2003.819265', '10.1145/285055.285059', '10.1007/11841036_43', '10.1016/j.patcog.2006.04.005', '10.1109/tnn.2009.2018547', '10.1016/j.datak.2006.01.008', '10.1007/978-1-84628-172-3', '10.1007/978-3-642-56927-2', '10.1007/978-3-642-33486-3_42', '10.1007/978-3-540-39907-0_32', '10.1186/1471-2105-8-25', '10.1007/978-3-642-40988-2_22', '10.1016/j.ijar.2006.01.004', '10.1016/j.ijar.2007.10.004', '10.1109/icter.2013.6761181', '10.1186/1471-2105-14-119', '10.1109/icdmw.2009.51', '10.1186/1471-2105-9-307', '10.1016/0095-0696(78)90006-2', '10.1007/978-1-4612-0919-5_20', '10.1016/s0378-4754(02)00253-7', '10.1016/s0378-4754(00)00270-6', '10.1109/ijcnn.1992.287101', '10.1109/cogann.1992.273940', '10.1016/0020-0190(76)90095-8', '10.1145/240455.240463', '10.1109/34.589207', '10.2202/1557-4679.1008', '10.1515/ijb', '10.1007/3-540-28803-1_10', '10.1016/s0736-5853(86)80073-9', '10.1109/c-m.1976.218587', '10.1109/nnsp.1999.788121', '10.1016/0004-3702(85)90067-0', '10.1007/978-1-4757-3264-1', '10.1016/s0304-3800(99)00108-8', '10.1016/s0304-3800(98)00149-5', '10.1016/s0304-3800(99)00092-7', '10.1016/0954-1810(94)00011-s', '10.1016/s0304-3800(96)01913-8', '10.1016/s0304-3800(99)00111-8', '10.1016/s0304-3800(99)00103-9', '10.1016/s0169-5347(00)89026-6', '10.1016/s0304-3800(99)00106-4', '10.1016/0304-3800(95)00142-5', '10.1016/s0764-4469(98)80307-7', '10.1016/s0304-3800(99)00099-x', '10.1016/s0304-3800(01)00294-0', '10.1016/s0304-3800(99)00110-6', '10.1016/s0304-3800(99)00113-1', '10.1016/s0304-3800(99)00112-x', '10.1007/978-1-4471-3087-1', '10.1007/978-3-642-57030-8', '10.1016/j.ins.2005.03.012', '10.1109/icdmw.2008.117', '10.48550/arxiv.1504.07614', '10.1016/0743-1066(94)90035-3', '10.1145/233269.233311', '10.1109/cvpr.2007.383171', '10.1109/69.846291', '10.1007/s10994-015-5529-5', '10.1007/978-3-7091-2668-4_10', '10.1007/bf00058655', '10.1007/bf00058680', '10.1038/76615', '10.1186/gb-2000-1-5-reviews0005', '10.1523/jneurosci.20-11-04069.2000', '10.1007/978-3-642-20267-4_3', '10.1007/978-0-387-21706-2', '10.1016/s0167-9473(01)00065-2', '10.1371/journal.pone.0083875', '10.1109/tvcg.2010.161', '10.1007/978-0-387-84858-7', '10.1145/1045343.1045371', '10.1016/b978-0-12-737550-2.50019-1', '10.1007/bf00116251', '10.48550/arxiv.cs/0504042', '10.1016/s1088-467x(98)00023-7', '10.1016/0377-2217(85)90321-2', '10.1016/j.ins.2010.02.023', '10.1016/j.artmed.2005.07.006', '10.1016/j.csda.2007.02.013', '10.1016/j.dss.2009.05.016', '10.1109/ijcnn.2002.1005488', '10.1109/cidm.2011.5949423', '10.1016/j.eswa.2011.01.174', '10.1016/j.ins.2010.11.023', '10.1109/titb.2007.902300', '10.1109/wsc.2007.4419667', '10.1016/s0898-1221(96)90007-3', '10.1007/978-3-642-23184-1', '10.5167/uzh-63130', '10.1007/978-3-642-30487-3_4']\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "l = []\n",
    "for paper in papers[:]:\n",
    "    if paper.get('text', '') != '':\n",
    "        count+=1\n",
    "        l.append(paper.get('doi'))\n",
    "print(count)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "847c24d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ieee_dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d155ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c71ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Replace with your university name\n",
    "university_name = 'University of Michigan - Ann Arbor'\n",
    "\n",
    "# Function to handle the initial navigation to the Institutional Sign In page\n",
    "def institutional_login(driver):\n",
    "    driver.get('https://ieeexplore.ieee.org/Xplore/home.jsp')\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Click the 'Institutional Sign In' button\n",
    "    inst_sign_in_button = driver.find_element(By.LINK_TEXT, 'Institutional Sign In')\n",
    "    inst_sign_in_button.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    # At this point, the user can manually select the university and enter credentials\n",
    "    print(\"Please select your university and complete the login manually (including Duo authentication if required).\")\n",
    "    input(\"Press Enter here after you have fully logged in...\")  # Wait for manual completion\n",
    "\n",
    "# Function to check if the PDF link is accessible\n",
    "def is_pdf_accessible(pdf_link):\n",
    "    try:\n",
    "        response = requests.head(pdf_link, timeout=5)\n",
    "        content_type = response.headers.get('Content-Type', '')\n",
    "        return 'pdf' in content_type  # Check if Content-Type includes 'pdf'\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error checking URL {pdf_link}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Main function to download IEEE PDFs for multiple DOIs\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# Main function to download IEEE PDFs for multiple DOIs\n",
    "def download_ieee_pdfs(driver, dois):\n",
    "    for doi in dois:\n",
    "        # Navigate to the DOI link\n",
    "        url = f\"https://doi.org/{doi}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        current_url = driver.current_url\n",
    "        if \"ieeexplore.ieee.org\" in current_url:\n",
    "            try:\n",
    "                # Extract the document number and construct the PDF link (stamp page link)\n",
    "                doc_number = current_url.split(\"/\")[-1]\n",
    "                # pdf_link = f\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber={doc_number}\"\n",
    "                pdf_link = f\"https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber={doc_number}&ref=\"\n",
    "                print(f\"Attempting to access the stamp page for DOI {doi}...\")\n",
    "\n",
    "                # Navigate to the stamp page\n",
    "                driver.get(pdf_link)\n",
    "                time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "                # Look for the \"Open\" button on the page\n",
    "                try:\n",
    "                    open_button = driver.find_element(By.XPATH, \"//button[contains(text(), 'Open')]\")\n",
    "                    open_button.click()\n",
    "                    print(f\"Clicked 'Open' button to download PDF for DOI {doi}\")\n",
    "                    retrieved.append(doi)\n",
    "                    time.sleep(3)  # Wait for download to start\n",
    "                except NoSuchElementException:\n",
    "                    print(f\"No 'Open' button found on page for DOI {doi}. PDF might have been downloaded automatically.\")\n",
    "                    retrieved.append(doi)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while attempting to download the PDF for DOI {doi}: {e}\")\n",
    "                not_retrieved.append(doi)\n",
    "\n",
    "                \n",
    "# Initialize WebDriver with download preferences\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless')\n",
    "options.add_argument(\n",
    "'user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36')\n",
    "\n",
    "options.add_argument(\"enable-automation\");\n",
    "options.add_argument(\"--window-size=1920,1080\");\n",
    "options.add_argument(\"--no-sandbox\");\n",
    "options.add_argument(\"--disable-extensions\");\n",
    "options.add_argument(\"--dns-prefetch-disable\");\n",
    "options.add_argument(\"--disable-gpu\");\n",
    "\n",
    "\n",
    "options.add_experimental_option('prefs', {      \n",
    "\"download.default_directory\": \"/Users/rishikesh/Downloads/try_2\",  # Papers will be downloaded at this address\n",
    "\"download.prompt_for_download\": False,  \n",
    "\"download.directory_upgrade\": True,\n",
    "\"plugins.always_open_pdf_externally\": True  \n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Perform the initial institutional login\n",
    "institutional_login(driver)\n",
    "\n",
    "# Download multiple PDFs without re-logging in\n",
    "download_ieee_pdfs(driver, ieee_dois)\n",
    "\n",
    "# Close the driver after completing all downloads\n",
    "# driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd578b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f07d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e43134e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fca9b4e-b2a6-411f-b3ac-47e2d52069bc",
   "metadata": {},
   "source": [
    "#### Sample usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f533a0-bee2-4c6d-8653-641fc08707d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Retrieved {len(retrieved)} papers. {len(not_retrieved)} papers unretrieved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c538e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a442fc-6abe-4942-8d47-f3fee4ef982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_retrieved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
