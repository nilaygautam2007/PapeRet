{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8c213e-39f1-440f-88f4-6c4f0768c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    # import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from document_preprocessor import RegexTokenizer\n",
    "from indexing import Indexer, IndexType, BasicInvertedIndex\n",
    "from ranker import Ranker, WordCountCosineSimilarity, DirichletLM, BM25, PivotedNormalization, TF_IDF, CubeRootRanker\n",
    "\n",
    "import relevance\n",
    "\n",
    "import json\n",
    "import ast\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c379efb-c5b4-44d8-8ae4-1bae6ebfe2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100024it [00:25, 3902.34it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "arxiv = set()\n",
    "dois = set()\n",
    "new_docs = []\n",
    "with open('all_papers.jsonl', 'r') as file:\n",
    "    idx=0\n",
    "    for line_num, line in enumerate(tqdm(file)):\n",
    "        idx+=1\n",
    "        document = json.loads(line.strip())\n",
    "        if document.get('doi', '') == '':\n",
    "            arxiv.add(document.get('arxiv_id', '')[0:10])\n",
    "        else:\n",
    "            doi = document.get('doi', '')\n",
    "            if doi in dois:\n",
    "                continue\n",
    "            dois.add(doi)\n",
    "            \n",
    "            if 'arxiv' in doi:\n",
    "                doi_2 = doi.split(\"/\")[1][6:]\n",
    "            if doi_2 in arxiv:\n",
    "                continue\n",
    "            \n",
    "        \n",
    "        new_docs.append(document)    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "763fa70d-f816-4ea2-9ac7-88b5a5a52b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98503"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2f98e79-511f-4ee4-834c-0d505701454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing the file.\n",
      "Combined data has been written to all_papers.jsonl\n"
     ]
    }
   ],
   "source": [
    "# file_paths = [\"arxiv_papers.json\", \"arxiv_papers_pt2.json\", \"OpenAlex_Text_Depth-3.json\"]\n",
    "output_file = \"all_papers.jsonl\"\n",
    "\n",
    "# combined_data = []\n",
    "\n",
    "# for file_path in file_paths:\n",
    "#     print(f'Reading {file_path}')\n",
    "#     with open(file_path, \"r\") as file:\n",
    "#         data = json.load(file)\n",
    "        \n",
    "#         if file_path == \"OpenAlex_Text_Depth-3.json\":\n",
    "#             data = [item for item in data if \"text\" in item and item[\"text\"].strip()]\n",
    "#             print(len(data))\n",
    "        \n",
    "#         combined_data.extend(data)\n",
    "\n",
    "print('Writing the file.')\n",
    "with open(output_file, \"w\") as jsonl_file:\n",
    "    for record in new_docs:\n",
    "        jsonl_file.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "print(f\"Combined data has been written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c76973-7484-41e8-bd7f-5bcfb40f9eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3abfe9e1-4baf-4f87-8e46-366730005fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90645"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "54545 + 36100 #+ 9471"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f364967b-3bb1-4826-83f6-4abcc0e577af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bb5665-303c-443e-a050-4a791619e62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ff57d0-7f3c-4e13-9c50-a76a82c2ecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import document_preprocessor\n",
    "reload(document_preprocessor)\n",
    "from document_preprocessor import RegexTokenizer\n",
    "\n",
    "\n",
    "import indexing\n",
    "reload(indexing)\n",
    "from indexing import Indexer, IndexType, BasicInvertedIndex\n",
    "\n",
    "\n",
    "\n",
    "import ranker\n",
    "reload(ranker)\n",
    "from ranker import Ranker, WordCountCosineSimilarity, DirichletLM, BM25, PivotedNormalization, TF_IDF, CubeRootRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930e2eb-561c-405c-8446-c560e36ae9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f0e99-ba75-42d8-8dc7-c0cf3cdca1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5241d-37a4-4d59-97d2-8dc5e3be1192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b58116d-f0f9-4a24-ae19-478f4fa3fe53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 documents done.\n",
      "20000 documents done.\n",
      "30000 documents done.\n",
      "40000 documents done.\n",
      "50000 documents done.\n",
      "60000 documents done.\n",
      "70000 documents done.\n",
      "80000 documents done.\n",
      "90000 documents done.\n"
     ]
    }
   ],
   "source": [
    "docid_authors_map = {}\n",
    "with open('all_papers.jsonl', 'r') as file:\n",
    "    idx=0\n",
    "    for line_num, line in enumerate(file):\n",
    "        idx+=1\n",
    "        # if idx>90643:\n",
    "        document = json.loads(line.strip())\n",
    "        a = document.get('authors', [])\n",
    "        # print(isinstance(a, list))\n",
    "        if isinstance(a, list):\n",
    "            docid_authors_map[idx] = [author.lower() for author in a]\n",
    "        else:\n",
    "            docid_authors_map[idx] = [author.lower() for author in ast.literal_eval(a)]\n",
    "        if idx%10000 == 0:\n",
    "            print(f'{idx} documents done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee988b-5d6d-4565-8e56-01239a87db9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b06bbaf-5110-4e31-a729-47c7bc714a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['steffen rendle',\n",
       " 'christoph freudenthaler',\n",
       " 'zeno gantner',\n",
       " 'lars schmidt-thieme']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docid_authors_map[90700] ## SAVE THIS MAPPING DICT!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c92ec7d0-5c8c-42b5-b18b-fe0b9b13dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('docid_authors_map.pkl', 'wb') as file:\n",
    "    pickle.dump(docid_authors_map, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7fb6f87-b205-4634-bbfc-76443baea7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_authors = set()\n",
    "\n",
    "for idx, auth in docid_authors_map.items():\n",
    "    for a in auth:\n",
    "        all_authors.add(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed0e15c0-04b9-4ad2-91dc-454d16827d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['andy brock',\n",
       " 'yifei xiao',\n",
       " 'jeremy huckins',\n",
       " 'gyan tatiya',\n",
       " 'suho yoo',\n",
       " 'janet van niekerk',\n",
       " 'junhan chang',\n",
       " 'finn verner jensen',\n",
       " 'andy way',\n",
       " 'liping si',\n",
       " 'vincenzo schininà',\n",
       " 'taylor henderson',\n",
       " 'karn seth',\n",
       " 'yuxing hu',\n",
       " 'lin gu',\n",
       " 'phillip b. levine',\n",
       " 'sebastian tschiatschek',\n",
       " 'sardana ivanova',\n",
       " 'chang chen',\n",
       " 'hannah metzler',\n",
       " 'shari liu',\n",
       " 'lars johan ersland',\n",
       " 'vivienne baillie gerritsen',\n",
       " 'ashley wu',\n",
       " 'per sidén',\n",
       " 'tanner schmidt',\n",
       " 'magnus rattray',\n",
       " 'ansgar philippsen',\n",
       " 'abu layek',\n",
       " 'elli heyes',\n",
       " 'hanjun dai',\n",
       " 'soohyun kim',\n",
       " 'jesper b. boldt',\n",
       " 'c. b. simone',\n",
       " 'longjun cai',\n",
       " 'drew van camp',\n",
       " 'yuan xu',\n",
       " 'yasir glani',\n",
       " 'yadira ortiz castellano',\n",
       " 'phillip maffettone',\n",
       " 'mirza mohtashim alam',\n",
       " 'leticia maria paz de lima',\n",
       " 'jiandong zhang',\n",
       " 'heather domin',\n",
       " 'hanyun yin',\n",
       " 'chungyi lin',\n",
       " 'emma schwartzman',\n",
       " 'chenfeng cao',\n",
       " 'rohit mishra',\n",
       " 'yulong mao',\n",
       " 'qinggang zhou',\n",
       " 'yinan peng',\n",
       " 'marvin schiller',\n",
       " 'laxmi kant',\n",
       " 'dalton jones',\n",
       " 'nikos k. logothetis',\n",
       " 'engin mendi',\n",
       " 'weixiong zhang',\n",
       " 'm.t. orchard',\n",
       " 'vahdat abdelzad',\n",
       " 'xingyu cai',\n",
       " 'jason pelecanos',\n",
       " 'yongjia xu',\n",
       " 'andreas gruebl',\n",
       " 'emilia apostolova',\n",
       " 'mlađan jovanović',\n",
       " 'andrew howes',\n",
       " 'sebastian macaluso',\n",
       " 'jorge calvo-zaragoza',\n",
       " 'roberto bifulco',\n",
       " 'anh vu luong',\n",
       " 'ferdous ahmed barbhuiya',\n",
       " 'h. muirhead',\n",
       " 'marta federici',\n",
       " 'julien lavauzelle',\n",
       " 'myrl g. marmarelis',\n",
       " 'himangi mittal',\n",
       " 'pier‐francesco fazzini',\n",
       " 'hong-zheng shi',\n",
       " 'katharina schneider',\n",
       " 'shihang wang',\n",
       " 'xiangchao meng',\n",
       " 'renārs liepiņš',\n",
       " 'zhuolin he',\n",
       " 'yongqi li',\n",
       " 'jenny benois-pineau',\n",
       " 'robert praas',\n",
       " 'genevieve gorrell',\n",
       " 'fa wu',\n",
       " 'nayan jyoti kalita',\n",
       " 'anna volkova',\n",
       " 'anthony philippakis',\n",
       " 'elise cappella',\n",
       " 'hamid majidi balanji',\n",
       " 'mohsen heidari khoozani',\n",
       " 'yanan chen',\n",
       " 'rashedur m. rahman',\n",
       " 'carlos d. castillo',\n",
       " 'yu hirate',\n",
       " 'leyi wei',\n",
       " 'guillaume baudart',\n",
       " 'nicola ferro',\n",
       " 'jack e. dixon',\n",
       " 'ali taylan cemgil',\n",
       " 'can zhang',\n",
       " 'gao weiguo',\n",
       " 'anna felipe',\n",
       " 'benjamin blankertz',\n",
       " 'victor h. moll',\n",
       " 'si-yuan cao',\n",
       " 'zhonglei wang',\n",
       " 'emmanuel cohen',\n",
       " 'saurabh amin',\n",
       " 'megan e. schwamb',\n",
       " 'zejun gong',\n",
       " 'nila mandal',\n",
       " 'zikang leng',\n",
       " 'yiting wu',\n",
       " 'hyemin gu',\n",
       " 'allan haldane',\n",
       " 'ilan kroo',\n",
       " 'adam godzik',\n",
       " 'si-guo fang',\n",
       " 'yuchuan wu',\n",
       " 'tongyang xu',\n",
       " 'masanobu abe',\n",
       " 'huichi zhou',\n",
       " 'ahmed fahmy',\n",
       " 'tanja kortemme',\n",
       " 'hamid mohayeji',\n",
       " 'hans-georg fill',\n",
       " 'courtney d. corley',\n",
       " 'jan-peter sowa',\n",
       " 'nicolas b. cowan',\n",
       " 'alexander warstadt',\n",
       " 'richard mccreadie',\n",
       " 'sung-ju hwang',\n",
       " 'jiankun hu',\n",
       " 'zhuoxuan jiang',\n",
       " 'xiaofeng qian',\n",
       " 'jeremy gordon',\n",
       " 'ruijie ma',\n",
       " 'soeren auer',\n",
       " 'sushovan majhi',\n",
       " 'jianhai su',\n",
       " 'john m. hoopes',\n",
       " 'arjuna flenner',\n",
       " 'anshu bhatia',\n",
       " 'brian yang',\n",
       " 'v. i. norkin',\n",
       " 'demjan grubic',\n",
       " 'samuel neumann',\n",
       " 'donya saless',\n",
       " 'thomas decker',\n",
       " 'emre gures',\n",
       " 'hsiu-chuan chen',\n",
       " 'jacob lichtefeld',\n",
       " 'pratyush dhingra',\n",
       " 'hamish ivey-law',\n",
       " 'mike zajko',\n",
       " 'andrew w. stephan',\n",
       " 'zachary pardos',\n",
       " 'shicheng xu',\n",
       " 'anna s. bosman',\n",
       " 'leon lan',\n",
       " 'liang ma',\n",
       " 'jesper dramsch',\n",
       " 'jason taylor',\n",
       " 'tobias friedrich',\n",
       " 'shahar azulay',\n",
       " 'haocheng feng',\n",
       " 'christopher forster',\n",
       " 'nguyen hung-quang',\n",
       " 'neda navidi',\n",
       " 'mingyu yan',\n",
       " 'geoffrey goh',\n",
       " 'dave cliff',\n",
       " 'antoine bordes',\n",
       " 'bram wallace',\n",
       " 'sumanth prabhu',\n",
       " 'xiaocui yang',\n",
       " 'alexey a. lukashin',\n",
       " 'pavlo liashchynskyi',\n",
       " 'shiyue yang',\n",
       " 'xin quan',\n",
       " 'andrei linde',\n",
       " 'vasyl s. tiberkevich',\n",
       " 'chengcan ying',\n",
       " 'hsingyu chen',\n",
       " 'laura schelenz',\n",
       " 'véronique hoste',\n",
       " 'maryline chen',\n",
       " 'michel bierlaire',\n",
       " 'renhao tan',\n",
       " 'timo müller',\n",
       " 'tianyu jia',\n",
       " 'igor filippov',\n",
       " 'seunghyun kim',\n",
       " 'zhenya huang',\n",
       " 'rabah abdul khalek',\n",
       " 'faruk polat',\n",
       " 'linda doyle',\n",
       " 'stephan zheng',\n",
       " 'andrea gentili',\n",
       " 'robert yang',\n",
       " 'peter hahn',\n",
       " 'seonmin rhee',\n",
       " 'warren he',\n",
       " 'sreejita ghosh',\n",
       " 'soumava kumar roy',\n",
       " 'masaya hibino',\n",
       " 'alessandro epasto',\n",
       " 'tianhao lai',\n",
       " 'jan buethe',\n",
       " 'thomas dooms',\n",
       " 'moshe mandel',\n",
       " 'pingjie wang',\n",
       " 'petter ögren',\n",
       " 'mengmeng zhang',\n",
       " 'alistair carson',\n",
       " 'mazin g. rahim',\n",
       " 'yi meng',\n",
       " 'haruka asanuma',\n",
       " 'peter kaever',\n",
       " 'jan silovsky',\n",
       " 'p.j.g. butler',\n",
       " 'jiaming tian',\n",
       " 'kenji yasuoka',\n",
       " 'jennifer dy',\n",
       " 'karol chlasta',\n",
       " 'shaan aryaman',\n",
       " 'song xiyu',\n",
       " 'dongdong yu',\n",
       " 'xinyuan chang',\n",
       " 'jackie chi kit cheung',\n",
       " 'james f fitzgerald',\n",
       " 'alba herrera-palacio',\n",
       " 'yijia liu',\n",
       " 'p g kubendran amos',\n",
       " 'jinfeng zhang',\n",
       " 'bart kranstauber',\n",
       " 'mengduo yang',\n",
       " 'jaloliddin rajabov',\n",
       " 'jeremy l. wyatt',\n",
       " 'pei lv',\n",
       " 'doseok jang',\n",
       " 'peter schüller',\n",
       " 'rajika l. perera',\n",
       " 'laurent vanbever',\n",
       " 'xueping wang',\n",
       " 'jaemin kang',\n",
       " 'a. zhao',\n",
       " 'liqun cheng',\n",
       " 'andrey krutov',\n",
       " 'patrice abry',\n",
       " 'huaren qu',\n",
       " 'william f. shen',\n",
       " 'ziming fu',\n",
       " 'c. a. knoblock',\n",
       " 'leo peckham',\n",
       " 'noa yehezkel lubin',\n",
       " 'takumi gomyou',\n",
       " 'guang chen',\n",
       " 'samuel sokota',\n",
       " 'jagadeesh balam',\n",
       " 'mark squillante',\n",
       " 'rashi goel',\n",
       " 'dequan zhou',\n",
       " 'yarah basyoni',\n",
       " 'fisher yu',\n",
       " 'sophie clavier',\n",
       " 'matthieu constant',\n",
       " 'odinaldo rodrigues',\n",
       " 'johann l. hurink',\n",
       " 'fuchen zheng',\n",
       " 'françois schnitzler',\n",
       " 'milco wansleeben',\n",
       " 'mario linares-vásquez',\n",
       " 'michael danner',\n",
       " 'carsten monka-ewe',\n",
       " 'raphael schiffmann',\n",
       " 'george fedoseev',\n",
       " 'yoav kantor',\n",
       " 'michalis faloutsos',\n",
       " 'gregory valiant',\n",
       " 'yunhao ba',\n",
       " 'carmadi machbub',\n",
       " 'shintaro ishikawa',\n",
       " 'andrea campagner',\n",
       " 'david hall',\n",
       " 'tetsuya sakai',\n",
       " 'nihal john george',\n",
       " 'alena shilova',\n",
       " 'bria long',\n",
       " 'grigore pintilie',\n",
       " 'félix doublet',\n",
       " 'yannis kalaidzidis',\n",
       " 'zhonghui gu',\n",
       " 'zhengfei wang',\n",
       " 'cecilia sönströd',\n",
       " 'chaoxiong ye',\n",
       " 'noriki nishida',\n",
       " 'salih selek',\n",
       " 'yongtao wang',\n",
       " 'yunfeng li',\n",
       " 'yunhan yang',\n",
       " 'spencer whitehead',\n",
       " 'augusto sarti',\n",
       " 'jirko rubruck',\n",
       " 'clemens schulze‐briese',\n",
       " 'kevin nassisid',\n",
       " 'giacomo meanti',\n",
       " 'yixiao yang',\n",
       " 'alexander fichtl',\n",
       " 'abhijit das',\n",
       " 'lukas cavigelli',\n",
       " 'hrishikesh aradhye',\n",
       " 'amos ng',\n",
       " 'gang song',\n",
       " 'leslee j shaw',\n",
       " 'paul catley',\n",
       " 'blaise thomson',\n",
       " 'hector n. b. pinheiro',\n",
       " 'abhishri ajit medewar',\n",
       " 'christian pehle',\n",
       " 'hassan alkhayuon',\n",
       " 'jie an',\n",
       " 'shanshan wan',\n",
       " 'fabio parmeggiani',\n",
       " 'corentin dancette',\n",
       " 'j. mcginn',\n",
       " 'sanjeev muralikrishnan',\n",
       " 'shuang zhang',\n",
       " 'iain phillips',\n",
       " 'sanil v',\n",
       " 'xuming lin',\n",
       " 'venkateswara rao kagita',\n",
       " 'yulan he',\n",
       " 'jonathan gair',\n",
       " 'greg brockman',\n",
       " 'ana f. loureiro',\n",
       " 'xinxing zu',\n",
       " 'v. lanzetta',\n",
       " 'liqiu meng',\n",
       " 'david r. hardoon',\n",
       " 'yan di',\n",
       " 'vincent fortuin',\n",
       " 'william s. evans',\n",
       " 'diego blas',\n",
       " 'mehdi miah',\n",
       " 'rui ning',\n",
       " 'sridhar suresh ragupathi',\n",
       " 'sayed anisul hoque',\n",
       " 'mathieu duchesneau',\n",
       " 'c. scimiterna',\n",
       " 'oleg tchernyshyov',\n",
       " 'maria ulan',\n",
       " 'david e. bernal neira',\n",
       " 'helmut griesser',\n",
       " 'vinay',\n",
       " 'mahdi torabi rad',\n",
       " 'paola malsot',\n",
       " 'xing yi liu',\n",
       " 'rohan sharma',\n",
       " 'michael miller yoder',\n",
       " 'zhuolin gao',\n",
       " 'sagi perel',\n",
       " 'yujia huang',\n",
       " 'stanley e. lazic',\n",
       " 'akifumi imanishi',\n",
       " 'kuan-chuen wu',\n",
       " 'ran levy',\n",
       " 'steven meyer',\n",
       " 'stephan balduin',\n",
       " 'anqi lin',\n",
       " 'james fairbanks',\n",
       " 'kianté brantley',\n",
       " 'thomas carette',\n",
       " 'lorenzo lamberti',\n",
       " 'lijun sun',\n",
       " 'e. a. zhukovskaya',\n",
       " 'caroline m. gevaert',\n",
       " 'tibor vellai',\n",
       " 'kaiqiang qi',\n",
       " 'changyeon park',\n",
       " 'zachary nado',\n",
       " 'lucian covaci',\n",
       " 'dohun lim',\n",
       " 'bishwajit purkaystha',\n",
       " 'farnoush banaei-kashani',\n",
       " 'dimitri percia david',\n",
       " 'najlaal-nabhan',\n",
       " 'sebastian dalleiger',\n",
       " 'ilya sutskever',\n",
       " 'dirk hovy',\n",
       " 'james a boehm iii',\n",
       " 'minjung shin',\n",
       " 'atul sahay',\n",
       " 'john joon young chung',\n",
       " 'kateryna lutsai',\n",
       " 'meiqian jiang',\n",
       " 'sunghyun baek',\n",
       " 'bruno sinopoli',\n",
       " 'sukhyun cho',\n",
       " 'zhenhou hong',\n",
       " 'david wadden',\n",
       " 'rené peinl',\n",
       " 'chenyang zhu',\n",
       " 'mohammad mahdian',\n",
       " 'david e. kaplan',\n",
       " 'h. k. hartline',\n",
       " 'jessica sage rauchberg',\n",
       " 'xiang feng',\n",
       " 'františek grézl',\n",
       " 'basem suleiman',\n",
       " 'chunming qiao',\n",
       " 'dang van thin',\n",
       " 'ron kohavi',\n",
       " 'ashwath sampath',\n",
       " 'hiroki sugisawa',\n",
       " 'luke hicks',\n",
       " 'yousheng zhang',\n",
       " 'minchuan chen',\n",
       " 'yong-ha park',\n",
       " 'yibo lin',\n",
       " 'alireza aghasi',\n",
       " 'howard barnum',\n",
       " 'jiming chen',\n",
       " 'martijn m. a. bosma',\n",
       " 'm. gori',\n",
       " 'kuang mao',\n",
       " 'patrick berrebi',\n",
       " 'julian fierrez',\n",
       " 'nicolas boumal',\n",
       " 'jiaao chen',\n",
       " 'ellis brown',\n",
       " 'leonardo scabini',\n",
       " 'eugene weinstein',\n",
       " 'laia tarrés',\n",
       " 'stefano bistarelli',\n",
       " 'gilles audemard',\n",
       " 'yanxin shen',\n",
       " 'katherine andriole',\n",
       " 'jiaying zou',\n",
       " 'takuya yamaguchi',\n",
       " 'cong leng',\n",
       " 'radka chaloupková',\n",
       " 'dahuin jung',\n",
       " 'sasmita parida',\n",
       " 'mohammad zarif joya',\n",
       " 'shijie xu',\n",
       " 'yinlong wen',\n",
       " 'paulo s. a. freitas',\n",
       " 'nicolas rojas',\n",
       " 'eric granger',\n",
       " 'amit singhai',\n",
       " 'yingpeng ma',\n",
       " 'kaikui liu',\n",
       " 'marcel wienöbst',\n",
       " 'jicong liu',\n",
       " 'labiba islam',\n",
       " 'robert mieth',\n",
       " 'andrea michiorri',\n",
       " 'akash kushal',\n",
       " 'evangelos bartzos',\n",
       " 'changliang li',\n",
       " 'hao chen',\n",
       " 'elisabeth remm',\n",
       " 'james liang',\n",
       " 'yi-an ma',\n",
       " 'rakesh rajpurohit',\n",
       " 'dieuwke hupkes',\n",
       " 'jo plested',\n",
       " 'andré correia',\n",
       " 'jani antikainen',\n",
       " 'morad behandish',\n",
       " 'federico sangati',\n",
       " 'zachary manchester',\n",
       " 'guneet singh kohli',\n",
       " 'jianbo yuan',\n",
       " 'zhiliang xu',\n",
       " 'clément gosselin',\n",
       " 'ameya joshi',\n",
       " 'zhaoyang zhang',\n",
       " 'vladimir v. arlazarov',\n",
       " 'henry kuo',\n",
       " 'manabu yoshida',\n",
       " 'max van kleek',\n",
       " 'xingru chen',\n",
       " 'ronghui you',\n",
       " 'vincent a. voelz',\n",
       " 'wei liang',\n",
       " 'zhenhua lin',\n",
       " 'sathiya keerthi selvaraj',\n",
       " 'michael sadovsky',\n",
       " 'vitaly feldman',\n",
       " 'sandip sarkar',\n",
       " 'linquan wu',\n",
       " 'avi kimchi',\n",
       " 'arthur hjorth',\n",
       " 'huy q. le',\n",
       " 'salsabil ahmed',\n",
       " 't. h. arjun',\n",
       " 'vitor hadad',\n",
       " 'agnieszka debudaj-grabysz',\n",
       " 'ethan c. chau',\n",
       " 'kechen qin',\n",
       " 'sounak dey',\n",
       " 'bonnie hei man liu',\n",
       " 'amarnath gupta',\n",
       " 'nishavi ranaweera',\n",
       " 'apu kapadia',\n",
       " 'bowen qin',\n",
       " 'francesco masulli',\n",
       " 'mohammadtaghi hajiaghayi',\n",
       " 'yfke dulek',\n",
       " 'yufeng liu',\n",
       " 'yali bian',\n",
       " 'sid-ahmed yahiaoui',\n",
       " 'aditya bhattacharya',\n",
       " 'sercan o arik',\n",
       " 'erik schwan',\n",
       " 'simon lefrançois',\n",
       " 'chen wei',\n",
       " 'sadaf gulshad',\n",
       " 'robert bain',\n",
       " 'otto nordander',\n",
       " 'karlis freivalds',\n",
       " 'jittat fakcharoenphol',\n",
       " 'peter k. koo',\n",
       " 'teresa m. brooks',\n",
       " 'javier morales',\n",
       " 'jingbo shang',\n",
       " 'paul zhang',\n",
       " 'zening chen',\n",
       " 'steven a. peterson',\n",
       " 'nesime tatbul',\n",
       " 'john arthur',\n",
       " 'gabriel falcao',\n",
       " 'eric heiden',\n",
       " 'zhaocong li',\n",
       " 'yogesh virkar',\n",
       " 'valéria reginatto',\n",
       " 'charles tappert',\n",
       " 'nicholas a. bokulich',\n",
       " 'zhuoxu huang',\n",
       " 'chunmo zheng',\n",
       " 'xiangmin zhou',\n",
       " 'bing-bing bian',\n",
       " 'eva-marie nosal',\n",
       " 'harrison bai',\n",
       " 'james t. laverty',\n",
       " 'guillaume jacques',\n",
       " 'jasper van der waa',\n",
       " 'suhas harish',\n",
       " 'tristan peng',\n",
       " 'alexander reznikov',\n",
       " 'ziqian wang',\n",
       " 'dosik hwang',\n",
       " 'zijin luo',\n",
       " 'ruiliang zhang',\n",
       " 'orhan arikan',\n",
       " 'georgia panagiotidou',\n",
       " 'davide rocchesso',\n",
       " 'fakhraddin alwajih',\n",
       " 'm. tennenholtz',\n",
       " 'wenjun xu',\n",
       " 'kailiang wu',\n",
       " 'ofek rafaeli',\n",
       " 'yilun hao',\n",
       " 'lina yao',\n",
       " 'xiangqian wu',\n",
       " 'ezekiel williams',\n",
       " 'kouta okubayashi',\n",
       " 'yanyan han',\n",
       " 'antti tarvainen',\n",
       " 'k. xiong',\n",
       " 'saurav musunuru',\n",
       " 'zhixiu lu',\n",
       " 'zakaria aldeneh',\n",
       " 'andreas groll',\n",
       " 'tomasz kuśmierczyk',\n",
       " 'rangan das',\n",
       " 'sabine wurmehl',\n",
       " 'ivan mazurenko',\n",
       " 'venkata gandikota',\n",
       " 'soo yong lee',\n",
       " 'jinwei zhao',\n",
       " 'michael f. wehner',\n",
       " 'jun kitazono',\n",
       " 'takashi shinozaki',\n",
       " 'shi feng',\n",
       " 'jiaoyang huang',\n",
       " 'ali eslami',\n",
       " 'matthew spotnitz',\n",
       " 'nigam h. shah',\n",
       " 'prithwish chakraborty',\n",
       " 'jack miller',\n",
       " 'cédric govaerts',\n",
       " 'sakkubai naidu',\n",
       " 'christian sivertsen',\n",
       " 'emma wakeling',\n",
       " 'prathap ramachandra',\n",
       " 'joon son chung',\n",
       " 'zhiguo he',\n",
       " 'alessandro castelnovo',\n",
       " 'richard l. wahl',\n",
       " 'jeonghwan cheon',\n",
       " 'andres altieri',\n",
       " 'amir rezaei balef',\n",
       " 'yefeng zheng',\n",
       " 'canh t. dinh',\n",
       " 'tianzhixi yin',\n",
       " 'chenglei yang',\n",
       " 'haidar harmanani',\n",
       " 'ahmed hefny',\n",
       " 'alina murad',\n",
       " 'marc höftmann',\n",
       " 'zhechao huang',\n",
       " 'mana zheng',\n",
       " 'daniele grandi',\n",
       " 'andres mafla',\n",
       " 'tai wang',\n",
       " 'stefan pukatzki',\n",
       " 'thomas firmin',\n",
       " 'sribala vidyadhari chinta',\n",
       " 'qifan zhang',\n",
       " 'kola ye',\n",
       " 'andrea dittadi',\n",
       " 'jinru wu',\n",
       " 'sihai tang',\n",
       " 'chao-yuan jin',\n",
       " 'thorsten dickhaus',\n",
       " 'jan šedivý',\n",
       " 'thore gerlach',\n",
       " 'yuanyuan wang',\n",
       " 'tomomi takenaga',\n",
       " 'marc-antoine rondeau',\n",
       " 'martijn wisse',\n",
       " 'yannan wang',\n",
       " 'andrés gvirtz',\n",
       " 'robert j. fletterick',\n",
       " 'ronald yu',\n",
       " 'shengqi shen',\n",
       " 'jean lahoud',\n",
       " 'tingying peng',\n",
       " 'renhao lu',\n",
       " 'michael friedlander',\n",
       " 'xinyu tian',\n",
       " 'weiwei deng',\n",
       " 'jose m. such',\n",
       " 'yuqing yang',\n",
       " 'michal guerquin',\n",
       " 'serena villata',\n",
       " 'emeson santana',\n",
       " 'muhammad iqbal',\n",
       " 'anh tong',\n",
       " 'mikyla k. bowen',\n",
       " 'yojan patel',\n",
       " 'ermei cao',\n",
       " 'zhongsheng li',\n",
       " 'rika antonova',\n",
       " 'hua-hua chang',\n",
       " 'tingnan hu',\n",
       " 'wenzao cui',\n",
       " 'cédric bousquet',\n",
       " 'rahul bagai',\n",
       " 'ali mehrban',\n",
       " 'najeeb kazmi',\n",
       " 'kaustuv mukherji',\n",
       " 'marko stalevski',\n",
       " 'safa cicek',\n",
       " 'jitkapat sawatphol',\n",
       " 'hannes thurnherr',\n",
       " 'sandika biswas',\n",
       " 'areej alsaafin',\n",
       " 'emre yilmaz',\n",
       " 'anish mishra',\n",
       " 'alireza azarfar',\n",
       " 'wided bakari',\n",
       " 'nurislam tursynbek',\n",
       " 'kelvin mariki',\n",
       " 'ruyu wang',\n",
       " 'jérôme bolte',\n",
       " 'rené bruikman',\n",
       " 'antony joseph',\n",
       " 'james babcock',\n",
       " 'gary m. lackmann',\n",
       " 'yiming yuan',\n",
       " 'hanxue liang',\n",
       " 'sharayu hiwarkhedkar',\n",
       " 'jean marie tshimula',\n",
       " 'yannick r. brunet',\n",
       " 'stefanie kaufmann',\n",
       " 'martin yang',\n",
       " 'aizaz hussain',\n",
       " 'rachael shaw',\n",
       " 'ismail uysal',\n",
       " 'samuel holtzman',\n",
       " 'jeffrey j. fredberg',\n",
       " 'eoin m. kenny',\n",
       " 'zenan zhai',\n",
       " 'chaobin you',\n",
       " 'yexu zhou',\n",
       " 'ahmed akinsola',\n",
       " 'masanori morise',\n",
       " 'matt w. jones',\n",
       " 'samir passi',\n",
       " 'jennifer santoso',\n",
       " 'prakhar gurawa',\n",
       " 'abhir bhalerao',\n",
       " 'alexandru maries',\n",
       " 'lee cooper',\n",
       " 'xingyuan dai',\n",
       " 'joonghyeok heo',\n",
       " 'shulin tian',\n",
       " 'seong-bae park',\n",
       " 'gil ettinger',\n",
       " 'ramesh illikkal',\n",
       " 'katherin yu',\n",
       " 'di qiu',\n",
       " 'mohammad a. yahya',\n",
       " 'ivan andonovic',\n",
       " 'david h. dowell',\n",
       " 'yongzhi xu',\n",
       " 'katharine m. flores',\n",
       " 'rahul rahaman',\n",
       " 'genke yang',\n",
       " 'barty pleydell-bouverie',\n",
       " 'chaeyoung jung',\n",
       " 'jose-maria urbano',\n",
       " 'longteng guo',\n",
       " 'gean trindade pereira',\n",
       " 'andreas athenodorou',\n",
       " 'juliano henrique foleiss',\n",
       " 'kang zhang',\n",
       " 'wei xue',\n",
       " 'zhoufan zhu',\n",
       " 'guangjun wu',\n",
       " 'bruno di giorgi',\n",
       " 'k. k. madsen',\n",
       " 'matt d. hoffman',\n",
       " 'sai akhil puranam',\n",
       " 'iordanis fostiropoulos',\n",
       " 'hongda hu',\n",
       " 'vishwas mruthyunjaya',\n",
       " 'daozheng chen',\n",
       " 'ifrah idrees',\n",
       " 'maurice rupp',\n",
       " 'paola casarosa',\n",
       " 'harsh dhillon',\n",
       " 'maria apostolaki',\n",
       " 'k vani',\n",
       " 'zhaoying li',\n",
       " 'jan ernst',\n",
       " 'sophie yang',\n",
       " 'sara sarto',\n",
       " 'nataliya sokolovska',\n",
       " 'lalita subedi',\n",
       " 'bohyung han',\n",
       " 'zeyang wang',\n",
       " 'tao liu',\n",
       " 'vatsal gupta',\n",
       " 'raphael sznitman',\n",
       " 'marcus wigan',\n",
       " 'shawn zheng',\n",
       " 'derrick m. anderson',\n",
       " 'trung duc nguyen',\n",
       " 'anuran makur',\n",
       " 'rohit gupta',\n",
       " 'pan-pan jiang',\n",
       " 'andrew beers',\n",
       " 'minghai qin',\n",
       " 'n. mata',\n",
       " 'guoqing zhao',\n",
       " 'meghan p. mccormick',\n",
       " 'ilya vasiliev',\n",
       " 'guiming hardy chen',\n",
       " 'xuxi chen',\n",
       " 'anneke buffone',\n",
       " 'szymon sidor',\n",
       " 'james requeima',\n",
       " 'lingteng qiu',\n",
       " 'guoqing wang',\n",
       " 'lukas pukelis',\n",
       " 'khanh-tung tran',\n",
       " 'ian barrett',\n",
       " 'yuichi kurita',\n",
       " 'jonathan a. clinger',\n",
       " 'jakub klikowski',\n",
       " 'a. stent',\n",
       " 'josé g. c. de souza',\n",
       " 'anne-laure boulesteix',\n",
       " 'ba-hien tran',\n",
       " 'fabian kögel',\n",
       " 'ricky x. f. chen',\n",
       " 'foteini simistira',\n",
       " 'marco lorenzi',\n",
       " 'sixiao zheng',\n",
       " 'brady taylor',\n",
       " 'arturo burguete-lopez',\n",
       " 'gianluca cerminara',\n",
       " 'yanan wu',\n",
       " 'gabriele farina',\n",
       " 'noura howell',\n",
       " 'stuart bowers',\n",
       " 'ji-hwan kim',\n",
       " 'cyril soler',\n",
       " 'zhitao wang',\n",
       " 'thang nguyen',\n",
       " 'yuan-sen ting',\n",
       " 'fabio piano',\n",
       " 'raef bassily',\n",
       " 'sylvain fontaine',\n",
       " 'yukti makhija',\n",
       " 'olga galinina',\n",
       " 'dario piga',\n",
       " 'emmanuelle salin',\n",
       " 'weikang chen',\n",
       " 'gil elbaz',\n",
       " \"supriya d'souza\",\n",
       " 'seung-bin kim',\n",
       " 'adrian johnston',\n",
       " 'brandon weissbourd',\n",
       " 'daniel fraunholz',\n",
       " 'noel c. f. codella',\n",
       " 'anwaar ulhaq',\n",
       " 'keith s. wilson',\n",
       " 'vanya cohen',\n",
       " 'shashi raj pandey',\n",
       " 'niraj k. jha',\n",
       " 'igor canadi',\n",
       " 'carl schultz',\n",
       " 'david m. groppe',\n",
       " 'dmitri chklovskii',\n",
       " 'fabian ritz',\n",
       " 'john c. duchi',\n",
       " 'fuhui zhou',\n",
       " 'yuhang ye',\n",
       " 'karim amer',\n",
       " 'hervé luga',\n",
       " 'padmanabhan pillai',\n",
       " 'le dong',\n",
       " 'anuj dubey',\n",
       " 'ibrahim a. hameed',\n",
       " 'huiqiang jiang',\n",
       " 'murat a. erdogdu',\n",
       " 'yih‐en andrew ban',\n",
       " 'ryan eloff',\n",
       " 'luyang zhu',\n",
       " 'sivanathan kandhasamy',\n",
       " 'a. para',\n",
       " 'samiulla shaikh',\n",
       " 'son van nguyen',\n",
       " 'zhecheng shi',\n",
       " 'luping ji',\n",
       " 'brandon duderstadt',\n",
       " 'ruizhi qiao',\n",
       " 'rita cucchiara',\n",
       " 'joachim giesen',\n",
       " 'matthias zeppelzauer',\n",
       " 'shamik sural',\n",
       " 'jamal atif',\n",
       " 'zhanhui zhou',\n",
       " 'xiaoling hu',\n",
       " 'graham leach-krouse',\n",
       " 'zongyi li',\n",
       " 'amazigh amrane',\n",
       " 'shigeru kuriyama',\n",
       " 't. dodds',\n",
       " 'dmitri demler',\n",
       " 'jimena guallar-blasco',\n",
       " 'y. j. ma',\n",
       " 'sri marini',\n",
       " 'gavin caulfield',\n",
       " 'cheng‐lin liu',\n",
       " 'shaun l. gabbidon',\n",
       " 'yeong-dae kwon',\n",
       " 'xiaolin huang',\n",
       " 'saeed hadadan',\n",
       " 'joshua c. zhao',\n",
       " 'alison m. maxwell',\n",
       " 'andreu vall',\n",
       " 'james. t kwok',\n",
       " 'lucianna h. s. santos',\n",
       " 'agnes m. kloft',\n",
       " 'a. quirrenbach',\n",
       " 'angela yuan',\n",
       " 'daniil selikhanovych',\n",
       " 'himmy tam',\n",
       " 'mohammad amaan sayeed',\n",
       " 'luiz jonata pires de araujo',\n",
       " 'guido sanguinetti',\n",
       " 'zhili wang',\n",
       " 'guoyang zeng',\n",
       " 'charlotte rodwell',\n",
       " 'matthew w. hoffman',\n",
       " 'yi liao',\n",
       " 'felipe b. martins',\n",
       " 'atsushi nakamura',\n",
       " 'muhammad i. qureshi',\n",
       " 'amanda a. mcmurray',\n",
       " 'jiace sun',\n",
       " 'hui ding',\n",
       " 'tabea m. g. pakull',\n",
       " 'irina sycheva',\n",
       " 'gong chen',\n",
       " 'christoph spiegel',\n",
       " 'billy carson',\n",
       " 'nesar ramachandra',\n",
       " 'noah marshall',\n",
       " 'anusua trivedi',\n",
       " 'parth katlana',\n",
       " 'lisa bylinina',\n",
       " 'hirozumi yamaguchi',\n",
       " 'yufan cao',\n",
       " 'yue him wong tim',\n",
       " 'chuang zhang',\n",
       " 'ga wu',\n",
       " 'ahmed taha',\n",
       " 'lorin jenkel',\n",
       " 'avisek gupta',\n",
       " 'xiaoyong zhu',\n",
       " 'francesco turro',\n",
       " 'yuyang sun',\n",
       " 'jihwan kwak',\n",
       " 'j. van tol',\n",
       " 'charchit sharma',\n",
       " 'arundhati ghosh',\n",
       " 'r. j. cava',\n",
       " 'jürgen fassbender',\n",
       " 'rosie sallis',\n",
       " 'minghui wu',\n",
       " 'lina c. jaurigue',\n",
       " 'hanruo liu',\n",
       " 'wali hakimpour',\n",
       " 'michael garcia-ortiz',\n",
       " 'tianle gu',\n",
       " 'radek grzeszczuk',\n",
       " 'fábio crestani',\n",
       " 's. muthukrishnan',\n",
       " 'tim durgin',\n",
       " 'angela ng',\n",
       " 'pierre carpentier',\n",
       " 'george a. miller',\n",
       " 'yuan du',\n",
       " 'abhinav reddy mandli',\n",
       " 'galymzhan begimov',\n",
       " 'emma frejinger',\n",
       " 'minghan wang',\n",
       " 'lei yang',\n",
       " 'd. e. jaramillo',\n",
       " 'juan luo',\n",
       " 'hannah craighead',\n",
       " 'rena kawamura',\n",
       " 'ernest mwebaze',\n",
       " 'joseph feneuil',\n",
       " 'filippo lanubile',\n",
       " 'henrik birkedala',\n",
       " 'pietro ferraro',\n",
       " 'thorsten hellert',\n",
       " 'ada bohm',\n",
       " 'saurabh garg',\n",
       " 'tolga m. duman',\n",
       " 'gahee kim',\n",
       " 'stanley n. cohen',\n",
       " 'ireneusz gawlik',\n",
       " 'harry dong',\n",
       " 'mattie fellows',\n",
       " 'oleksandr galkin',\n",
       " 'prodip hore',\n",
       " 'benjamin towle',\n",
       " 'igor g. vladimirov',\n",
       " 'hyunhee park',\n",
       " 'eren sasoglu',\n",
       " 'piyush kumar',\n",
       " 'jason held',\n",
       " 'jacob feala',\n",
       " 'liangtian wan',\n",
       " 'michael whitney',\n",
       " 'samuel goree',\n",
       " 'lesley a. earl',\n",
       " 'przemysław nogły',\n",
       " 'bilal randeree',\n",
       " 'arash gholamidavoodi',\n",
       " 'victor goulart',\n",
       " 'matt piekenbrock',\n",
       " 'w. utschick',\n",
       " 'stefan kesselheim',\n",
       " 'mohd. fazil',\n",
       " 'kevin kwok',\n",
       " 'junko yano',\n",
       " 'shu yu',\n",
       " 'winfried ripken',\n",
       " 'yisong wang',\n",
       " 'steffen jung',\n",
       " 'victor ojewale',\n",
       " 'hans‐peter seidel',\n",
       " 'ralf steinberger',\n",
       " 'aline de campos',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(all_authors) # need to treat as Multiword Expressions!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "556545b6-f651-4cf5-9477-59b919017052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168566"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc18895a-992e-48d0-93bf-6b6c4f8fc854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('all_authors.pkl', 'wb') as file:\n",
    "    pickle.dump(all_authors, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefac6b-738e-4f77-84f7-d294ccc68253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fd7a429-c35c-48a1-9282-458826475d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 documents done.\n",
      "20000 documents done.\n",
      "30000 documents done.\n",
      "40000 documents done.\n",
      "50000 documents done.\n",
      "60000 documents done.\n",
      "70000 documents done.\n",
      "80000 documents done.\n",
      "90000 documents done.\n"
     ]
    }
   ],
   "source": [
    "docid_year_map = {}\n",
    "\n",
    "with open('all_papers.jsonl', 'r') as file:\n",
    "    idx=0\n",
    "    for line_num, line in enumerate(file):\n",
    "        idx+=1\n",
    "        if idx>0:\n",
    "            document = json.loads(line.strip())\n",
    "            # print(type(document.get('publish_year', '')))\n",
    "            if document.get('publish_year', '') != '':\n",
    "                docid_year_map[idx] = document.get('publish_year', 0)\n",
    "            elif document.get('published', '') != '':\n",
    "                parsed_date = datetime.strptime(document.get('published', ''), '%Y-%m-%dT%H:%M:%SZ')\n",
    "                year = parsed_date.year\n",
    "                docid_year_map[idx] = year\n",
    "            else:\n",
    "                docid_year_map[idx] = ''\n",
    "            if idx%10000 == 0:\n",
    "                print(f'{idx} documents done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e30991bd-8715-49bc-96bc-d8f84cbecaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docid_year_map[1] ## SAVE THIS MAPPING!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1b50d0c-fd8f-4c98-81af-920098514592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('docid_year_map.pkl', 'wb') as file:\n",
    "    pickle.dump(docid_year_map, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3122a57-efb5-4839-9880-aac20f05203a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0a3fc6-39ef-4a8d-bca7-9a45f3a8aa23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359a9fa-1d30-49c2-92d2-fdc870683c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443027ba-b31a-4459-bd2f-6f84a467f993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3875120-502b-41c2-942f-d74bc1ac1546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd91a78c-1276-429e-bc4b-4da7077160b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf59add-4ea7-43be-bd9d-6ac7aa1b2aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f9f3f-0e29-4401-8087-f8335c3a173e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409199c7-7fe2-41b6-8357-c9feef261edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8a2c8-b447-4b9d-bc1b-28a08269f3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16f5eaca-569f-4f5a-a4fe-5d23f3844705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = ['Research on Reinforcement Learning techniques',\n",
    "#                 'Find articles on unsupervised learning for text',\n",
    "#                 'Find studies on Zero-Shot Learning',\n",
    "#                 'adversarial attacks in neural networks',\n",
    "#                 'federated learning',\n",
    "#                 'Papers on meta-learning techniques which also talk about Reinforcement-Learning or Preference-Learning',\n",
    "#                 'Find papers on Transformer Architecture published after 2010',\n",
    "#                 'Find all papers published by lun ai',\n",
    "#                 'Find papers by tao zeng published after 2010'\n",
    "#                ]\n",
    "\n",
    "# queries = [\n",
    "#                 'Find papers on Transformer Architecture published after 2010',\n",
    "#                 'Find all papers published by Ashish Vaswani',\n",
    "#                 'Find papers by tao zeng or lun ai published between 2010 and 2020'\n",
    "#                ]\n",
    "\n",
    "\n",
    "queries = [\n",
    "    \"Transformers in NLP\",\n",
    "    \"Papers on BERT\",\n",
    "    \"Recent studies on GANs\",\n",
    "    \"Fetch papers by Yann LeCun on Convolutional Neural Networks\",\n",
    "    \"Show me publications by Geoffrey Hinton on Deep Learning\",\n",
    "    \"Research on Reinforcement Learning techniques\",\n",
    "    \"Co-authored papers by Yoshua Bengio after 2018\",\n",
    "    \"Machine learning for healthcare applications\",\n",
    "    \"Attention mechanisms in Computer Vision\",\n",
    "    \"Papers on Mamba Architecture in IEEE journals\",\n",
    "    \"Fetch recent studies on LSTM networks\",\n",
    "    \"Show me all papers on Natural Language Processing by Christopher Manning\",\n",
    "    \"Research on Self-Supervised Learning\",\n",
    "    \"Articles by Jürgen Schmidhuber on Recurrent Neural Networks\",\n",
    "    \"Recent advances in Robotics and AI\",\n",
    "    \"Studies on Transfer Learning for computer vision tasks\",\n",
    "    \"Show me all publications on Object Detection\",\n",
    "    \"Fetch co-authored research by Li Fei-Fei since 2020\",\n",
    "    \"Applications of NLP in finance\",\n",
    "    \"Recent studies on XGBoost for classification\",\n",
    "    \"Show me papers by Andrew Ng on Machine Learning published in 2021\",\n",
    "    \"Retrieve papers by Ruslan Salakhutdinov on Collaborative Filtering\",\n",
    "    \"Research on explainability in AI\",\n",
    "    \"Papers on Fairness in Machine Learning\",\n",
    "    \"Studies on medical image analysis using deep learning\",\n",
    "    \"Find articles on unsupervised learning for text\",\n",
    "    \"Papers by Tomas Mikolov on word embeddings\",\n",
    "    \"Research on adversarial attacks in neural networks\",\n",
    "    \"Recent studies on the Transformer model in 2023\",\n",
    "    \"Show me papers on multi-modal learning by Pieter Abbeel\",\n",
    "    \"Machine Learning for social good\",\n",
    "    \"Fetch studies on reinforcement learning for robotics\",\n",
    "    \"Recent papers on data augmentation techniques\",\n",
    "    \"Publications on ethical AI by Timnit Gebru\",\n",
    "    \"Explainability in neural networks\",\n",
    "    \"Show me all co-authored work by Alex Krizhevsky on CNNs\",\n",
    "    \"Latest research on language models in healthcare\",\n",
    "    'Find papers on Transformer Architecture published after 2010',\n",
    "    'Find all papers published by Ashish Vaswani',\n",
    "    'Find papers by tao zeng or lun ai published between 2010 and 2020'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e109f963-7c53-4123-b1cc-102c6cdc9d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a3c6ffc-ba1d-48cc-b6d3-1e6657fa94d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stopwords.txt\", \"r\") as file:\n",
    "        stopwords = [line.strip() for line in file]\n",
    "    \n",
    "preprocessor = RegexTokenizer()\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d98b08-ab49-4049-b24b-93112890e12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a4341c-355c-4580-9971-3225841479eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37e9ae86-6a3d-483b-8c73-b9b4b1f0dafc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print('Reading JSON file.')\n",
    "# with open(\"arxiv_papers.json\", \"r\") as json_file:\n",
    "#     data = json.load(json_file)\n",
    "\n",
    "# # Write data to JSONL file\n",
    "# print('Converting to JSONL')\n",
    "# with open(\"arxiv_papers.jsonl\", \"w\") as jsonl_file:\n",
    "#     for entry in data:\n",
    "#         jsonl_file.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "\n",
    "print('Creating Index')\n",
    "index = Indexer.create_index(IndexType.BasicInvertedIndex, 'all_papers.jsonl', \n",
    "                             preprocessor, stopwords=stopwords, minimum_word_frequency=50)\n",
    "print('Index Created')\n",
    "print('Saving Index')\n",
    "index.save('all_papers_index')\n",
    "\n",
    "\n",
    "print('Creating Title Index')\n",
    "title_index = Indexer.create_index(IndexType.BasicInvertedIndex, 'all_papers.jsonl', \n",
    "                             preprocessor, stopwords=stopwords, minimum_word_frequency=0, text_key='title')\n",
    "print('Title Index Created')\n",
    "\n",
    "print('Saving Index')\n",
    "title_index.save('all_papers_title_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf7550-e2f7-44a4-bfdf-a43f3275732b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d807c7f4-22cc-430c-bd9f-e92cf30c6bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Index\n",
      "Index loaded from all_papers_index\n",
      "Loading Title Index\n",
      "Index loaded from all_papers_title_index\n"
     ]
    }
   ],
   "source": [
    "print('Loading Index')\n",
    "index = BasicInvertedIndex()\n",
    "index.load('all_papers_index')\n",
    "\n",
    "print('Loading Title Index')\n",
    "title_index = BasicInvertedIndex()\n",
    "title_index.load('all_papers_title_index')\n",
    "\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03d43738-748c-4ec1-b8f8-ca9b3f3592a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BMRanker = Ranker(index, title_index, preprocessor, stopwords=stopwords, scorer=BM25(index), \n",
    "                  all_authors=list(all_authors), docid_to_authors=docid_authors_map, docid_to_year=docid_year_map)\n",
    "\n",
    "TFIDFRanker = Ranker(index, title_index, preprocessor, stopwords=stopwords, scorer=TF_IDF(index),\n",
    "                    all_authors=list(all_authors), docid_to_authors=docid_authors_map, docid_to_year=docid_year_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "131bc437-1aba-4dc4-974a-ddfe6d8092a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers in NLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36623/36623 [00:00<00:00, 221635.77it/s]\n",
      "100%|██████████| 1500/1500 [00:00<00:00, 239118.85it/s]\n",
      "100%|██████████| 36623/36623 [00:00<00:00, 286960.05it/s]\n",
      "100%|██████████| 1500/1500 [00:00<00:00, 274844.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers on BERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45367/45367 [00:00<00:00, 223994.63it/s]\n",
      "100%|██████████| 1324/1324 [00:00<00:00, 241582.57it/s]\n",
      "100%|██████████| 45367/45367 [00:00<00:00, 304430.86it/s]\n",
      "100%|██████████| 1324/1324 [00:00<00:00, 307949.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent studies on GANs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80250/80250 [00:00<00:00, 175462.79it/s]\n",
      "100%|██████████| 401/401 [00:00<00:00, 173949.31it/s]\n",
      "100%|██████████| 80250/80250 [00:00<00:00, 280288.30it/s]\n",
      "100%|██████████| 401/401 [00:00<00:00, 247158.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch papers by Yann LeCun on Convolutional Neural Networks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88131/88131 [00:01<00:00, 85883.25it/s]\n",
      "100%|██████████| 13153/13153 [00:00<00:00, 101177.95it/s]\n",
      "100%|██████████| 88131/88131 [00:00<00:00, 172030.86it/s]\n",
      "100%|██████████| 13153/13153 [00:00<00:00, 238497.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show me publications by Geoffrey Hinton on Deep Learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95940/95940 [00:01<00:00, 93235.28it/s]\n",
      "100%|██████████| 23882/23882 [00:00<00:00, 118502.76it/s]\n",
      "100%|██████████| 95940/95940 [00:00<00:00, 170518.88it/s]\n",
      "100%|██████████| 23882/23882 [00:00<00:00, 308684.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research on Reinforcement Learning techniques\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95787/95787 [00:00<00:00, 124411.99it/s]\n",
      "100%|██████████| 21946/21946 [00:00<00:00, 159914.45it/s]\n",
      "100%|██████████| 95787/95787 [00:00<00:00, 193943.60it/s]\n",
      "100%|██████████| 21946/21946 [00:00<00:00, 310088.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-authored papers by Yoshua Bengio after 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83066/83066 [00:00<00:00, 118987.77it/s]\n",
      "100%|██████████| 124/124 [00:00<00:00, 113285.49it/s]\n",
      "100%|██████████| 83066/83066 [00:00<00:00, 210883.15it/s]\n",
      "100%|██████████| 124/124 [00:00<00:00, 199269.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning for healthcare applications\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94554/94554 [00:00<00:00, 123674.60it/s]\n",
      "100%|██████████| 24262/24262 [00:00<00:00, 160650.36it/s]\n",
      "100%|██████████| 94554/94554 [00:00<00:00, 193311.22it/s]\n",
      "100%|██████████| 24262/24262 [00:00<00:00, 316061.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention mechanisms in Computer Vision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86171/86171 [00:00<00:00, 132318.32it/s]\n",
      "100%|██████████| 3349/3349 [00:00<00:00, 153618.52it/s]\n",
      "100%|██████████| 86171/86171 [00:00<00:00, 218427.48it/s]\n",
      "100%|██████████| 3349/3349 [00:00<00:00, 268929.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers on Mamba Architecture in IEEE journals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86963/86963 [00:00<00:00, 120855.54it/s]\n",
      "100%|██████████| 1412/1412 [00:00<00:00, 133787.18it/s]\n",
      "100%|██████████| 86963/86963 [00:00<00:00, 238006.97it/s]\n",
      "100%|██████████| 1412/1412 [00:00<00:00, 287172.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch recent studies on LSTM networks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91512/91512 [00:00<00:00, 114331.43it/s]\n",
      "100%|██████████| 7264/7264 [00:00<00:00, 139250.37it/s]\n",
      "100%|██████████| 91512/91512 [00:00<00:00, 206562.77it/s]\n",
      "100%|██████████| 7264/7264 [00:00<00:00, 298352.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show me all papers on Natural Language Processing by Christopher Manning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96390/96390 [00:01<00:00, 78335.33it/s]\n",
      "100%|██████████| 8721/8721 [00:00<00:00, 103803.61it/s]\n",
      "100%|██████████| 96390/96390 [00:00<00:00, 140153.57it/s]\n",
      "100%|██████████| 8721/8721 [00:00<00:00, 261585.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research on Self-Supervised Learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94992/94992 [00:00<00:00, 148371.25it/s]\n",
      "100%|██████████| 22704/22704 [00:00<00:00, 192961.14it/s]\n",
      "100%|██████████| 94992/94992 [00:00<00:00, 205520.18it/s]\n",
      "100%|██████████| 22704/22704 [00:00<00:00, 322335.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles by Jürgen Schmidhuber on Recurrent Neural Networks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86683/86683 [00:00<00:00, 100299.61it/s]\n",
      "100%|██████████| 12892/12892 [00:00<00:00, 113507.58it/s]\n",
      "100%|██████████| 86683/86683 [00:00<00:00, 199688.28it/s]\n",
      "100%|██████████| 12892/12892 [00:00<00:00, 248683.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent advances in Robotics and AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 88067/88067 [00:00<00:00, 133172.12it/s]\n",
      "100%|██████████| 4582/4582 [00:00<00:00, 160132.49it/s]\n",
      "100%|██████████| 88067/88067 [00:00<00:00, 223563.97it/s]\n",
      "100%|██████████| 4582/4582 [00:00<00:00, 309862.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studies on Transfer Learning for computer vision tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95515/95515 [00:01<00:00, 85696.04it/s]\n",
      "100%|██████████| 23898/23898 [00:00<00:00, 119925.43it/s]\n",
      "100%|██████████| 95515/95515 [00:00<00:00, 139865.14it/s]\n",
      "100%|██████████| 23898/23898 [00:00<00:00, 308763.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show me all publications on Object Detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90910/90910 [00:00<00:00, 139581.82it/s]\n",
      "100%|██████████| 4241/4241 [00:00<00:00, 152450.21it/s]\n",
      "100%|██████████| 90910/90910 [00:00<00:00, 246651.31it/s]\n",
      "100%|██████████| 4241/4241 [00:00<00:00, 239456.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch co-authored research by Li Fei-Fei since 2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94121/94121 [00:00<00:00, 98296.03it/s]\n",
      "100%|██████████| 797/797 [00:00<00:00, 113398.02it/s]\n",
      "100%|██████████| 94121/94121 [00:00<00:00, 189615.05it/s]\n",
      "100%|██████████| 797/797 [00:00<00:00, 250120.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applications of NLP in finance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75448/75448 [00:00<00:00, 183330.57it/s]\n",
      "100%|██████████| 1543/1543 [00:00<00:00, 187436.60it/s]\n",
      "100%|██████████| 75448/75448 [00:00<00:00, 302511.22it/s]\n",
      "100%|██████████| 1543/1543 [00:00<00:00, 264392.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent studies on XGBoost for classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82823/82823 [00:00<00:00, 141508.91it/s]\n",
      "100%|██████████| 4219/4219 [00:00<00:00, 160609.27it/s]\n",
      "100%|██████████| 82823/82823 [00:00<00:00, 241100.50it/s]\n",
      "100%|██████████| 4219/4219 [00:00<00:00, 280138.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show me papers by Andrew Ng on Machine Learning published in 2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96961/96961 [00:01<00:00, 70111.58it/s]\n",
      "100%|██████████| 23629/23629 [00:00<00:00, 95633.51it/s]\n",
      "100%|██████████| 96961/96961 [00:00<00:00, 129437.12it/s]\n",
      "100%|██████████| 23629/23629 [00:00<00:00, 309236.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve papers by Ruslan Salakhutdinov on Collaborative Filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55967/55967 [00:00<00:00, 110710.15it/s]\n",
      "100%|██████████| 744/744 [00:00<00:00, 112294.87it/s]\n",
      "100%|██████████| 55967/55967 [00:00<00:00, 239652.86it/s]\n",
      "100%|██████████| 744/744 [00:00<00:00, 151498.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research on explainability in AI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89538/89538 [00:00<00:00, 173850.10it/s]\n",
      "100%|██████████| 4893/4893 [00:00<00:00, 193386.25it/s]\n",
      "100%|██████████| 89538/89538 [00:00<00:00, 271935.80it/s]\n",
      "100%|██████████| 4893/4893 [00:00<00:00, 298760.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers on Fairness in Machine Learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91919/91919 [00:00<00:00, 125150.21it/s]\n",
      "100%|██████████| 23762/23762 [00:00<00:00, 159448.52it/s]\n",
      "100%|██████████| 91919/91919 [00:00<00:00, 202340.86it/s]\n",
      "100%|██████████| 23762/23762 [00:00<00:00, 317665.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studies on medical image analysis using deep learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96599/96599 [00:01<00:00, 85258.80it/s]\n",
      "100%|██████████| 28987/28987 [00:00<00:00, 118448.51it/s]\n",
      "100%|██████████| 96599/96599 [00:00<00:00, 139611.31it/s]\n",
      "100%|██████████| 28987/28987 [00:00<00:00, 296350.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find articles on unsupervised learning for text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92577/92577 [00:00<00:00, 112150.66it/s]\n",
      "100%|██████████| 26339/26339 [00:00<00:00, 138986.60it/s]\n",
      "100%|██████████| 92577/92577 [00:00<00:00, 201952.39it/s]\n",
      "100%|██████████| 26339/26339 [00:00<00:00, 325375.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers by Tomas Mikolov on word embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62085/62085 [00:00<00:00, 116081.47it/s]\n",
      "100%|██████████| 3047/3047 [00:00<00:00, 130620.54it/s]\n",
      "100%|██████████| 62085/62085 [00:00<00:00, 208095.95it/s]\n",
      "100%|██████████| 3047/3047 [00:00<00:00, 252520.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research on adversarial attacks in neural networks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94685/94685 [00:00<00:00, 104358.46it/s]\n",
      "100%|██████████| 15012/15012 [00:00<00:00, 129768.60it/s]\n",
      "100%|██████████| 94685/94685 [00:00<00:00, 174792.51it/s]\n",
      "100%|██████████| 15012/15012 [00:00<00:00, 255424.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent studies on the Transformer model in 2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95539/95539 [00:00<00:00, 104669.65it/s]\n",
      "100%|██████████| 6389/6389 [00:00<00:00, 138158.74it/s]\n",
      "100%|██████████| 95539/95539 [00:00<00:00, 173839.77it/s]\n",
      "100%|██████████| 6389/6389 [00:00<00:00, 274310.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show me papers on multi-modal learning by Pieter Abbeel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96485/96485 [00:01<00:00, 83571.59it/s]\n",
      "100%|██████████| 24288/24288 [00:00<00:00, 106509.87it/s]\n",
      "100%|██████████| 96485/96485 [00:00<00:00, 162073.74it/s]\n",
      "100%|██████████| 24288/24288 [00:00<00:00, 310799.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning for social good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93504/93504 [00:00<00:00, 121350.88it/s]\n",
      "100%|██████████| 24388/24388 [00:00<00:00, 161936.39it/s]\n",
      "100%|██████████| 93504/93504 [00:00<00:00, 187012.90it/s]\n",
      "100%|██████████| 24388/24388 [00:00<00:00, 321685.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch studies on reinforcement learning for robotics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92378/92378 [00:00<00:00, 120928.26it/s]\n",
      "100%|██████████| 21310/21310 [00:00<00:00, 135996.57it/s]\n",
      "100%|██████████| 92378/92378 [00:00<00:00, 232323.05it/s]\n",
      "100%|██████████| 21310/21310 [00:00<00:00, 297383.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent papers on data augmentation techniques\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95777/95777 [00:00<00:00, 105259.95it/s]\n",
      "100%|██████████| 5889/5889 [00:00<00:00, 133691.23it/s]\n",
      "100%|██████████| 95777/95777 [00:00<00:00, 174976.03it/s]\n",
      "100%|██████████| 5889/5889 [00:00<00:00, 266689.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publications on ethical AI by Timnit Gebru\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49760/49760 [00:00<00:00, 129975.83it/s]\n",
      "100%|██████████| 4594/4594 [00:00<00:00, 136261.71it/s]\n",
      "100%|██████████| 49760/49760 [00:00<00:00, 263623.44it/s]\n",
      "100%|██████████| 4594/4594 [00:00<00:00, 310579.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explainability in neural networks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85201/85201 [00:00<00:00, 160795.02it/s]\n",
      "100%|██████████| 12779/12779 [00:00<00:00, 181550.01it/s]\n",
      "100%|██████████| 85201/85201 [00:00<00:00, 240441.71it/s]\n",
      "100%|██████████| 12779/12779 [00:00<00:00, 267818.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show me all co-authored work by Alex Krizhevsky on CNNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96325/96325 [00:00<00:00, 99864.65it/s] \n",
      "100%|██████████| 278/278 [00:00<00:00, 110178.26it/s]\n",
      "100%|██████████| 96325/96325 [00:00<00:00, 196220.73it/s]\n",
      "100%|██████████| 278/278 [00:00<00:00, 226586.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest research on language models in healthcare\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95504/95504 [00:00<00:00, 108460.26it/s]\n",
      "100%|██████████| 12640/12640 [00:00<00:00, 131042.19it/s]\n",
      "100%|██████████| 95504/95504 [00:00<00:00, 187760.17it/s]\n",
      "100%|██████████| 12640/12640 [00:00<00:00, 259252.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find papers on Transformer Architecture published after 2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89570/89570 [00:00<00:00, 99174.39it/s] \n",
      "100%|██████████| 2816/2816 [00:00<00:00, 119022.12it/s]\n",
      "100%|██████████| 89570/89570 [00:00<00:00, 188092.82it/s]\n",
      "100%|██████████| 2816/2816 [00:00<00:00, 281358.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find all papers published by Ashish Vaswani\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62089/62089 [00:00<00:00, 120847.92it/s]\n",
      "100%|██████████| 121/121 [00:00<00:00, 116642.33it/s]\n",
      "100%|██████████| 62089/62089 [00:00<00:00, 238069.25it/s]\n",
      "100%|██████████| 121/121 [00:00<00:00, 158498.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find papers by tao zeng or lun ai published between 2010 and 2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92379/92379 [00:01<00:00, 71784.73it/s]\n",
      "100%|██████████| 4577/4577 [00:00<00:00, 88235.59it/s]\n",
      "100%|██████████| 92379/92379 [00:00<00:00, 161977.79it/s]\n",
      "100%|██████████| 4577/4577 [00:00<00:00, 304632.48it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf = []\n",
    "bm25 = []\n",
    "\n",
    "for q in queries:\n",
    "    print(q)\n",
    "    tfidf.append(TFIDFRanker.query(q))\n",
    "    bm25.append(BMRanker.query(q))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc8cf5ec-f61c-4206-b6a0-1552a570b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "393b5bb6-b24e-4380-8fa3-09b70bd4f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf2 = []\n",
    "for i in tfidf:\n",
    "    tfidf2.append(i[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb3088bb-c692-4f64-adaf-75081eb8820a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(58233, 5.658143411992613),\n",
       "  (59201, 5.030026580547734),\n",
       "  (51504, 4.793171721288175),\n",
       "  (59105, 4.739575119150212),\n",
       "  (18201, 4.719587053884814),\n",
       "  (34225, 4.634700400866777),\n",
       "  (57440, 4.604259605200757),\n",
       "  (79684, 4.594000285338274),\n",
       "  (52666, 4.580572172004343),\n",
       "  (72920, 4.578745644715002),\n",
       "  (56381, 4.527463473308957),\n",
       "  (58138, 4.499807426771711),\n",
       "  (16928, 4.437536782240654),\n",
       "  (57904, 4.22874379457927),\n",
       "  (59662, 4.207781449073225),\n",
       "  (17207, 4.20449961843522),\n",
       "  (58635, 4.202934913521098),\n",
       "  (47599, 4.193351588749996),\n",
       "  (41822, 4.143252313259141),\n",
       "  (16985, 4.136883102339302)],\n",
       " [(52666, 5.039433035200425),\n",
       "  (51586, 4.68922630111536),\n",
       "  (50854, 4.5629886449296055),\n",
       "  (44746, 4.525014488480336),\n",
       "  (50861, 4.4743730458585915),\n",
       "  (32184, 4.4399304307176015),\n",
       "  (36569, 4.364270388074816),\n",
       "  (50695, 4.3461780263823355),\n",
       "  (33115, 4.328865636642121),\n",
       "  (32888, 4.32801899319349),\n",
       "  (2992, 4.320501122064888),\n",
       "  (51080, 4.298696996927923),\n",
       "  (51899, 4.294172938334379),\n",
       "  (50855, 4.294010250432626),\n",
       "  (51106, 4.288015813070984),\n",
       "  (51100, 4.277979383317113),\n",
       "  (50897, 4.263546931876236),\n",
       "  (51209, 4.260644308241471),\n",
       "  (50818, 4.230585537257619),\n",
       "  (50994, 4.221837446313092)],\n",
       " [(88177, 6.359215898999135),\n",
       "  (84165, 5.3410441534436375),\n",
       "  (86043, 5.177193379026403),\n",
       "  (83883, 5.009982858790076),\n",
       "  (90882, 5.006609037997189),\n",
       "  (73705, 4.995919557044392),\n",
       "  (23596, 4.942221739577968),\n",
       "  (48251, 4.898844056916771),\n",
       "  (97140, 4.877041422364681),\n",
       "  (65925, 4.876678918092987),\n",
       "  (87925, 4.847148580624751),\n",
       "  (76029, 4.839554452091267),\n",
       "  (75413, 4.784592585180866),\n",
       "  (21130, 4.761330435617686),\n",
       "  (87866, 4.757687241946144),\n",
       "  (7518, 4.706909856169762),\n",
       "  (66342, 4.671740213281105),\n",
       "  (45704, 4.657825630473757),\n",
       "  (74962, 4.646307772876671),\n",
       "  (87966, 4.644205394796581)],\n",
       " [(14664, 106.37545754722314),\n",
       "  (15431, 106.19996130059573),\n",
       "  (95497, 106.19831247103431),\n",
       "  (13857, 106.02181305790619),\n",
       "  (91324, 105.87451399365828),\n",
       "  (90602, 105.7300540817544),\n",
       "  (97442, 105.50737931121738),\n",
       "  (16022, 105.3106820247078),\n",
       "  (98293, 105.30996142842027),\n",
       "  (16343, 105.2949093915227),\n",
       "  (90968, 104.93565533696464),\n",
       "  (94916, 104.9240829601203),\n",
       "  (7100, 104.85156574562349),\n",
       "  (91077, 104.71500522903021),\n",
       "  (91000, 104.70119062928933),\n",
       "  (94911, 104.55095193312228),\n",
       "  (96305, 104.53447803768856),\n",
       "  (14550, 104.35085066623121),\n",
       "  (14050, 104.2902974108023),\n",
       "  (8332, 104.25467666632352)],\n",
       " [(7272, 103.51602456971831),\n",
       "  (82819, 103.0680032037086),\n",
       "  (67829, 103.06071665230061),\n",
       "  (75662, 102.94489838555877),\n",
       "  (46258, 102.6828533147951),\n",
       "  (11532, 102.63119757136958),\n",
       "  (94680, 8.189064503894873),\n",
       "  (27808, 7.471403241221596),\n",
       "  (4842, 7.039886195653047),\n",
       "  (7593, 6.740865799392868),\n",
       "  (61349, 6.482332276793269),\n",
       "  (4196, 6.445605699229228),\n",
       "  (4408, 6.384413241829355),\n",
       "  (67190, 6.374488888108212),\n",
       "  (11991, 6.358238085521734),\n",
       "  (24088, 6.286432803075764),\n",
       "  (4731, 6.2811857906971),\n",
       "  (4365, 6.2197924763590695),\n",
       "  (24457, 6.216851545122129),\n",
       "  (62562, 6.182668659242934)],\n",
       " [(4731, 7.7287328890526314),\n",
       "  (871, 7.422437346582183),\n",
       "  (5294, 7.057416059893866),\n",
       "  (5417, 7.006982798161146),\n",
       "  (4159, 6.921382716566715),\n",
       "  (5299, 6.776179550998547),\n",
       "  (5528, 6.746168763253425),\n",
       "  (26169, 6.720498919934274),\n",
       "  (1174, 6.699248246197072),\n",
       "  (28789, 6.67747557327278),\n",
       "  (5189, 6.673561466097382),\n",
       "  (6083, 6.659164045492686),\n",
       "  (5648, 6.624860401411946),\n",
       "  (6004, 6.597887739729648),\n",
       "  (715, 6.577505558407326),\n",
       "  (5501, 6.575908084273842),\n",
       "  (5432, 6.570296634812005),\n",
       "  (5164, 6.568568258697916),\n",
       "  (10376, 6.557170112564543),\n",
       "  (5743, 6.512388289244129)],\n",
       " [(24446, 123.80624165539012),\n",
       "  (848, 123.1974243311729),\n",
       "  (3900, 122.87086001306182),\n",
       "  (14478, 122.8068776605263),\n",
       "  (27223, 122.71740431047931),\n",
       "  (7955, 122.71626497831615),\n",
       "  (27839, 122.67007732679832),\n",
       "  (71065, 122.65618601966632),\n",
       "  (19784, 122.65169983733452),\n",
       "  (64700, 122.56566247309047),\n",
       "  (82995, 122.48240237593977),\n",
       "  (543, 122.48099005427204),\n",
       "  (9198, 122.38222919856534),\n",
       "  (71139, 122.34714491307405),\n",
       "  (66731, 122.33602807868061),\n",
       "  (432, 122.27422356584214),\n",
       "  (28426, 122.25212385568817),\n",
       "  (19026, 122.23175801436992),\n",
       "  (26294, 122.0759455263788),\n",
       "  (84141, 122.0514957170466)],\n",
       " [(29754, 8.849708005720451),\n",
       "  (90481, 8.681133093665151),\n",
       "  (22217, 8.508393649840176),\n",
       "  (90334, 8.345633635228637),\n",
       "  (90357, 8.27943432100263),\n",
       "  (27561, 8.195470720774253),\n",
       "  (29659, 8.140919477946674),\n",
       "  (29664, 7.97964991941628),\n",
       "  (29660, 7.7867370818082575),\n",
       "  (1169, 7.77742094696345),\n",
       "  (22586, 7.704224280473885),\n",
       "  (25619, 7.703795959893485),\n",
       "  (90322, 7.645801451939831),\n",
       "  (45969, 7.629066966485622),\n",
       "  (22038, 7.5314881882312585),\n",
       "  (45829, 7.513311854376506),\n",
       "  (90360, 7.498007330533525),\n",
       "  (23, 7.42087272987612),\n",
       "  (23617, 7.221142690638088),\n",
       "  (29142, 7.203078517488924)],\n",
       " [(64454, 9.4952332708862),\n",
       "  (64364, 8.587922414226462),\n",
       "  (71345, 8.182643593808965),\n",
       "  (71219, 7.897974629343535),\n",
       "  (70984, 7.331410850911503),\n",
       "  (64506, 7.277066667800135),\n",
       "  (71031, 7.2687716729240694),\n",
       "  (71282, 7.141181836626123),\n",
       "  (70964, 7.013501737042422),\n",
       "  (64323, 6.859574606043624),\n",
       "  (64419, 6.834315840986326),\n",
       "  (70966, 6.743277715777806),\n",
       "  (70994, 6.668894139775387),\n",
       "  (24457, 6.23907923077477),\n",
       "  (64372, 6.066783212981067),\n",
       "  (68373, 6.059498469628972),\n",
       "  (70972, 6.031476400047557),\n",
       "  (67464, 6.026916359953508),\n",
       "  (14599, 6.021578642446478),\n",
       "  (63580, 5.980117142658852)],\n",
       " [(12970, 12.779627574411652),\n",
       "  (64371, 12.417350666675194),\n",
       "  (49004, 12.208246089598425),\n",
       "  (64422, 11.768879296418262),\n",
       "  (59614, 10.898458985093027),\n",
       "  (65036, 10.804108134963268),\n",
       "  (64809, 10.75335415183326),\n",
       "  (59799, 10.730791965406553),\n",
       "  (60363, 10.607318284544611),\n",
       "  (15860, 9.91943092735917),\n",
       "  (18980, 9.65947596728186),\n",
       "  (47968, 9.571111172951483),\n",
       "  (52635, 9.556625882173108),\n",
       "  (81864, 9.423646712112799),\n",
       "  (59841, 9.2702491975917),\n",
       "  (64399, 9.070877003155104),\n",
       "  (9941, 8.803008866441772),\n",
       "  (54295, 8.770724345482856),\n",
       "  (68080, 8.224708892725598),\n",
       "  (4507, 7.848322403573998)],\n",
       " [(94680, 8.231484562143692),\n",
       "  (48259, 5.874275429658633),\n",
       "  (19869, 5.162304955953197),\n",
       "  (73827, 5.148971755036957),\n",
       "  (84145, 5.075541801045857),\n",
       "  (14818, 5.011350912035789),\n",
       "  (77720, 4.999989300747279),\n",
       "  (63681, 4.958411668741297),\n",
       "  (4534, 4.938585910415972),\n",
       "  (81174, 4.8292216360035045),\n",
       "  (91499, 4.823263360153035),\n",
       "  (93599, 4.818149009416576),\n",
       "  (38941, 4.770438297617725),\n",
       "  (36574, 4.766028797070996),\n",
       "  (35759, 4.7558122103268),\n",
       "  (60162, 4.7454974297746455),\n",
       "  (67346, 4.698064293093425),\n",
       "  (14110, 4.657776585768991),\n",
       "  (4575, 4.656634851218348),\n",
       "  (44484, 4.65445175590607)],\n",
       " [(3729, 105.60819324464155),\n",
       "  (58510, 104.36712292924179),\n",
       "  (85520, 104.06823549945565),\n",
       "  (47577, 9.551515341878902),\n",
       "  (94680, 9.439244023882916),\n",
       "  (57958, 9.285236399258565),\n",
       "  (48259, 9.214561338327265),\n",
       "  (48105, 9.165878075322475),\n",
       "  (4885, 8.824716533936368),\n",
       "  (53587, 8.71178186033476),\n",
       "  (3712, 8.622324511473362),\n",
       "  (64161, 8.541380971604818),\n",
       "  (57683, 8.452002936419047),\n",
       "  (34324, 8.45017331902115),\n",
       "  (57682, 8.360020317116627),\n",
       "  (63546, 8.350166145007432),\n",
       "  (3904, 8.324197883580549),\n",
       "  (94609, 8.313495109285583),\n",
       "  (57438, 8.247768843606053),\n",
       "  (3970, 8.190604376160046)],\n",
       " [(7090, 5.718542271729839),\n",
       "  (7107, 5.402283555644681),\n",
       "  (7273, 5.304391787611365),\n",
       "  (7129, 5.058377195806779),\n",
       "  (7372, 5.026960883203296),\n",
       "  (7236, 4.998282387354949),\n",
       "  (73033, 4.919054148616462),\n",
       "  (10670, 4.85302669141348),\n",
       "  (7089, 4.794384953531743),\n",
       "  (7277, 4.781964735107005),\n",
       "  (7164, 4.764112783169107),\n",
       "  (7176, 4.7411826825889305),\n",
       "  (89915, 4.729250714077249),\n",
       "  (7119, 4.712034174856326),\n",
       "  (7167, 4.7117502365457735),\n",
       "  (19374, 4.707066573453703),\n",
       "  (7116, 4.692029297541562),\n",
       "  (7156, 4.661180927505278),\n",
       "  (60339, 4.6558324207683475),\n",
       "  (98276, 4.574921408974871)],\n",
       " [(16350, 106.00539202906037),\n",
       "  (56865, 105.69167003182507),\n",
       "  (67885, 105.6047775326652),\n",
       "  (77173, 105.225866536297),\n",
       "  (91624, 105.06388432999452),\n",
       "  (90572, 104.67959344351726),\n",
       "  (83953, 104.60350820061728),\n",
       "  (79674, 104.49168131158675),\n",
       "  (91659, 104.43757862964966),\n",
       "  (4171, 104.30792769659786),\n",
       "  (8576, 104.17448422413383),\n",
       "  (6640, 104.05606282683779),\n",
       "  (68033, 104.03759019418275),\n",
       "  (94870, 104.02283751676343),\n",
       "  (8786, 103.92703456880484),\n",
       "  (91619, 103.76401681920294),\n",
       "  (8881, 103.74770071394761),\n",
       "  (76395, 103.67582605415068),\n",
       "  (90946, 103.66162211900081),\n",
       "  (70783, 103.55848205850582)],\n",
       " [(24440, 7.541164031514832),\n",
       "  (28110, 7.225186439501216),\n",
       "  (29931, 7.119583121013394),\n",
       "  (24140, 6.958876310354553),\n",
       "  (28507, 6.923790301444809),\n",
       "  (29939, 6.818039028238056),\n",
       "  (27818, 6.764544108779013),\n",
       "  (25893, 6.726304328350908),\n",
       "  (29180, 6.581702102464247),\n",
       "  (30022, 6.434862474064999),\n",
       "  (3685, 6.387503597024265),\n",
       "  (25902, 6.266790940381496),\n",
       "  (26671, 6.227977990772604),\n",
       "  (25263, 6.202563368451535),\n",
       "  (25290, 6.181834739756452),\n",
       "  (50159, 6.057859520024583),\n",
       "  (29944, 6.042531522275622),\n",
       "  (24559, 6.035480255718758),\n",
       "  (4278, 6.020102597664389),\n",
       "  (23787, 5.999902392526673)],\n",
       " [(8456, 8.519882461985922),\n",
       "  (94680, 8.476497903408974),\n",
       "  (9425, 8.273063671171537),\n",
       "  (9923, 8.190142738732096),\n",
       "  (64323, 7.982492066803785),\n",
       "  (2397, 7.928480828979255),\n",
       "  (10777, 7.772264812064302),\n",
       "  (65903, 7.7379958627882495),\n",
       "  (10641, 7.70464213816097),\n",
       "  (10205, 7.681838357134072),\n",
       "  (12326, 7.66412650545626),\n",
       "  (24457, 7.638409186988074),\n",
       "  (9779, 7.625965858373118),\n",
       "  (10010, 7.624822597170059),\n",
       "  (11498, 7.578763225400625),\n",
       "  (10856, 7.5502183859840795),\n",
       "  (12268, 7.541455490909819),\n",
       "  (9783, 7.537497790165569),\n",
       "  (3530, 7.395900555861168),\n",
       "  (90617, 7.364866236048275)],\n",
       " [(92137, 6.635720037822573),\n",
       "  (90970, 6.411967052142048),\n",
       "  (68295, 6.363983629981879),\n",
       "  (35479, 6.276887962541732),\n",
       "  (91216, 6.275241816474827),\n",
       "  (88818, 6.211974964358342),\n",
       "  (4186, 6.210684671388872),\n",
       "  (89397, 6.192984382235025),\n",
       "  (87678, 6.176321752654214),\n",
       "  (92126, 6.169621044861727),\n",
       "  (91157, 6.046152845519611),\n",
       "  (92159, 6.040858787694599),\n",
       "  (60590, 6.028873541329767),\n",
       "  (87042, 6.02813904328782),\n",
       "  (98444, 6.027932842728513),\n",
       "  (89747, 6.022826023625862),\n",
       "  (90969, 6.020453876942051),\n",
       "  (89730, 5.982015546303246),\n",
       "  (91156, 5.933353880446493),\n",
       "  (40390, 5.921476478476963)],\n",
       " [(27028, 105.72786791549304),\n",
       "  (30182, 103.98719865152427),\n",
       "  (28221, 103.31775764389928),\n",
       "  (64414, 102.92151373366018),\n",
       "  (64362, 102.69204561541022),\n",
       "  (11137, 102.5788674604303),\n",
       "  (30071, 102.55752399656133),\n",
       "  (8819, 102.54952179239393),\n",
       "  (89884, 102.45556709589187),\n",
       "  (95527, 102.43912400614555),\n",
       "  (90988, 102.39115608214479),\n",
       "  (83688, 102.38913493534537),\n",
       "  (83994, 102.36515364854431),\n",
       "  (84273, 102.3567416130626),\n",
       "  (90577, 102.3552417436586),\n",
       "  (28638, 102.28796466399262),\n",
       "  (88867, 102.25800776299502),\n",
       "  (30085, 102.25158651203672),\n",
       "  (87614, 102.10820013401353),\n",
       "  (22775, 102.07055790750012)],\n",
       " [(25726, 7.599450962815945),\n",
       "  (26655, 7.307534281553913),\n",
       "  (1273, 7.233563139290829),\n",
       "  (25396, 7.17954757109598),\n",
       "  (65150, 6.963294005191778),\n",
       "  (65092, 6.766912131808965),\n",
       "  (65056, 6.437754961597322),\n",
       "  (66451, 6.386894580281512),\n",
       "  (65065, 6.194746925521054),\n",
       "  (56424, 6.097413385705922),\n",
       "  (39471, 6.009633769316802),\n",
       "  (57440, 5.983075557049043),\n",
       "  (65290, 5.7321448284955405),\n",
       "  (65149, 5.66506261044179),\n",
       "  (65069, 5.500914474864704),\n",
       "  (65063, 5.354165877325519),\n",
       "  (65152, 5.346689133978938),\n",
       "  (25677, 5.298575418091866),\n",
       "  (56373, 5.149925538852179),\n",
       "  (65074, 5.1403485630560946)],\n",
       " [(55054, 8.46377832077468),\n",
       "  (73555, 7.377057456792563),\n",
       "  (22064, 7.28124984702233),\n",
       "  (23501, 7.0529977573864695),\n",
       "  (19205, 6.830986087777452),\n",
       "  (39829, 6.596425722676484),\n",
       "  (75289, 6.48173897501717),\n",
       "  (41719, 6.386135504256566),\n",
       "  (23139, 6.380484955281401),\n",
       "  (40910, 6.319918795951102),\n",
       "  (22386, 6.265568458520706),\n",
       "  (22246, 6.246835535865982),\n",
       "  (40062, 6.233947344006744),\n",
       "  (24881, 6.168944511220638),\n",
       "  (94680, 6.155122284214946),\n",
       "  (73078, 6.114579857896034),\n",
       "  (818, 6.069682810371419),\n",
       "  (23040, 5.9610018622184855),\n",
       "  (22777, 5.8638696535291075),\n",
       "  (65785, 5.799522120980685)],\n",
       " [(3685, 28.18605133750976),\n",
       "  (27028, 27.948735230085433),\n",
       "  (7526, 27.731996017463608),\n",
       "  (57958, 26.99905612702475),\n",
       "  (25433, 26.948740697620742),\n",
       "  (1799, 26.93524307024989),\n",
       "  (25024, 26.8394262921858),\n",
       "  (43237, 26.656367952575703),\n",
       "  (10835, 26.51382607577005),\n",
       "  (493, 26.491939573306976),\n",
       "  (46244, 26.429299635499778),\n",
       "  (68388, 26.327100441773982),\n",
       "  (47607, 26.297530617276077),\n",
       "  (26760, 26.109870682129895),\n",
       "  (26896, 26.038669426911838),\n",
       "  (71282, 26.008702464221408),\n",
       "  (1126, 25.929210793279115),\n",
       "  (1147, 25.924750077952286),\n",
       "  (298, 25.917275614978443),\n",
       "  (53587, 25.91246027581223)],\n",
       " [(74999, 106.81832274238522),\n",
       "  (57438, 104.16822691286134),\n",
       "  (38427, 103.60358804685227),\n",
       "  (78753, 103.19944251937797),\n",
       "  (53215, 103.0114471060155),\n",
       "  (11190, 102.99483901895805),\n",
       "  (76693, 102.89833251227458),\n",
       "  (50639, 102.73264270682577),\n",
       "  (80065, 102.70522627136778),\n",
       "  (48315, 102.69965461081262),\n",
       "  (28445, 102.47575257024253),\n",
       "  (38399, 102.33477068041432),\n",
       "  (95222, 102.31510302248071),\n",
       "  (24435, 102.28530542823223),\n",
       "  (77751, 102.27781272149483),\n",
       "  (40873, 102.21997801156931),\n",
       "  (90982, 102.20681227697708),\n",
       "  (31759, 102.16415783288085),\n",
       "  (91582, 102.02529193033365),\n",
       "  (34277, 102.01786939241867)],\n",
       " [(24143, 8.748605632861949),\n",
       "  (23921, 8.61996035863053),\n",
       "  (24568, 8.186414590618394),\n",
       "  (23995, 8.125697068506675),\n",
       "  (23912, 7.874630087538522),\n",
       "  (29798, 7.844171700049165),\n",
       "  (23951, 7.8186296581443475),\n",
       "  (24125, 7.620165833955237),\n",
       "  (24087, 7.546698282641745),\n",
       "  (24464, 7.537364072692798),\n",
       "  (24017, 7.537014254410418),\n",
       "  (23932, 7.443120645222224),\n",
       "  (24451, 7.319400222064507),\n",
       "  (23967, 7.1693440701813085),\n",
       "  (23952, 7.143878557152263),\n",
       "  (24694, 7.113080766632709),\n",
       "  (24049, 7.0305409295551815),\n",
       "  (24379, 7.010769562174026),\n",
       "  (3418, 7.007801164765417),\n",
       "  (24357, 6.958839787288156)],\n",
       " [(782, 8.812562292343397),\n",
       "  (40556, 8.573813870845436),\n",
       "  (27511, 8.39621237040808),\n",
       "  (22715, 8.013219442991357),\n",
       "  (28363, 7.799692737419055),\n",
       "  (58493, 7.780483130836114),\n",
       "  (25362, 7.672660625980427),\n",
       "  (24113, 7.59299216768198),\n",
       "  (25488, 7.5455870700946726),\n",
       "  (164, 7.484441475038562),\n",
       "  (475, 7.461534001528156),\n",
       "  (30736, 7.431016958116102),\n",
       "  (22296, 7.393168393019597),\n",
       "  (27770, 7.360715491294429),\n",
       "  (22910, 7.3201753932100955),\n",
       "  (830, 7.309061757176013),\n",
       "  (26307, 7.305183680136978),\n",
       "  (26185, 7.253840051128041),\n",
       "  (28828, 7.212445713603781),\n",
       "  (29785, 7.193909539747928)],\n",
       " [(4161, 12.106040982975468),\n",
       "  (4721, 11.218087417171434),\n",
       "  (5077, 10.886237298911473),\n",
       "  (29080, 10.526253422457685),\n",
       "  (22191, 10.377793701481968),\n",
       "  (5141, 10.362853018434858),\n",
       "  (7671, 10.178782729501481),\n",
       "  (4244, 10.128885724037904),\n",
       "  (26359, 10.10867622771171),\n",
       "  (7090, 10.04516379729964),\n",
       "  (24144, 9.659229033821049),\n",
       "  (5229, 9.487649970214239),\n",
       "  (4539, 9.359476266842767),\n",
       "  (10670, 9.301398063834174),\n",
       "  (9687, 9.177796306795438),\n",
       "  (5130, 9.129214821447242),\n",
       "  (17614, 8.889997804888363),\n",
       "  (55569, 8.820965614490937),\n",
       "  (11279, 8.762516976523962),\n",
       "  (73751, 8.735796685464749)],\n",
       " [(94680, 8.23816306973208),\n",
       "  (10449, 7.267397973086329),\n",
       "  (7593, 6.943268825923006),\n",
       "  (42027, 6.429085841233193),\n",
       "  (35024, 6.423524224932481),\n",
       "  (35830, 6.3942420993553775),\n",
       "  (33880, 6.292153916523086),\n",
       "  (36220, 6.235064866330308),\n",
       "  (49933, 6.22758187490227),\n",
       "  (46668, 6.161601104914492),\n",
       "  (42280, 6.121743559851948),\n",
       "  (52666, 6.117884512412329),\n",
       "  (46648, 6.114140254934501),\n",
       "  (48267, 6.086868601288601),\n",
       "  (42907, 6.067604949441668),\n",
       "  (42261, 6.050665816959234),\n",
       "  (48105, 6.040233836904386),\n",
       "  (46548, 6.04013955839927),\n",
       "  (39730, 6.039165936912889),\n",
       "  (65237, 6.010595441020496)],\n",
       " [(32804, 104.87094578936671),\n",
       "  (33389, 104.82024189559436),\n",
       "  (34930, 104.79611514904144),\n",
       "  (41593, 104.3116859357721),\n",
       "  (32849, 104.22222374308744),\n",
       "  (43335, 103.95960229297569),\n",
       "  (50434, 103.4218688164101),\n",
       "  (50119, 103.38860286971769),\n",
       "  (56429, 103.25020288305996),\n",
       "  (35290, 103.12136876650546),\n",
       "  (40119, 102.79437353420954),\n",
       "  (36330, 102.6140237886888),\n",
       "  (69798, 102.27628663716364),\n",
       "  (77216, 102.08661800913984),\n",
       "  (31865, 9.028745942129131),\n",
       "  (31318, 8.684133483687496),\n",
       "  (32254, 8.454950190890852),\n",
       "  (31364, 8.150817519582597),\n",
       "  (18927, 8.130818718378254),\n",
       "  (32218, 8.086621454207394)],\n",
       " [(250, 10.756539984069155),\n",
       "  (20192, 10.539480091070194),\n",
       "  (41390, 10.261190826750031),\n",
       "  (11803, 10.203500151156277),\n",
       "  (47463, 10.161090555897959),\n",
       "  (826, 10.153566698796329),\n",
       "  (13415, 10.107309865976024),\n",
       "  (10245, 10.021967398460962),\n",
       "  (74222, 9.995364988327365),\n",
       "  (13562, 9.929490726090261),\n",
       "  (13325, 9.873917209418801),\n",
       "  (15971, 9.862021147276685),\n",
       "  (958, 9.834887836667436),\n",
       "  (13513, 9.77400067040423),\n",
       "  (77047, 9.654412357500695),\n",
       "  (52479, 9.634755809453974),\n",
       "  (74208, 9.611744713884832),\n",
       "  (13554, 9.561218896023306),\n",
       "  (13542, 9.520132533882617),\n",
       "  (6678, 9.509869141800834)],\n",
       " [(17083, 26.690472503360407),\n",
       "  (27337, 26.302389512701247),\n",
       "  (55565, 26.01934032390649),\n",
       "  (64462, 25.917678993490647),\n",
       "  (55007, 25.90406966433045),\n",
       "  (17487, 25.6711759814653),\n",
       "  (4720, 25.632846426913325),\n",
       "  (17670, 25.536703628153354),\n",
       "  (28636, 25.514966377020336),\n",
       "  (64364, 25.508073180469992),\n",
       "  (50447, 25.498486012005685),\n",
       "  (9686, 25.481742033267178),\n",
       "  (29653, 25.466371593095253),\n",
       "  (57333, 25.416111667283747),\n",
       "  (51571, 25.33402478817873),\n",
       "  (18201, 25.32403425642663),\n",
       "  (18210, 25.260928507892462),\n",
       "  (59283, 25.25247015995584),\n",
       "  (17996, 25.222974496741163),\n",
       "  (89931, 25.221510132756315)],\n",
       " [(9372, 106.19463509207702),\n",
       "  (6293, 105.47558332645521),\n",
       "  (50519, 105.37134147019782),\n",
       "  (6149, 105.08796136696813),\n",
       "  (4976, 105.07349430404561),\n",
       "  (9447, 104.87465647263693),\n",
       "  (11640, 104.8236888189879),\n",
       "  (18940, 104.74732278037504),\n",
       "  (86443, 104.74522773694346),\n",
       "  (19386, 104.74440701629007),\n",
       "  (86056, 104.62642115958002),\n",
       "  (21588, 104.61245411577487),\n",
       "  (8704, 104.53369940427305),\n",
       "  (9658, 104.51789553364877),\n",
       "  (4488, 104.50333273232275),\n",
       "  (18879, 104.48983284951812),\n",
       "  (85999, 104.48097145363015),\n",
       "  (84459, 104.33641396772909),\n",
       "  (5569, 104.31378143929778),\n",
       "  (83426, 104.29800138089779)],\n",
       " [(30808, 7.805822261527273),\n",
       "  (781, 6.788909409099725),\n",
       "  (58641, 6.464788904899608),\n",
       "  (30807, 6.437235240684771),\n",
       "  (1927, 6.389099088835422),\n",
       "  (8641, 6.349879686684401),\n",
       "  (94481, 6.188565142070879),\n",
       "  (83475, 6.072511314787809),\n",
       "  (94680, 5.989573937582233),\n",
       "  (94399, 5.969348683728597),\n",
       "  (8476, 5.950862612291191),\n",
       "  (25655, 5.936266367140783),\n",
       "  (5398, 5.852968344607895),\n",
       "  (30809, 5.810331445126294),\n",
       "  (5317, 5.806582328444594),\n",
       "  (40789, 5.803527869481781),\n",
       "  (25516, 5.766598100803979),\n",
       "  (33798, 5.733346776879818),\n",
       "  (27839, 5.725240208369273),\n",
       "  (9623, 5.717196480155126)],\n",
       " [(5398, 10.179881268316716),\n",
       "  (5501, 9.592994863698305),\n",
       "  (4731, 8.93499647796661),\n",
       "  (4468, 8.789308447986993),\n",
       "  (73348, 8.743999786572891),\n",
       "  (5202, 8.721954539705724),\n",
       "  (84980, 8.719728765538314),\n",
       "  (6214, 8.473414289435848),\n",
       "  (19535, 8.459664213304006),\n",
       "  (94680, 8.37722181804719),\n",
       "  (11658, 8.23927704460123),\n",
       "  (871, 8.18447883016497),\n",
       "  (5885, 8.143796603657862),\n",
       "  (5703, 8.08025106857982),\n",
       "  (10647, 8.003350426738715),\n",
       "  (4377, 7.823547868078549),\n",
       "  (8633, 7.783737814789971),\n",
       "  (5294, 7.729913990698445),\n",
       "  (4159, 7.686871051535233),\n",
       "  (10376, 7.665253118526039)],\n",
       " [(65543, 8.306477967272807),\n",
       "  (94680, 8.157928155911007),\n",
       "  (65874, 7.465787808116632),\n",
       "  (57613, 7.307298745828071),\n",
       "  (65605, 7.219464861464454),\n",
       "  (65564, 7.196392614260171),\n",
       "  (57948, 7.04037763546309),\n",
       "  (65579, 7.033556490703422),\n",
       "  (65714, 7.031606015602693),\n",
       "  (58485, 6.977507889076277),\n",
       "  (65578, 6.87559566552042),\n",
       "  (65611, 6.841225687339996),\n",
       "  (65744, 6.82997352681012),\n",
       "  (65590, 6.791423660594775),\n",
       "  (1116, 6.773157961190565),\n",
       "  (73333, 6.719071452110645),\n",
       "  (41141, 6.6930914374632415),\n",
       "  (66080, 6.687880534048359),\n",
       "  (65903, 6.686402677214716),\n",
       "  (65989, 6.682066164782411)],\n",
       " [(25777, 107.99194170657825),\n",
       "  (25265, 107.0921965347533),\n",
       "  (25609, 106.39175097332003),\n",
       "  (27535, 105.94935121925904),\n",
       "  (95547, 105.09731632105488),\n",
       "  (28332, 103.31752229773447),\n",
       "  (25406, 102.7465525568342),\n",
       "  (87780, 101.10264475019436),\n",
       "  (25024, 11.851933355993088),\n",
       "  (3685, 10.544699333250293),\n",
       "  (25012, 10.242771869677222),\n",
       "  (25039, 9.620879156162719),\n",
       "  (25433, 9.347637224245394),\n",
       "  (23918, 9.310303348641888),\n",
       "  (25985, 9.231247060325206),\n",
       "  (24976, 9.228640464046414),\n",
       "  (25230, 8.936934881028368),\n",
       "  (25443, 8.936490932973616),\n",
       "  (25153, 8.89466319124039),\n",
       "  (25902, 8.726484041913356)],\n",
       " [(15929, 7.564336505784394),\n",
       "  (83630, 7.255313060045993),\n",
       "  (47783, 7.12450629172568),\n",
       "  (24568, 6.867544016503857),\n",
       "  (24636, 6.857795410859697),\n",
       "  (2200, 6.810682677861554),\n",
       "  (24821, 6.563806414777465),\n",
       "  (82575, 6.560946389283232),\n",
       "  (24017, 6.393367934292771),\n",
       "  (24666, 6.357060504799056),\n",
       "  (67533, 6.323456729200693),\n",
       "  (24549, 6.306000730487909),\n",
       "  (24464, 6.293933799248597),\n",
       "  (3418, 6.2394464250428685),\n",
       "  (34356, 6.237028416079154),\n",
       "  (24743, 6.17039341752617),\n",
       "  (24087, 6.101455500707944),\n",
       "  (34024, 6.0923001090977795),\n",
       "  (53513, 6.078740235718797),\n",
       "  (24750, 6.054369806673418)],\n",
       " [(91072, 103.08961899196044),\n",
       "  (90978, 101.90116010444873),\n",
       "  (95167, 101.76063902051398),\n",
       "  (90565, 101.57949636803987),\n",
       "  (94680, 7.870852417305912),\n",
       "  (16358, 5.527043453009465),\n",
       "  (2009, 5.475648263680577),\n",
       "  (89151, 5.334457293554476),\n",
       "  (15983, 5.217578763622287),\n",
       "  (94479, 5.195118185617041),\n",
       "  (4300, 5.14468576198455),\n",
       "  (77091, 5.128172598342429),\n",
       "  (74718, 5.100331583366563),\n",
       "  (41405, 5.089631532225549),\n",
       "  (46844, 5.057656337532962),\n",
       "  (85090, 5.026597493674174),\n",
       "  (18202, 5.011677785766469),\n",
       "  (27028, 4.995539471179944),\n",
       "  (87824, 4.994737767943066),\n",
       "  (16065, 4.946569911866941)],\n",
       " [(25619, 9.997869721632787),\n",
       "  (63866, 9.292920583340107),\n",
       "  (23617, 9.281252661365976),\n",
       "  (90300, 8.872734195562053),\n",
       "  (25726, 8.817031115429785),\n",
       "  (90333, 8.703751962719037),\n",
       "  (47557, 8.48966178579893),\n",
       "  (63411, 8.392067250843624),\n",
       "  (63546, 8.333745222352288),\n",
       "  (90299, 8.256324165687701),\n",
       "  (29664, 8.108957598412937),\n",
       "  (25674, 8.077993728043584),\n",
       "  (90301, 8.054536730663486),\n",
       "  (26724, 7.983102132811659),\n",
       "  (29816, 7.941063500864507),\n",
       "  (94680, 7.869426326452405),\n",
       "  (29663, 7.868569124183205),\n",
       "  (26435, 7.755955192692564),\n",
       "  (25515, 7.716464045190925),\n",
       "  (29661, 7.708076064995019)],\n",
       " [(94680, 29.37167842070611),\n",
       "  (48105, 26.854676080644282),\n",
       "  (74025, 26.37844386907865),\n",
       "  (52666, 26.317463184222213),\n",
       "  (27337, 26.314382234790273),\n",
       "  (58425, 26.226149607243133),\n",
       "  (25436, 26.086808227386143),\n",
       "  (34710, 26.00655108340038),\n",
       "  (44384, 25.997369440915964),\n",
       "  (68744, 25.95182019343435),\n",
       "  (64393, 25.90058740255286),\n",
       "  (35939, 25.893137354757187),\n",
       "  (83694, 25.86981548508456),\n",
       "  (17996, 25.86821071670888),\n",
       "  (74262, 25.861230985718084),\n",
       "  (3685, 25.843388574013257),\n",
       "  (90804, 25.82032124007448),\n",
       "  (57682, 25.772124124447423),\n",
       "  (10005, 25.767433292181213),\n",
       "  (44377, 25.74998583517476)],\n",
       " [(86067, 101.8534049847066),\n",
       "  (95348, 101.80097501265945),\n",
       "  (95049, 101.54372826342855),\n",
       "  (45910, 101.45077771254377),\n",
       "  (44359, 101.44828150859935),\n",
       "  (46025, 101.40090094412717),\n",
       "  (94680, 8.10932883455614),\n",
       "  (58425, 5.0243379550926575),\n",
       "  (39454, 4.90008105254452),\n",
       "  (57682, 4.814812433432433),\n",
       "  (71192, 4.793317740826083),\n",
       "  (44384, 4.767043432930365),\n",
       "  (63207, 4.687617941601559),\n",
       "  (10005, 4.68644070354828),\n",
       "  (27808, 4.561707348394704),\n",
       "  (55029, 4.435998467055321),\n",
       "  (34710, 4.430454877650341),\n",
       "  (82202, 4.418124310181405),\n",
       "  (11258, 4.374698513277661),\n",
       "  (59321, 4.373756216052749)],\n",
       " [(87, 123.33698631827255),\n",
       "  (21501, 102.44817387570015),\n",
       "  (24993, 27.81159147923581),\n",
       "  (94474, 27.497323475180732),\n",
       "  (90804, 27.3408387279236),\n",
       "  (27724, 27.085547414327372),\n",
       "  (25811, 27.064339413076603),\n",
       "  (25283, 26.9697762637486),\n",
       "  (86665, 26.871251750588705),\n",
       "  (2198, 26.75215589649345),\n",
       "  (94481, 26.541500433122145),\n",
       "  (24088, 26.470070188648336),\n",
       "  (25282, 26.226961478413127),\n",
       "  (3273, 26.187959872662493),\n",
       "  (27421, 26.164594021777923),\n",
       "  (94479, 26.146086503109323),\n",
       "  (25001, 26.100243976992907),\n",
       "  (26898, 26.07854794696745),\n",
       "  (39454, 25.95653638315357),\n",
       "  (1533, 25.910583458275784)]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e43f423-e5bb-4b66-9ba5-b8f55943602c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Attention Augmented Convolutional Networks', 'doi': '10.1109/iccv.2019.00338', 'openalex_id': 'https://openalex.org/W2981413347', 'authors': ['Irwan Bello', 'Barret Zoph', 'Quoc V. Le', 'Ashish Vaswani', 'Jonathon Shlens'], 'publication_date': '2019-10-01', 'publish_year': 2019, 'keywords': ['Computer science', 'Convolutional neural network', 'Convolution (computer science)', 'Artificial intelligence', 'Feature (linguistics)', 'Neighbourhood (mathematics)', 'Pattern recognition (psychology)', 'Object detection', 'Set (abstract data type)', 'Object (grammar)', 'Baseline (sea)', 'Machine learning', 'Artificial neural network', 'Mathematics', 'Mathematical analysis', 'Philosophy', 'Linguistics', 'Programming language', 'Oceanography', 'Geology'], 'abstract': 'Convolutional networks have enjoyed much success in many computer vision applications. The convolution operation however has a significant weakness in that it only operates on a local neighbourhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we propose to augment convolutional networks with self-attention by concatenating convolutional feature maps with a set of feature maps produced via a novel relative self-attention mechanism. In particular, we extend previous work on relative self-attention over sequences to images and discuss a memory efficient implementation. Unlike Squeeze-and-Excitation, which performs attention over the channels and ignores spatial information, our self-attention mechanism attends jointly to both features and spatial locations while preserving translation equivariance. We find that Attention Augmentation leads to consistent improvements in image classification on ImageNet and object detection on COCO across many different models and scales, including ResNets and a state-of-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a 1.3% top-1 accuracy improvement on ImageNet classification over a ResNet50 baseline and outperforms other attention mechanisms for images such as Squeeze-and-Excitation. It also achieves an improvement of 1.4 AP in COCO Object Detection on top of a RetinaNet baseline.', 'global_link_openable': 'https://openalex.org/W2981413347', 'citation_count': 964, 'publication': [{'venue_name': 'arXiv (Cornell University)', 'publisher': 'https://openalex.org/I205783295', 'pdf_url': 'https://arxiv.org/pdf/1904.09925'}], 'references': [{'openalex_id': 'https://openalex.org/W2803137490'}, {'openalex_id': 'https://openalex.org/W2884585870'}, {'openalex_id': 'https://openalex.org/W2890166761'}, {'openalex_id': 'https://openalex.org/W2891778567'}, {'openalex_id': 'https://openalex.org/W2894741346'}, {'openalex_id': 'https://openalex.org/W2898732869'}, {'openalex_id': 'https://openalex.org/W2899423466'}, {'openalex_id': 'https://openalex.org/W2903105043'}, {'openalex_id': 'https://openalex.org/W2907502844'}, {'openalex_id': 'https://openalex.org/W2949718784'}, {'openalex_id': 'https://openalex.org/W2952332632'}, {'openalex_id': 'https://openalex.org/W2952634764'}, {'openalex_id': 'https://openalex.org/W2963045198'}, {'openalex_id': 'https://openalex.org/W2963091558'}, {'openalex_id': 'https://openalex.org/W2963163009'}, {'openalex_id': 'https://openalex.org/W2963351448'}, {'openalex_id': 'https://openalex.org/W2963623257'}, {'openalex_id': 'https://openalex.org/W2963843116'}, {'openalex_id': 'https://openalex.org/W2963918968'}, {'openalex_id': 'https://openalex.org/W2963984455'}, {'openalex_id': 'https://openalex.org/W2964444661'}, {'openalex_id': 'https://openalex.org/W4295253143'}, {'openalex_id': 'https://openalex.org/W4295727797'}, {'openalex_id': 'https://openalex.org/W4297810817'}, {'openalex_id': 'https://openalex.org/W4298395628'}, {'openalex_id': 'https://openalex.org/W4320930577'}, {'openalex_id': 'https://openalex.org/W4394659543'}], 'text': \"Attention Augmented Convolutional Networks\\nIrwan Bello\\nBarret Zoph\\nAshish Vaswani\\nJonathon Shlens\\nQuoc V. Le\\nGoogle Brain\\n{ibello,barretzoph,avaswani,shlens,qvl}@google.com\\nAbstract\\nConvolutional networks have been the paradigm of\\nchoice in many computer vision applications. The convolu-\\ntion operation however has a signiﬁcant weakness in that it\\nonly operates on a local neighborhood, thus missing global\\ninformation. Self-attention, on the other hand, has emerged\\nas a recent advance to capture long range interactions, but\\nhas mostly been applied to sequence modeling and gener-\\native modeling tasks. In this paper, we consider the use of\\nself-attention for discriminative visual tasks as an alterna-\\ntive to convolutions. We introduce a novel two-dimensional\\nrelative self-attention mechanism that proves competitive\\nin replacing convolutions as a stand-alone computational\\nprimitive for image classiﬁcation. We ﬁnd in control exper-\\niments that the best results are obtained when combining\\nboth convolutions and self-attention. We therefore propose\\nto augment convolutional operators with this self-attention\\nmechanism by concatenating convolutional feature maps\\nwith a set of feature maps produced via self-attention. Ex-\\ntensive experiments show that Attention Augmentation leads\\nto consistent improvements in image classiﬁcation on Im-\\nageNet and object detection on COCO across many dif-\\nferent models and scales, including ResNets and a state-\\nof-the art mobile constrained network, while keeping the\\nnumber of parameters similar. In particular, our method\\nachieves a 1.3% top-1 accuracy improvement on ImageNet\\nclassiﬁcation over a ResNet50 baseline and outperforms\\nother attention mechanisms for images such as Squeeze-\\nand-Excitation [17]. It also achieves an improvement of\\n1.4 mAP in COCO Object Detection on top of a RetinaNet\\nbaseline.\\n1. Introduction\\nConvolutional Neural Networks have enjoyed tremen-\\ndous success in many computer vision applications, espe-\\ncially in image classiﬁcation [24, 23]. The design of the\\nconvolutional layer imposes 1) locality via a limited recep-\\ntive ﬁeld and 2) translation equivariance via weight sharing.\\nFigure 1. Attention Augmentation systematically improves im-\\nage classiﬁcation across a large variety of networks of different\\nscales. ImageNet classiﬁcation accuracy [9] versus the number of\\nparameters for baseline models (ResNet) [14], models augmented\\nwith channel-wise attention (SE-ResNet) [17] and our proposed\\narchitecture (AA-ResNet).\\nBoth these properties prove to be crucial inductive biases\\nwhen designing models that operate over images. However,\\nthe local nature of the convolutional kernel prevents it from\\ncapturing global contexts in an image, often necessary for\\nbetter recognition of objects in images [33].\\nSelf-attention [43], on the other hand, has emerged as a\\nrecent advance to capture long range interactions, but has\\nmostly been applied to sequence modeling and generative\\nmodeling tasks. The key idea behind self-attention is to\\nproduce a weighted average of values computed from hid-\\nden units. Unlike the pooling or the convolutional operator,\\nthe weights used in the weighted average operation are pro-\\nduced dynamically via a similarity function between hid-\\nden units. As a result, the interaction between input signals\\ndepends on the signals themselves rather than being prede-\\ntermined by their relative location like in convolutions. In\\nparticular, this allows self-attention to capture long range\\n\\x14\\x13\\x19\\x16\\n\\x13\\x11\\x12\\x1a\\x01*&&&\\x10$7'\\x01*OUFSOBUJPOBM\\x01$POGFSFODF\\x01PO\\x01$PNQVUFS\\x017JTJPO\\x01\\t*$$7\\n\\x13\\x14\\x19\\x11\\x0e\\x18\\x16\\x11\\x15\\x10\\x12\\x1a\\x10\\x05\\x14\\x12\\x0f\\x11\\x11\\x01¥\\x13\\x11\\x12\\x1a\\x01*&&&\\n%0*\\x01\\x12\\x11\\x0f\\x12\\x12\\x11\\x1a\\x10*$$7\\x0f\\x13\\x11\\x12\\x1a\\x0f\\x11\\x11\\x14\\x14\\x19\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 15,2024 at 11:45:57 UTC from IEEE Xplore.  Restrictions apply. \\nInput\\nAttention maps\\nWeighted average of the values\\nStandard convolution\\nOutput\\nW\\nH\\nNh = 2\\nValues\\n²\\nHead\\nHead\\n¹\\nFigure 2. Attention-augmented convolution: For each spatial location (h, w), Nh attention maps over the image are computed from\\nqueries and keys. These attention maps are used to compute Nh weighted averages of the values V. The results are then concatenated,\\nreshaped to match the original volume’s spatial dimensions and mixed with a pointwise convolution. Multi-head attention is applied in\\nparallel to a standard convolution operation and the outputs are concatenated.\\ninteractions without increasing the number of parameters.\\nIn this paper, we consider the use of self-attention for\\ndiscriminative visual tasks as an alternative to convolu-\\ntions. We develop a novel two-dimensional relative self-\\nattention mechanism [37] that maintains translation equiv-\\nariance while being infused with relative position informa-\\ntion, making it well suited for images. Our self-attention\\nformulation proves competitive for replacing convolutions\\nentirely, however we ﬁnd in control experiments that the\\nbest results are obtained when combining both. We there-\\nfore do not completely abandon the idea of convolutions,\\nbut instead propose to augment convolutions with this self-\\nattention mechanism.\\nThis is achieved by concatenating\\nconvolutional feature maps, which enforce locality, to self-\\nattentional feature maps capable of modeling longer range\\ndependencies (see Figure 2).\\nWe test our method on the CIFAR-100 and ImageNet\\nclassiﬁcation [22, 9] and the COCO object detection [27]\\ntasks, across a wide range of architectures at different com-\\nputational budgets, including a state-of-the art resource\\nconstrained architecture [42].\\nAttention Augmentation\\nyields systematic improvements with minimal additional\\ncomputational burden and notably outperforms the popu-\\nlar Squeeze-and-Excitation [17] channelwise attention ap-\\nproach in all experiments. In particular, Attention Augmen-\\ntation achieves a 1.3% top-1 accuracy ImageNet on top of\\na ResNet50 baseline and 1.4 mAP increase in COCO ob-\\nject detection on top of a RetinaNet baseline. Suprisingly,\\nexperiments also reveal that fully self-attentional models,\\na special case of Attention Augmentation, only perform\\nslightly worse than their fully convolutional counterparts on\\nImageNet, indicating that self-attention is a powerful stand-\\nalone computational primitive for image classiﬁcation.\\n2. Related Work\\n2.1. Convolutional networks\\nModern computer vision has been built on powerful im-\\nage featurizers learned on image classiﬁcation tasks such\\nas CIFAR-10 [22] and ImageNet [9]. These datasets have\\nbeen used as benchmarks for delineating better image fea-\\nturizations and network architectures across a broad range\\nof tasks [21]. For example, improving the “backbone” net-\\nwork typically leads to improvements in object detection\\n[19] and image segmentation [6]. These observations have\\ninspired the research and design of new architectures, which\\nare typically derived from the composition of convolution\\noperations across an array of spatial scales and skip con-\\nnections [23, 41, 39, 40, 14, 47, 13]. Indeed, automated\\nsearch strategies for designing architectures based on con-\\nvolutional primitives result in state-of-the-art accuracy on\\nlarge-scale image classiﬁcation tasks that translate across a\\nrange of tasks [55, 21].\\n2.2. Attention mechanisms in networks\\nAttention has enjoyed widespread adoption as a com-\\nputational module for modeling sequences because of its\\nability to capture long distance interactions [2, 44, 4, 3].\\nMost notably, Bahdanau et al. [2] ﬁrst proposed to com-\\nbine attention with a Recurrent Neural Network [15] for\\nalignment in Machine Translation. Attention was further\\nextended by Vaswani et al. [43], where the self-attentional\\nTransformer architecture achieved state-of-the-art results in\\nMachine Translation. Using self-attention in cooperation\\nwith convolutions is a theme shared by recent work in Nat-\\nural Language Processing [49] and Reinforcement Learn-\\ning [52]. For example, the QANet [50] and Evolved Trans-\\n\\x14\\x13\\x19\\x17\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 15,2024 at 11:45:57 UTC from IEEE Xplore.  Restrictions apply. \\nformer [38] architectures alternate between self-attention\\nlayers and convolution layers for Question Answering ap-\\nplications and Machine Translation respectively.\\nAddi-\\ntionally, multiple attention mechanisms have been pro-\\nposed for visual tasks to address the weaknesses of con-\\nvolutions [17, 16, 7, 46, 45, 53]. For instance, Squeeze-\\nand-Excitation [17] and Gather-Excite [16] reweigh feature\\nchannels using signals aggregated from entire feature maps,\\nwhile BAM [31] and CBAM [46] reﬁne convolutional fea-\\ntures independently in the channel and spatial dimensions.\\nIn non-local neural networks [45], improvements are shown\\nin video classiﬁcation and object detection via the addi-\\ntive use of a few non-local residual blocks that employ\\nself-attention in convolutional architectures. However, non-\\nlocal blocks are only added to the architecture after Ima-\\ngeNet pretraining and are initialized in such a way that they\\ndo not break pretraining.\\nIn contrast, our attention augmented networks do not rely\\non pretraining of their fully convolutional counterparts and\\nemploy self-attention along the entire architecture. The use\\nof multi-head attention allows the model to attend jointly\\nto both spatial and feature subspaces. Additionally, we en-\\nhance the representational power of self-attention over im-\\nages by extending relative self-attention [37, 18] to two di-\\nmensional inputs allowing us to model translation equivari-\\nance in a principled way. Finally our method produces addi-\\ntional feature maps, rather than recalibrating convolutional\\nfeatures via addition [45, 53] or gating [17, 16, 31, 46]. This\\nproperty allows us to ﬂexibly adjust the fraction of atten-\\ntional channels and consider a spectrum of architectures,\\nranging from fully convolutional to fully attentional mod-\\nels.\\n3. Methods\\nWe now formally describe our proposed Attention Aug-\\nmentation method. We use the following naming conven-\\ntions: H, W and Fin refer to the height, width and number\\nof input ﬁlters of an activation map. Nh, dv and dk respec-\\ntively refer the number of heads, the depth of values and the\\ndepth of queries and keys in multihead-attention (MHA).\\nWe further assume that Nh divides dv and dk evenly and\\ndenote dh\\nv and dh\\nk the depth of values and queries/keys per\\nattention head.\\n3.1. Self-attention over images\\nGiven an input tensor of shape (H, W, Fin),1 we ﬂatten\\nit to a matrix X ∈RHW ×Fin and perform multihead atten-\\ntion as proposed in the Transformer architecture [43]. The\\noutput of the self-attention mechanism for a single head h\\n1We omit the batch dimension for simplicity.\\ncan be formulated as:\\nOh = Softmax\\n⎛\\n⎝(XWq)(XWk)T\\n\\x04\\ndh\\nk\\n⎞\\n⎠(XWv)\\n(1)\\nwhere Wq, Wk ∈RFin×dh\\nk and Wv ∈RFin×dh\\nv are learned\\nlinear transformations that map the input X to queries Q =\\nXWq, keys K = XWk and values V = XWv. The outputs\\nof all heads are then concatenated and projected again as\\nfollows:\\nMHA(X) = Concat\\n\\x07\\nO1, . . . , ONh\\n\\x08\\nW O\\n(2)\\nwhere W O ∈Rdv×dv is a learned linear transformation.\\nMHA(X) is then reshaped into a tensor of shape (H, W, dv)\\nto match the original spatial dimensions.\\nWe note that\\nmulti-head attention incurs a complexity of O((HW)2dk)\\nand a memory cost of O((HW)2Nh) as it requires to store\\nattention maps for each head.\\n3.1.1\\nTwo-dimensional Positional Encodings\\nWithout explicit information about positions, self-attention\\nis permutation equivariant:\\nMHA(π(X)) = π(MHA(X))\\nfor any permutation π of the pixel locations, making it in-\\neffective for modeling highly structured data such as im-\\nages. Multiple positional encodings that augment activation\\nmaps with explicit spatial information have been proposed\\nto alleviate related issues. In particular, the Image Trans-\\nformer [32] extends the sinusoidal waves ﬁrst introduced in\\nthe original Transformer [43] to 2 dimensional inputs and\\nCoordConv [29] concatenates positional channels to an ac-\\ntivation map.\\nHowever these encodings did not help in our experi-\\nments on image classiﬁcation and object detection (see Sec-\\ntion 4.5). We hypothesize that this is because such posi-\\ntional encodings, while not permutation equivariant, do not\\nsatisfy translation equivariance, which is a desirable prop-\\nerty when dealing with images. As a solution, we propose\\nto extend the use of relative position encodings [37] to two\\ndimensions and present a memory efﬁcient implementation\\nbased on the Music Transformer [18].\\nRelative positional encodings:\\nIntroduced in [37] for the\\npurpose of language modeling, relative self-attention aug-\\nments self-attention with relative position encodings and\\nenables translation equivariance while preventing permuta-\\ntion equivariance. We implement two-dimensional relative\\nself-attention by independently adding relative height infor-\\nmation and relative width information. The attention logit\\n\\x14\\x13\\x19\\x18\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 15,2024 at 11:45:57 UTC from IEEE Xplore.  Restrictions apply. \\nfor how much pixel i = (ix, iy) attends to pixel j = (jx, jy)\\nis computed as:\\nli,j =\\nqT\\ni\\n\\x04\\ndh\\nk\\n(kj + rW\\njx−ix + rH\\njy−iy)\\n(3)\\nwhere qi is the query vector for pixel i (the i-th row of Q),\\nkj is the key vector for pixel j (the j-th row of K) and rW\\njx−ix\\nand rH\\njy−iy are learned embeddings for relative width jx−ix\\nand relative height jy −iy, respectively. The output of head\\nh now becomes:\\nOh = Softmax\\n⎛\\n⎝QKT + Srel\\nH + Srel\\nW\\n\\x04\\ndh\\nk\\n⎞\\n⎠V\\n(4)\\nwhere Srel\\nH , Srel\\nW ∈RHW ×HW are matrices of relative po-\\nsition logits along height and width dimensions that sat-\\nisfy Srel\\nH [i, j] = qT\\ni rH\\njy−iy and Srel\\nW [i, j] = qT\\ni rW\\njx−ix. As\\nwe consider relative height and width information sepa-\\nrately, Srel\\nH and Srel\\nW also satisfy the properties Srel\\nW [i, j] =\\nSrel\\nW [i, j + W] and Srel\\nH [i, j] = Srel\\nH [i + H, j], which pre-\\nvents from having to compute the logits for all (i, j) pairs.\\nThe relative attention algorithm in [37] explicitly\\nstores all relative embeddings rij in a tensor of shape\\n(HW, HW, dh\\nk), thus incurring an additional memory cost\\nof O((HW)2dh\\nk). This compares to O((HW)2Nh) for the\\nposition-unaware version self-attention that does not use\\nposition encodings. As we typically have Nh < dh\\nk, such an\\nimplementation can prove extremely prohibitive and restrict\\nthe number of images that can ﬁt in a minibatch. Instead, we\\nextend the memory efﬁcient relative masked attention algo-\\nrithm presented in [18] to unmasked relative self-attention\\nover 2 dimensional inputs. Our implementation has a mem-\\nory cost of O(HWdh\\nk). We leave the Tensorﬂow code of\\nthe algorithm in the Appendix.\\nThe relative positional embeeddings rH and rW are\\nlearned and shared across heads but not layers. For each\\nlayer, we add (2(H + W) −2)dh\\nk parameters to model rel-\\native distances along height and width.\\n3.2. Attention Augmented Convolution\\nMultiple previously proposed attention mechanisms over\\nimages [17, 16, 31, 46] suggest that the convolution op-\\nerator is limited by its locality and lack of understanding\\nof global contexts. These methods capture long-range de-\\npendencies by recalibrating convolutional feature maps. In\\nparticular, Squeeze-and-Excitation (SE) [17] and Gather-\\nExcite (GE) [16] perform channelwise reweighing while\\nBAM [31] and CBAM [46] reweigh both channels and\\nspatial positions independently.\\nIn contrast to these ap-\\nproaches, we 1) use an attention mechanism that can attend\\njointly to spatial and feature subspaces (each head corre-\\nsponding to a feature subspace) and 2) introduce additional\\nfeature maps rather than reﬁning them. Figure 2 summa-\\nrizes our proposed augmented convolution.\\nConcatenating convolutional and attentional feature\\nmaps:\\nFormally, consider an original convolution oper-\\nator with kernel size k, Fin input ﬁlters and Fout output\\nﬁlters. The corresponding attention augmented convolution\\ncan be written as\\nAAConv(X) = Concat\\n\\x07\\nConv(X), MHA(X)\\n\\x08\\n.\\nWe denote υ =\\ndv\\nFout the ratio of attentional channels to\\nnumber of original output ﬁlters and κ =\\ndk\\nFout the ratio of\\nkey depth to number of original output ﬁlters. Similarly to\\nthe convolution, the proposed attention augmented convo-\\nlution 1) is equivariant to translation and 2) can readily op-\\nerate on inputs of different spatial dimensions. We include\\nTensorﬂow code for the proposed attention augmented con-\\nvolution in the Appendix A.3.\\nEffect on number of parameters:\\nMultihead attention\\nintroduces a 1x1 convolution with Fin input ﬁlters and\\n(2dk+dv) = Fout(2κ+υ) output ﬁlters to compute queries,\\nkeys and values and an additional 1x1 convolution with\\ndv = Foutυ input and output ﬁlters to mix the contribu-\\ntion of different heads. Considering the decrease in ﬁlters\\nin the convolutional part, this leads to the following change\\nin parameters:\\nΔparams ∼FinFout(2κ + (1 −k2)υ + Fout\\nFin\\nυ2),\\n(5)\\nwhere we ignore the parameters introduced by relative po-\\nsition embeddings for simplicity as these are negligible. In\\npractice, this causes a slight decrease in parameters when\\nreplacing 3x3 convolutions and a slight increase in parame-\\nters when replacing 1x1 convolutions. Interestingly, we ﬁnd\\nin experiments that attention augmented networks still sig-\\nniﬁcantly outperform their fully convolutional counterparts\\nwhile using less parameters.\\nAttention Augmented Convolutional Architectures:\\nIn\\nall our experiments, the augmented convolution is followed\\nby a batch normalization [20] layer which can learn to scale\\nthe contribution of the convolution feature maps and the at-\\ntention feature maps. We apply our augmented convolution\\nonce per residual block similarly to other visual attention\\nmechanisms [17, 16, 31, 46] and along the entire architec-\\nture as memory permits (see Section 4 for more details).\\nSince the memory cost O((Nh(HW)2) can be pro-\\nhibitive for large spatial dimensions, we augment convolu-\\ntions with attention starting from the last layer (with small-\\nest spatial dimension) until we hit memory constraints. To\\n\\x14\\x13\\x19\\x19\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 15,2024 at 11:45:57 UTC from IEEE Xplore.  Restrictions apply. \\nreduce the memory footprint of augmented networks, we\\ntypically resort to a smaller batch size and sometimes addi-\\ntionally downsample the inputs to self-attention in the lay-\\ners with the largest spatial dimensions where it is applied.\\nDownsampling is performed by applying 3x3 average pool-\\ning with stride 2 while the following upsampling (required\\nfor the concatenation) is obtained via bilinear interpolation.\\n4. Experiments\\nIn the subsequent experiments, we test Attention Aug-\\nmentation on standard computer vision architectures such\\nas ResNets [14, 47, 13], and MnasNet [42] on the CIFAR-\\n100 [22], ImageNet [9] and COCO [25] datasets. Our ex-\\nperiments show that Attention Augmentation leads to sys-\\ntematic improvements on both image classiﬁcation and ob-\\nject detection tasks across a broad array of architectures and\\ncomputational demands. We validate the utility of the pro-\\nposed two-dimensional relative attention mechanism in ab-\\nlation experiments. In all experiments, we substitute con-\\nvolutional feature maps with self-attention feature maps as\\nit makes for an easier comparison against the baseline mod-\\nels. Unless speciﬁed otherwise, all results correspond to our\\ntwo-dimensional relative self-attention mechanism. Exper-\\nimental details can be found in the Appendix.\\n4.1. CIFAR-100 image classiﬁcation\\nWe ﬁrst investigate how Attention Augmentation per-\\nforms on CIFAR-100 [22], a standard benchmark for low-\\nresolution imagery, using a Wide ResNet architecture [51].\\nThe Wide-ResNet-28-10 architecture is comprised of 3\\nstages of 4 residual blocks each using two 3 × 3 convolu-\\ntions. We augment the Wide-ResNet-28-10 by augmenting\\nthe ﬁrst convolution of all residual blocks with relative at-\\ntention using Nh=8 heads and κ=2υ=0.2 and a minimum of\\n20 dimensions per head for the keys. We compare Attention\\nAugmentation (AA) against other forms of attention includ-\\ning Squeeze-and-Excitation (SE) [17] and the parameter-\\nfree formulation of Gather-Excite (GE) [16]. Table 1 shows\\nthat Attention Augmentation improves performance both\\nover the baseline network and Squeeze-and-Excitation at a\\nsimilar parameter and complexity cost.\\nArchitecture\\nParams\\nGFlops\\ntop-1\\ntop-5\\nWide-ResNet [51]\\n36.3M\\n10.4\\n80.3\\n95.0\\nGE-Wide-ResNet [16]\\n36.3M\\n10.4\\n79.8\\n95.0\\nSE-Wide-ResNet [17]\\n36.5M\\n10.4\\n81.0\\n95.3\\nAA-Wide-ResNet (ours)\\n36.2M\\n10.9\\n81.6\\n95.2\\nTable 1. Image classiﬁcation on the CIFAR-100 dataset [22] using\\nthe Wide-ResNet 28-10 architecture [51].\\n4.2. ImageNet image classiﬁcation with ResNet\\nWe next examine how Attention Augmentation performs\\non ImageNet [9, 21], a standard large-scale dataset for high\\nresolution imagery, across an array of architectures. We\\nstart with the ResNet architecture [14, 47, 13] because of its\\nwidespread use and its ability to easily scale across several\\ncomputational budgets. The building block in ResNet-34\\ncomprises two 3x3 convolutions with the same number of\\noutput ﬁlters. ResNet-50 and its larger counterparts use a\\nbottleneck block comprising of 1x1, 3x3, 1x1 convolutions\\nwhere the last pointwise convolution expands the number\\nof ﬁlters and the ﬁrst one contracts the number of ﬁlters.\\nWe modify all ResNets by augmenting the 3x3 convolu-\\ntions as this decreases number of parameters.2 We apply\\nAttention Augmentation in each residual block of the last 3\\nstages of the architecture – when the spatial dimensions of\\nthe activation maps are 28x28, 14x14 and 7x7 – and down-\\nsample only during the ﬁrst stage. All attention augmented\\nnetworks use κ=2υ=0.2, except for ResNet-34 which uses\\nκ=υ=0.25. The number of attention heads is ﬁxed to Nh=8.\\nArchitecture\\nParams (M)\\nΔInfer\\nΔT rain\\ntop-1\\nResNet-50\\n25.6\\n-\\n-\\n76.4\\nSE [17]\\n28.1\\n+12%\\n+92%\\n77.5 (77.0)\\nBAM [31]\\n25.9\\n+19%\\n+43%\\n77.3\\nCBAM [46]\\n28.1\\n+56%\\n+132%\\n77.4 (77.4)\\nGALA [28]\\n29.4\\n+86%\\n+133%\\n77.5 (77.3)\\nAA (υ = 0.25)\\n24.3\\n+29%\\n+25%\\n77.7\\nTable 2. Image classiﬁcation performance of different attention\\nmechanisms on the ImageNet dataset. Δ refers to the increase\\nin latency times compared to the ResNet50 on a single Tesla V100\\nGPU with Tensorﬂow using a batch size of 128. For fair compar-\\nison, we also include top-1 results (in parentheses) when scaling\\nnetworks in width to match ∼25.6M parameters as the ResNet50\\nbaseline.\\nTable 2 benchmarks Attention Augmentation against\\nchannel and spatial attention mechanisms BAM [31],\\nCBAM [46] and GALA [28] with channel reduction ra-\\ntio σ = 16 on the ResNet50 architecture.\\nDespite the\\nlack of specialized kernels (See Appendix A.3), Attention\\nAugmentation offers a competitive accuracy/computational\\ntrade-off compared to previously proposed attention mech-\\nanisms. Table 3 compares the non-augmented networks and\\nSqueeze-and-Excitation (SE) [17] across different network\\nscales.\\nIn all experiments, Attention Augmentation sig-\\nniﬁcantly increases performance over the non-augmented\\nbaseline and notably outperforms Squeeze-and-Excitation\\n(SE) [17] while being more parameter efﬁcient (Figure 1).\\nRemarkably, our AA-ResNet-50 performs comparably to\\nthe baseline ResNet-101 and our AA-ResNet-101 outper-\\nforms the baseline ResNet-152. These results suggest that\\n2We found that augmenting the pointwise expansions works just as well\\nbut does not save parameters or computations.\\n\\x14\\x13\\x19\\x1a\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 15,2024 at 11:45:57 UTC from IEEE Xplore.  Restrictions apply. \\nArchitecture\\nGFlops\\nParams\\ntop-1\\ntop-5\\nResNet-34 [14]\\n7.4\\n21.8M\\n73.6\\n91.5\\nSE-ResNet-34 [17]\\n7.4\\n22.0M\\n74.3\\n91.8\\nAA-ResNet-34 (ours)\\n7.1\\n20.7M\\n74.7\\n92.0\\nResNet-50 [14]\\n8.2\\n25.6M\\n76.4\\n93.1\\nSE-ResNet-50 [17]\\n8.2\\n28.1M\\n77.5\\n93.7\\nAA-ResNet-50 (ours)\\n8.3\\n25.8M\\n77.7\\n93.8\\nResNet-101 [14]\\n15.6\\n44.5M\\n77.9\\n94.0\\nSE-ResNet-101 [17]\\n15.6\\n49.3M\\n78.4\\n94.2\\nAA-ResNet-101 (ours)\\n16.1\\n45.4M\\n78.7\\n94.4\\nResNet-152 [14]\\n23.0\\n60.2M\\n78.4\\n94.2\\nSE-ResNet-152 [17]\\n23.1\\n66.8M\\n78.9\\n94.5\\nAA-ResNet-152 (ours)\\n23.8\\n61.6M\\n79.1\\n94.6\\nTable 3. Image classiﬁcation on the ImageNet dataset [9] across\\na range of ResNet architectures: ResNet-34, ResNet-50, Resnet-\\n101, and ResNet-152 [14, 47, 13].\\nArchitecture\\nGFlops\\nParams\\ntop-1\\ntop-5\\nMnasNet-0.75\\n0.45\\n2.91M\\n73.3\\n91.3\\nAA-MnasNet-0.75\\n0.51\\n3.02M\\n73.9\\n91.6\\nMnasNet-1.0\\n0.63\\n3.89M\\n75.2\\n92.4\\nAA-MnasNet-1.0\\n0.70\\n4.06M\\n75.7\\n92.6\\nMnasNet-1.25\\n1.01\\n5.26M\\n76.7\\n93.2\\nAA-MnasNet-1.25\\n1.11\\n5.53M\\n77.2\\n93.6\\nMnasNet-1.4\\n1.17\\n6.10M\\n77.2\\n93.5\\nAA-MnasNet-1.4\\n1.29\\n6.44M\\n77.7\\n93.8\\nTable 4. Baseline and attention augmented MnasNet [42] accura-\\ncies with width multipliers 0.75, 1.0, 1.25 and 1.4.\\nattention augmentation is preferable to simply making net-\\nworks deeper. We include and discuss attention maps visu-\\nalizations from different pixel positions in the appendix.\\n4.3. ImageNet classiﬁcation with MnasNet\\nIn this section, we inspect the use of Attention Aug-\\nmentation in a resource constrained setting by conducting\\nImageNet experiments with the MnasNet architecture [42],\\nwhich is an extremely parameter-efﬁcient architecture. In\\nparticular, the MnasNet was found by neural architec-\\nture search [54], using only the highly optimized mo-\\nbile inverted bottleneck block [36] and the Squeeze-and-\\nExcitation operation [17] as the primitives in its search\\nspace.\\nWe apply Attention Augmentation to the mobile\\ninverted bottleneck by replacing convolutional channels in\\nthe expansion pointwise convolution using κ=2υ=0.1 and\\nNh=4 heads. Our augmented MnasNets use augmented in-\\nverted bottlenecks in the the last 13 blocks out of 18 in the\\nMnasNet architecture, starting when the spatial dimension\\nis 28x28. We downsample only in the ﬁrst stage where At-\\ntention Augmentation is applied. We leave the ﬁnal point-\\nwise convolution, also referred to as the “head”, unchanged.\\nIn Table 4, we report ImageNet accuracies for the base-\\nline MnasNet and its attention augmented variants at dif-\\nFigure 3. ImageNet top-1 accuracy as a function of number of pa-\\nrameters for MnasNet (black) and Attention-Augmented-MnasNet\\n(red) with depth multipliers 0.75, 1.0, 1.25 and 1.4.\\nferent width multipliers. Our experiments show that At-\\ntention Augmentation yields accuracy improvements across\\nall width multipliers. Augmenting MnasNets with relative\\nself-attention incurs a slight parameter increase, however\\nwe verify in Figure 3 that the accuracy improvements are\\nnot just explained by the parameter increase. Additionally,\\nwe note that the MnasNet architecture employs Squeeze-\\nand-Excitation at multiple locations that were optimally se-\\nlected via architecture search, further suggesting the bene-\\nﬁts of our method.\\n4.4. Object Detection with COCO dataset\\nWe next investigate the use of Attention Augmentation\\non the task of object detection on the COCO dataset [27].\\nWe employ the RetinaNet architecture with a ResNet-50\\nand ResNet-101 backbone as done in [26], using the open-\\nsourced RetinaNet codebase.3\\nWe apply Attention Aug-\\nmentation uniquely on the ResNet backbone, modifying\\nthem similarly as in our ImageNet classiﬁcation experi-\\nments.\\nOur relative self-attention mechanism improves the per-\\nformance of the RetinaNet on both ResNet-50 and ResNet-\\n101 as shown in Table 5. Most notably, Attention Aug-\\nmentation yields a 1.4% mAP improvement over a strong\\nRetinaNet baseline from [26]. In contrast to the success\\nof Squeeze-and-Excitation in image classiﬁcation with Im-\\nageNet, our experiments show that adding Squeeze-and-\\nExcitation operators in the backbone network of the Reti-\\nnaNet signiﬁcantly hurts performance, in spite of grid\\nsearching over the squeeze ratio σ ∈{4, 8, 16}. We hy-\\npothesize that localization requires precise spatial informa-\\n3https://github.com/tensorflow/tpu/tree/master/\\nmodels/official/retinanet\\n\\x14\\x13\\x1a\\x11\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 15,2024 at 11:45:57 UTC from IEEE Xplore.  Restrictions apply. \\nBackbone architecture\\nGFlops\\nParams\\nmAPCOCO\\nmAP50\\nmAP75\\nResNet-50 [26]\\n182\\n33.4M\\n36.8\\n54.5\\n39.5\\nSE-ResNet-50 [17]\\n183\\n35.9M\\n36.5\\n54.0\\n39.1\\nAA-ResNet-50 (ours)\\n182\\n33.1M\\n38.2\\n56.5\\n40.7\\nResNet-101 [26]\\n243\\n52.4M\\n38.5\\n56.4\\n41.2\\nSE-ResNet-101 [17]\\n243\\n57.2M\\n37.4\\n55.0\\n39.9\\nAA-ResNet-101 (ours)\\n245\\n51.7M\\n39.2\\n57.8\\n41.9\\nTable 5. Object detection on the COCO dataset [27] using the RetinaNet architecture [26] with different backbone architectures. We report\\nmean Average Precision at three different IoU values.\\ntion which SE discards during the spatial pooling operation,\\nthereby negatively affecting performance. Self-attention on\\nthe other hand maintains spatial information and is likely to\\nbe able to identify object boundaries successfully. Visual-\\nizations of attention maps (See Figures 9 and 10 in the Ap-\\npendix) reveal that some heads are indeed delineating ob-\\njects from their background which might be important for\\nlocalization.\\n4.5. Ablation Study\\nFully-attentional vision models:\\nIn this section, we in-\\nvestigate the performance of Attention Augmentation as a\\nfunction of the fraction of attentional channels. As we in-\\ncrease this fraction to 100%, we begin to replace a Con-\\nvNet with a fully attentional model, only leaving pointwise\\nconvolutions and the stem unchanged. Table 6 presents the\\nperformance of Attention Augmentation on the ResNet-50\\narchitecture for varying ratios κ=υ ∈{0.25, 0.5, 0.75, 1.0}.\\nPerformance slightly degrades as the ratio of attentional\\nchannels increases, which we hypothesize is partly ex-\\nplained by the average pooling operation for downsampling\\nat the ﬁrst stage where Attention Augmentation is applied.\\nAttention Augmentation proves however quite robust to the\\nfraction of attentional channels. For instance, AA-ResNet-\\n50 with κ=υ=0.75 outperforms its ResNet-50 counterpart,\\nwhile being more parameter and ﬂops efﬁcient, indicating\\nthat mostly employing attentional channels is readily com-\\npetitive.\\nPerhaps surprisingly, these experiments also reveal that\\nour proposed self-attention mechanism is a powerful stand-\\nalone computational primitive for image classiﬁcation and\\nthat fully attentional models are viable for discriminative vi-\\nsual tasks. In particular, AA-ResNet-50 with κ=υ=1, which\\nuses exclusively attentional channels, is only 2.5% worse\\nin accuracy than its fully convolutional counterpart, in spite\\nof downsampling with average pooling and having 25% less\\nparameters. Notably, this fully attentional architecture4 also\\noutperforms ResNet-34 while being more parameter and\\n4We consider pointwise convolutions as dense layers. This architecture\\nemploys 4 non-pointwise convolutions in the stem and the ﬁrst stage of the\\narchitecture, but we believe such operations can be replaced by attention\\ntoo.\\nFigure 4. Effect of relative position embeddings as the ratio\\nof attentional channels increases on our Attention-Augmented\\nResNet50.\\nﬂops efﬁcient (see Table 6).\\nArchitecture\\nGFlops\\nParams\\ntop-1\\ntop-5\\nResNet-34 [14]\\n7.4\\n21.8M\\n73.6\\n91.5\\nResNet-50 [14]\\n8.2\\n25.6M\\n76.4\\n93.1\\nκ = υ = 0.25\\n7.9\\n24.3M\\n77.7\\n93.8\\nκ = υ = 0.5\\n7.3\\n22.3M\\n77.3\\n93.6\\nκ = υ = 0.75\\n6.8\\n20.7M\\n76.7\\n93.2\\nκ = υ = 1.0\\n6.3\\n19.4M\\n73.9\\n91.5\\nTable 6. Attention Augmented ResNet-50 with varying ratios of\\nattentional channels.\\nImportance of position encodings:\\nIn Figure 4, we show\\nthe effect of our proposed two-dimensional relative posi-\\ntion encodings as a function of the fraction of attentional\\nchannels. As expected, experiments demonstrate that our\\nrelative position encodings become increasingly more im-\\nportant as the architecture employs more attentional chan-\\nnels. In particular, the fully self-attentional ResNet-50 gains\\n2.8% top-1 ImageNet accuracy when using relative position\\n\\x14\\x13\\x1a\\x12\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 15,2024 at 11:45:57 UTC from IEEE Xplore.  Restrictions apply. \\nArchitecture\\nPosition Encodings\\ntop-1\\ntop-5\\nAA-ResNet-34\\nNone\\n74.4\\n91.9\\nAA-ResNet-34\\n2d Sine\\n74.4\\n92.0\\nAA-ResNet-34\\nCoordConv\\n74.4\\n92.0\\nAA-ResNet-34\\nRelative (ours)\\n74.7\\n92.0\\nAA-ResNet-50\\nNone\\n77.5\\n93.7\\nAA-ResNet-50\\n2d Sine\\n77.5\\n93.7\\nAA-ResNet-50\\nCoordConv\\n77.5\\n93.8\\nAA-ResNet-50\\nRelative (ours)\\n77.7\\n93.8\\nTable 7. Effects of different position encodings in Attention Aug-\\nmentation on ImageNet classiﬁcation.\\nPosition Encodings\\nmAPCOCO\\nmAP50\\nmAP75\\nNone\\n37.7\\n56.0\\n40.2\\nCoordConv [29]\\n37.4\\n55.5\\n40.1\\nRelative (ours)\\n38.2\\n56.5\\n40.7\\nTable 8. Effects of different position encodings in Attention Aug-\\nmentation on the COCO object detection task using a RetinaNet\\nAA-ResNet-50 backbone.\\nencodings, which indicates the necessity of maintaining po-\\nsition information for fully self-attentional vision models.\\nWe additionally compare our proposed two-dimensional\\nrelative position encodings to other position encoding\\nschemes. We apply Attention Augmentation using the same\\nhyperparameters as 4.2 with the following different posi-\\ntion encoding schemes: 1) The position-unaware version of\\nself-attention (referred to as None), 2) a two-dimensional\\nimplementation of the sinusoidal positional waves (referred\\nto as 2d Sine) as used in [32], 3) CoordConv [29] for which\\nwe concatenate (x,y,r) coordinate channels to the inputs of\\nthe attention function, and 4) our proposed two-dimensional\\nrelative position encodings (referred to as Relative).\\nIn Table 7 and 8, we present the results on ImageNet\\nclassiﬁcation and the COCO object detection task respec-\\ntively. On both tasks, Attention Augmentation without po-\\nsition encodings already yields improvements over the fully\\nconvolutional non-augmented variants.\\nOur experiments\\nalso reveal that the sinusoidal encodings and the coordinate\\nconvolution do not provide improvements over the position-\\nunaware version of Attention Augmentation. We obtain ad-\\nditional improvements when using our two-dimensional rel-\\native attention, demonstrating the utility of preserving trans-\\nlation equivariance while preventing permutation equivari-\\nance.\\n5. Discussion and future work\\nIn this work, we consider the use of self-attention for vi-\\nsion models as an alternative to convolutions. We introduce\\na novel two-dimensional relative self-attention mechanism\\nfor images that enables training of competitive fully self-\\nattentional vision models on image classiﬁcation for the ﬁrst\\ntime. We propose to augment convolutional operators with\\nthis self-attention mechanism and validate the superiority of\\nthis approach over other attention schemes. Extensive ex-\\nperiments show that Attention Augmentation leads to sys-\\ntematic improvements on both image classiﬁcation and ob-\\nject detection tasks across a wide range of architectures and\\ncomputational settings.\\nSeveral open questions from this work remain. In fu-\\nture work, we will focus on the fully attentional regime\\nand explore how different attention mechanisms trade off\\ncomputational efﬁciency versus representational power. For\\ninstance, identifying a local attention mechanism may re-\\nsult in an efﬁcient and scalable computational mechanism\\nthat could prevent the need for downsampling with average\\npooling [34]. Additionally, it is plausible that architectural\\ndesign choices that are well suited when exclusively relying\\non convolutions are suboptimal when using self-attention\\nmechanisms. As such, it would be interesting to see if us-\\ning Attention Augmentation as a primitive in automated ar-\\nchitecture search procedures proves useful to ﬁnd even bet-\\nter models than those previously found in image classiﬁca-\\ntion [55], object detection [12], image segmentation [6] and\\nother domains[5, 1, 35, 8]. Finally, one can ask to which\\ndegree fully attentional models can replace convolutional\\nnetworks for visual tasks.\\nAcknowledgements\\nThe authors would like to thank Tsung-Yi Lin, Pra-\\njit Ramachandran, Mingxing Tan, Yanping Huang and the\\nGoogle Brain team for insightful comments and discus-\\nsions.\\nReferences\\n[1] Maximilian Alber, Irwan Bello, Barret Zoph, Pieter-Jan Kin-\\ndermans, Prajit Ramachandran, and Quoc V. Le. Backprop\\nevolution. CoRR, abs/1808.02822, 2018. 8\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\\nNeural machine translation by jointly learning to align and\\ntranslate. In International Conference on Learning Repre-\\nsentations, 2015. 2\\n[3] Irwan Bello, Sayali Kulkarni, Sagar Jain, Craig Boutilier,\\nEd Huai-hsin Chi, Elad Eban, Xiyang Luo, Alan Mackey,\\nand Ofer Meshi. Seq2slate: Re-ranking and slate optimiza-\\ntion with rnns. CoRR, abs/1810.02019, 2018. 2\\n[4] Irwan Bello, Hieu Pham, Quoc V. Le, Mohammad Norouzi,\\nand Samy Bengio. Neural combinatorial optimization with\\nreinforcement learning. 2016. 2\\n[5] Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V. Le.\\nNeural optimizer search with reinforcement learning. In Pro-\\nceedings of the 34th International Conference on Machine\\nLearning - Volume 70, ICML’17, pages 459–468. JMLR.org,\\n2017. 8\\n\\x14\\x13\\x1a\\x13\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 15,2024 at 11:45:57 UTC from IEEE Xplore.  Restrictions apply. \\n[6] Liang-Chieh Chen, Maxwell Collins, Yukun Zhu, George\\nPapandreou, Barret Zoph, Florian Schroff, Hartwig Adam,\\nand Jon Shlens.\\nSearching for efﬁcient multi-scale archi-\\ntectures for dense image prediction. In Advances in Neural\\nInformation Processing Systems, pages 8713–8724, 2018. 2,\\n8\\n[7] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\\nYan, and Jiashi Feng. A2-nets: Double attention networks.\\nCoRR, abs/1810.11579, 2018. 3\\n[8] Ekin Dogus Cubuk, Barret Zoph, Dandelion Man´e, Vijay Va-\\nsudevan, and Quoc V. Le. Autoaugment: Learning augmen-\\ntation policies from data. CoRR, abs/1805.09501, 2018. 8\\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\\ndatabase. In IEEE Conference on Computer Vision and Pat-\\ntern Recognition. IEEE, 2009. 1, 2, 5, 6\\n[10] Xavier Gastaldi. Shake-shake regularization. arXiv preprint\\narXiv:1705.07485, 2017. 11\\n[11] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Dropblock:\\nA regularization method for convolutional networks.\\nIn\\nAdvances in Neural Information Processing Systems, pages\\n10750–10760, 2018. 11\\n[12] Golnaz Ghiasi, Tsung-Yi Lin, Ruoming Pang, and Quoc V\\nLe. NAS-FPN: Learning scalable feature pyramid architec-\\nture for object detection. In The IEEE Conference on Com-\\nputer Vision and Pattern Recognition (CVPR), June 2019. 8\\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In IEEE Con-\\nference on Computer Vision and Pattern Recognition, 2016.\\n2, 5, 6\\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nIdentity mappings in deep residual networks. In European\\nConference on Computer Vision, 2016. 1, 2, 5, 6, 7, 11\\n[15] Sepp Hochreiter and Juergen Schmidhuber. Long short-term\\nmemory. Neural Computation, 1997. 2\\n[16] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea\\nVedaldi. Gather-excite: Exploiting feature context in convo-\\nlutional neural networks. In Advances in Neural Information\\nProcessing Systems, pages 9423–9433, 2018. 3, 4, 5\\n[17] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-\\nworks. In Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, 2018. 1, 2, 3, 4, 5, 6, 7\\n[18] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszko-\\nreit, Noam Shazeer, Curtis Hawthorne, Andrew M Dai,\\nMatthew D Hoffman, and Douglas Eck. Music transformer.\\nIn Advances in Neural Processing Systems, 2018. 3, 4\\n[19] Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu,\\nAnoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wo-\\njna, Yang Song, Sergio Guadarrama, et al.\\nSpeed/accu-\\nracy trade-offs for modern convolutional object detectors. In\\nIEEE Conference on Computer Vision and Pattern Recogni-\\ntion, 2017. 2\\n[20] Sergey Ioffe and Christian Szegedy. Batch normalization:\\nAccelerating deep network training by reducing internal co-\\nvariate shift. In International Conference on Learning Rep-\\nresentations, 2015. 4\\n[21] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do bet-\\nter imagenet models transfer better? In Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recogni-\\ntion, 2019. 2, 5\\n[22] Alex Krizhevsky. Learning multiple layers of features from\\ntiny images. Technical report, University of Toronto, 2009.\\n2, 5\\n[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.\\nImagenet classiﬁcation with deep convolutional neural net-\\nworks. In Advances in Neural Information Processing Sys-\\ntem, 2012. 1, 2\\n[24] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick\\nHaffner. Gradient-based learning applied to document recog-\\nnition. Proceedings of the IEEE, 1998. 1\\n[25] Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick, Kaiming He,\\nBharath Hariharan, and Serge Belongie.\\nFeature pyramid\\nnetworks for object detection. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition,\\n2017. 5\\n[26] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\\nPiotr Doll´ar. Focal loss for dense object detection. In Pro-\\nceedings of the IEEE international conference on computer\\nvision, pages 2980–2988, 2017. 6, 7, 11\\n[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\\nZitnick. Microsoft coco: Common objects in context. In\\nEuropean Conference on Computer Vision, pages 740–755.\\nSpringer, 2014. 2, 6, 7\\n[28] Drew Linsley, Dan Scheibler, Sven Eberhardt, and Thomas\\nSerre. Global-and-local attention networks for visual recog-\\nnition. CoRR, abs/1805.08819, 2018. 5\\n[29] Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski\\nSuch, Eric Frank, Alex Sergeev, and Jason Yosinski.\\nAn\\nintriguing failing of convolutional neural networks and the\\ncoordconv solution. In Advances in Neural Information Pro-\\ncessing Systems, pages 9628–9639, 2018. 3, 8\\n[30] Ilya Loshchilov and Frank Hutter.\\nSGDR: Stochas-\\ntic gradient descent with warm restarts.\\narXiv preprint\\narXiv:1608.03983, 2016. 11\\n[31] Jongchan Park, Sanghyun Woo, Joon-Young Lee, and In So\\nKweon. Bam: bottleneck attention module. In British Ma-\\nchine Vision Conference, 2018. 3, 4, 5\\n[32] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz\\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\\nage transformer. In International Conference on Machine\\nLearning, 2018. 3, 8\\n[33] Andrew Rabinovich, Andrea Vedaldi, Carolina Galleguillos,\\nEric Wiewiora, and Serge Belongie.\\nObjects in context.\\n2007. 1\\n[34] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-alone\\nself-attention in vision models.\\nCoRR, abs/1906.05909,\\n2019. 8\\n[35] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Search-\\ning for activation functions. CoRR, abs/1710.05941, 2017.\\n8\\n\\x14\\x13\\x1a\\x14\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 15,2024 at 11:45:57 UTC from IEEE Xplore.  Restrictions apply. \\n[36] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\\nmoginov, and Liang-Chieh Chen.\\nMobilenetv2: Inverted\\nresiduals and linear bottlenecks. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition,\\npages 4510–4520, 2018. 6\\n[37] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\\nSelf-\\nattention with relative position representations.\\narXiv\\npreprint arXiv:1803.02155, 2018. 2, 3, 4\\n[38] David R. So, Chen Liang, and Quoc V. Le.\\nThe evolved\\ntransformer. CoRR, abs/1901.11117, 2019. 3\\n[39] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and\\nAlex Alemi. Inception-v4, Inception-Resnet and the impact\\nof residual connections on learning. In International Con-\\nference on Learning Representations Workshop Track, 2016.\\n2\\n[40] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich.\\nGoing deeper with\\nconvolutions. In IEEE Conference on Computer Vision and\\nPattern Recognition, 2015. 2\\n[41] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\\nShlens, and Zbigniew Wojna. Rethinking the Inception ar-\\nchitecture for computer vision. In IEEE Conference on Com-\\nputer Vision and Pattern Recognition, 2016. 2, 11\\n[42] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan,\\nand Quoc V Le. Mnasnet: Platform-aware neural architec-\\nture search for mobile. In Proceedings of the IEEE Confer-\\nence on Computer Vision and Pattern Recognition, 2018. 2,\\n5, 6, 11\\n[43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In Advances in Neural\\nInformation Processing Systems, pages 5998–6008, 2017. 1,\\n2, 3\\n[44] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer\\nnetworks. In NIPS, pages 2692–2700, 2015. 2\\n[45] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\\ning He. Non-local neural networks. In Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recogni-\\ntion, pages 7794–7803, 2018. 3\\n[46] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In\\nSo Kweon. Cbam: Convolutional block attention module.\\nIn Proceedings of the European Conference on Computer Vi-\\nsion (ECCV), pages 3–19, 2018. 3, 4, 5\\n[47] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and\\nKaiming He. Aggregated residual transformations for deep\\nneural networks. In Proceedings of the IEEE Conference on\\nComputer Vision and Pattern Recognition, 2017. 2, 5, 6\\n[48] Yoshihiro Yamada, Masakazu Iwamura, Takuya Akiba, and\\nKoichi Kise.\\nShakedrop regularization for deep residual\\nlearning. arXiv preprint arXiv:1802.02375, 2018. 11\\n[49] Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S.\\nChao, and Zhaopeng Tu. Convolutional self-attention net-\\nwork. In CoRR, volume abs/1810.13320, 2018. 2\\n[50] Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui\\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V Le.\\nQAnet:\\nCombining local convolution with global self-\\nattention for reading comprehension. In International Con-\\nference on Learning Representations, 2018. 2\\n[51] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-\\nworks. In British Machine Vision Conference, 2016. 5\\n[52] Vinicius Zambaldi, David Raposo, Adam Santoro, Vic-\\ntor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David\\nReichert, Timothy Lillicrap, Edward Lockhart, Murray\\nShanahan, Victoria Langston, Razvan Pascanu, Matthew\\nBotvinick, Oriol Vinyals, and Peter Battaglia. Deep rein-\\nforcement learning with relational inductive biases. In ICLR,\\n2019. 2\\n[53] Han Zhang, Ian J. Goodfellow, Dimitris N. Metaxas, and\\nAugustus Odena. Self-attention generative adversarial net-\\nworks. arXiv:1805.08318, 2018. 3\\n[54] Barret Zoph and Quoc V. Le.\\nNeural architecture search\\nwith reinforcement learning. In International Conference on\\nLearning Representations, 2017. 6\\n[55] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V\\nLe. Learning transferable architectures for scalable image\\nrecognition.\\nIn Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pages 8697–8710,\\n2018. 2, 8, 11\\n\\x14\\x13\\x1a\\x15\\nAuthorized licensed use limited to: University of Michigan Library. Downloaded on November 15,2024 at 11:45:57 UTC from IEEE Xplore.  Restrictions apply. \\n\"}\n"
     ]
    }
   ],
   "source": [
    "with open('all_papers.jsonl', 'r') as file:\n",
    "    idx=0\n",
    "    for line_num, line in enumerate(file):\n",
    "        idx+=1\n",
    "        if idx==95348:\n",
    "            document = json.loads(line.strip())\n",
    "            print(document)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eac98f-3144-4527-8d27-50454178d799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c27bd-6e26-48d0-9135-83c6d578797e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cf579436-b01b-4561-ba15-c57f72a260ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jakub jakubowski',\n",
       " 'natalia wojak-strzelecka',\n",
       " 'rita p. ribeiro',\n",
       " 'sepideh pashami',\n",
       " 'szymon bobek',\n",
       " 'joao gama',\n",
       " 'grzegorz j nalepa']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docid_authors_map[3488]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2774e0-9ee6-4597-acab-1fb4e9f7cdc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de49c764-6107-429f-866d-c627b9141559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f46bc-17a7-4173-af01-747afd762942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e1137-4e65-4ae9-b1d2-6b6a6f7014a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc06a30-aa68-4264-9975-6a7f1c9c733c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c647010a-7fe0-43c2-87c3-9c8dac97d4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55a49026-32ea-4dae-92b6-cdda8a9e83e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf3 = []\n",
    "for i in tfidf2:\n",
    "    tfidf3.append(random.sample(i,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf09c84e-c173-4338-b97a-07576f99ae5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(10376, 26.83205945838511),\n",
       "  (5432, 26.89778253202587),\n",
       "  (4731, 34.193236102163546),\n",
       "  (944, 27.29377962131445)],\n",
       " [(8069, 31.88462768514792),\n",
       "  (1273, 32.53567861804069),\n",
       "  (3251, 33.57193979354456),\n",
       "  (2397, 35.80564288002584)],\n",
       " [(19520, 24.989189315812425),\n",
       "  (10931, 25.31929817140096),\n",
       "  (1169, 25.816239398625164),\n",
       "  (4731, 31.601511777583532)],\n",
       " [(4113, 37.27603672397212),\n",
       "  (6678, 35.98090571126465),\n",
       "  (13037, 35.82425533722886),\n",
       "  (498, 38.84393183001212)],\n",
       " [(9092, 35.533234433401105),\n",
       "  (8856, 29.85808104975241),\n",
       "  (11274, 28.058224909340545),\n",
       "  (9097, 27.187835555588723)],\n",
       " [(8456, 29.51662512490252),\n",
       "  (254, 30.432991995082148),\n",
       "  (19384, 28.032995653749612),\n",
       "  (4731, 35.10656886778013)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df166300-8aa6-466a-8cb5-f8635ff8eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm252 = []\n",
    "for i in bm25:\n",
    "    bm252.append(i[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fead01a-261c-4e13-827d-77478ed58b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(53070, 2.8572678768896806),\n",
       "  (56381, 2.8542177337876247),\n",
       "  (46577, 2.789671061684337),\n",
       "  (16648, 2.7443922167334835),\n",
       "  (55048, 2.701059318890985),\n",
       "  (79684, 2.6399390850343525),\n",
       "  (51504, 2.5071875550303138),\n",
       "  (34225, 2.4408273417087374),\n",
       "  (59105, 2.2319126678373156),\n",
       "  (72920, 2.173646081608272),\n",
       "  (59201, 2.1402022170473263),\n",
       "  (36754, 2.1006932108492298),\n",
       "  (63555, 1.9686947574101397),\n",
       "  (1776, 1.9454869853868262),\n",
       "  (17207, 1.9318846937412926),\n",
       "  (58233, 1.8764594209654493),\n",
       "  (58138, 1.8732431746904017),\n",
       "  (57956, 1.8580589735690576),\n",
       "  (53921, 1.8514766321851321),\n",
       "  (47714, 1.8452128838514061),\n",
       "  (31024, 1.8300648362425684),\n",
       "  (51954, 1.829327177386196),\n",
       "  (38752, 1.824354553726153),\n",
       "  (26734, 1.8188403919665004),\n",
       "  (51392, 1.8089042713453614),\n",
       "  (17698, 1.8071502883674107),\n",
       "  (37940, 1.8061723005580987),\n",
       "  (40431, 1.8050222309376398),\n",
       "  (59042, 1.7941152921152197),\n",
       "  (51225, 1.7920467010253294)],\n",
       " [(51339, 2.184399018145183),\n",
       "  (50700, 2.0762100863201947),\n",
       "  (52494, 2.0317630531751543),\n",
       "  (50903, 2.029459388936118),\n",
       "  (54412, 2.011513683594305),\n",
       "  (42421, 1.9974424074269903),\n",
       "  (51364, 1.9958814502384081),\n",
       "  (50484, 1.9842202905702777),\n",
       "  (50766, 1.9803806570065068),\n",
       "  (32349, 1.9757448804435966),\n",
       "  (50746, 1.9722433829580917),\n",
       "  (51190, 1.9710998516747704),\n",
       "  (50886, 1.9628439882546322),\n",
       "  (52183, 1.9556566861625524),\n",
       "  (50861, 1.9469074917505163),\n",
       "  (34709, 1.927762418950877),\n",
       "  (51002, 1.9272407890234375),\n",
       "  (50698, 1.9256334323613813),\n",
       "  (36569, 1.9248413530793265),\n",
       "  (50994, 1.9239799307016068),\n",
       "  (46455, 1.922449986302416),\n",
       "  (51067, 1.9195046808960372),\n",
       "  (50997, 1.9159174687147043),\n",
       "  (50793, 1.9154331336446084),\n",
       "  (2992, 1.9130277178167303),\n",
       "  (52323, 1.9111970247745296),\n",
       "  (50831, 1.9096605075340576),\n",
       "  (51046, 1.903005093444123),\n",
       "  (38109, 1.9003268245570832),\n",
       "  (39002, 1.9000866469555993)],\n",
       " [(44655, 4.126170561747636),\n",
       "  (46234, 4.0048527932432885),\n",
       "  (84165, 3.955795912024671),\n",
       "  (77119, 3.923603579956447),\n",
       "  (66121, 3.9174370964415144),\n",
       "  (22261, 3.8686330297480356),\n",
       "  (50279, 3.857014270180484),\n",
       "  (73698, 3.8526610446560676),\n",
       "  (23776, 3.8495297878795793),\n",
       "  (11481, 3.7517565565673587),\n",
       "  (75397, 3.6976286512126113),\n",
       "  (65703, 3.6533833496744212),\n",
       "  (60763, 3.6532277063729466),\n",
       "  (65720, 3.639705333409597),\n",
       "  (90105, 3.6186420359825426),\n",
       "  (62926, 3.6156577354663897),\n",
       "  (23556, 3.6121031822558627),\n",
       "  (7313, 3.6075034996950106),\n",
       "  (97143, 3.585020203069858),\n",
       "  (73869, 3.570405173956115),\n",
       "  (75680, 3.560653606134441),\n",
       "  (88177, 3.560420473702796),\n",
       "  (55130, 3.5554402055033703),\n",
       "  (83467, 3.5524730597273666),\n",
       "  (73443, 3.532949853964643),\n",
       "  (83204, 3.5165755621297823),\n",
       "  (67422, 3.4912831761529315),\n",
       "  (21130, 3.486494839540188),\n",
       "  (5460, 3.464166906150631),\n",
       "  (13446, 3.4629880195690204)],\n",
       " [(92208, 101.50842607659658),\n",
       "  (86883, 100.92072207239953),\n",
       "  (94827, 100.71379619814684),\n",
       "  (90918, 100.70237153246435),\n",
       "  (91329, 100.70063571402913),\n",
       "  (14050, 100.70030851208756),\n",
       "  (56951, 100.68709535404821),\n",
       "  (91077, 100.66382082140063),\n",
       "  (94676, 100.65650832551596),\n",
       "  (9558, 100.62515008806365),\n",
       "  (7879, 100.57208297293467),\n",
       "  (75732, 100.55277382352158),\n",
       "  (77104, 100.55171728000768),\n",
       "  (75554, 100.54440900792451),\n",
       "  (8332, 100.53022236277064),\n",
       "  (92017, 100.5228347665458),\n",
       "  (40356, 100.51660562735415),\n",
       "  (94288, 100.50760903607026),\n",
       "  (91652, 100.50760226114588),\n",
       "  (91000, 100.49769990775049),\n",
       "  (73478, 100.49207485737044),\n",
       "  (7100, 100.48031803473728),\n",
       "  (91059, 100.47818794418657),\n",
       "  (98254, 100.47780530303488),\n",
       "  (91870, 100.47064191340804),\n",
       "  (95497, 100.46813711212152),\n",
       "  (75825, 100.46582128171623),\n",
       "  (7510, 100.43156123642714),\n",
       "  (91651, 100.42397794808102),\n",
       "  (90975, 100.41597746336429)],\n",
       " [(29139, 99.63652632693574),\n",
       "  (97021, 3.2030629844977403),\n",
       "  (3364, 2.4893964276375833),\n",
       "  (37128, 2.462487566945877),\n",
       "  (40230, 2.3455522454598787),\n",
       "  (26756, 2.3124025552538545),\n",
       "  (48906, 2.3092435189914697),\n",
       "  (46949, 2.12954114042396),\n",
       "  (48114, 2.1084231263724016),\n",
       "  (77312, 2.0902402878063424),\n",
       "  (40810, 2.084676491368673),\n",
       "  (8006, 2.071030633446736),\n",
       "  (42365, 2.0502605773253477),\n",
       "  (32973, 2.0040707831242868),\n",
       "  (31756, 1.8568426442611319),\n",
       "  (60590, 1.850473426145379),\n",
       "  (78359, 1.822242729515771),\n",
       "  (32855, 1.7966933097081474),\n",
       "  (27362, 1.7845978452262325),\n",
       "  (95116, 1.636999644237773),\n",
       "  (26907, 1.6253993899121262),\n",
       "  (41238, 1.542976152156066),\n",
       "  (54073, 1.514619607031489),\n",
       "  (48144, 1.2494632164543522),\n",
       "  (40407, 1.1552818069200914),\n",
       "  (29369, 0.9960266869158404),\n",
       "  (27959, 0.8760331280165002),\n",
       "  (91013, 0.7732216807985707),\n",
       "  (64801, 0.7701666010881084),\n",
       "  (93730, 0.7652939570547236)],\n",
       " [(33036, 2.049463508671854),\n",
       "  (46798, 1.6488037970534994),\n",
       "  (61442, 1.2800595375258481),\n",
       "  (12653, 1.180675290476835),\n",
       "  (6721, 1.1420741965553236),\n",
       "  (41114, 1.019726505816471),\n",
       "  (60662, 1.0022400605626949),\n",
       "  (6125, 0.9420910795007793),\n",
       "  (6185, 0.8538660551578744),\n",
       "  (60476, 0.8315632614372123),\n",
       "  (24870, 0.8007034997829932),\n",
       "  (6247, 0.7995210506368283),\n",
       "  (66926, 0.7134402887162158),\n",
       "  (76929, 0.7071742804042507),\n",
       "  (7036, 0.6965101357307698),\n",
       "  (43231, 0.6718415779129535),\n",
       "  (38194, 0.6218106311252966),\n",
       "  (74497, 0.6144909253084895),\n",
       "  (39830, 0.6016591563687915),\n",
       "  (40568, 0.5658401142765238),\n",
       "  (6062, 0.5408864922013125),\n",
       "  (10474, 0.5338244311127749),\n",
       "  (26274, 0.5215669524176505),\n",
       "  (81497, 0.5212697468091115),\n",
       "  (7969, 0.4980310686168056),\n",
       "  (5347, 0.48469939173186893),\n",
       "  (58982, 0.4701839826618728),\n",
       "  (2564, 0.45031426472980524),\n",
       "  (76605, 0.41229503903345743),\n",
       "  (94695, 0.40683277088771275)],\n",
       " [(68085, 120.79144502658201),\n",
       "  (432, 120.74680144353958),\n",
       "  (26294, 120.70423766066372),\n",
       "  (76489, 120.65647559188737),\n",
       "  (83932, 120.53077804004809),\n",
       "  (24446, 120.50078996428181),\n",
       "  (76843, 120.4825840198202),\n",
       "  (63337, 120.47964971043994),\n",
       "  (19784, 120.47058893732563),\n",
       "  (64700, 120.46941819074846),\n",
       "  (71522, 120.46778617270331),\n",
       "  (52490, 120.45925542457339),\n",
       "  (14478, 120.45680860738237),\n",
       "  (66731, 120.44871776771267),\n",
       "  (71139, 120.43612746388118),\n",
       "  (848, 120.4186758102501),\n",
       "  (82517, 120.41782789332945),\n",
       "  (3900, 120.41608737022088),\n",
       "  (43989, 120.41373844616332),\n",
       "  (27839, 120.40530540234087),\n",
       "  (71065, 120.3826903462553),\n",
       "  (61708, 120.36560494089407),\n",
       "  (7955, 120.36493904667212),\n",
       "  (82995, 120.36025704151997),\n",
       "  (60399, 120.3576244021304),\n",
       "  (13228, 120.3564767090595),\n",
       "  (9198, 120.34828748580591),\n",
       "  (61683, 120.34249792852943),\n",
       "  (9192, 120.33699336765605),\n",
       "  (84868, 120.33143665220085)],\n",
       " [(90379, 3.9100444850989877),\n",
       "  (90422, 3.8853821397134114),\n",
       "  (90416, 3.6655816689987257),\n",
       "  (90415, 3.6458555998947997),\n",
       "  (90417, 3.5304658410281156),\n",
       "  (90513, 3.3689177830726074),\n",
       "  (90414, 3.3520185022508855),\n",
       "  (90368, 3.244996474360951),\n",
       "  (29658, 3.2227835177337565),\n",
       "  (43963, 3.11776307343817),\n",
       "  (28066, 3.0636350162777792),\n",
       "  (90502, 3.053725591053314),\n",
       "  (90425, 3.0455391659612268),\n",
       "  (29853, 3.025073101508313),\n",
       "  (90421, 2.9024477658002263),\n",
       "  (29801, 2.8638985127725327),\n",
       "  (29752, 2.8371601889215423),\n",
       "  (90335, 2.8223856848592437),\n",
       "  (90543, 2.803508750694845),\n",
       "  (29751, 2.8013988327688626),\n",
       "  (29848, 2.7877551447487616),\n",
       "  (90364, 2.6628148880566362),\n",
       "  (90520, 2.62245777712769),\n",
       "  (90306, 2.61361582316563),\n",
       "  (29845, 2.58665019167764),\n",
       "  (1771, 2.548181787155254),\n",
       "  (57225, 2.493603839581294),\n",
       "  (28816, 2.426813537500601),\n",
       "  (25955, 2.425649810982532),\n",
       "  (90418, 2.4150465566767094)],\n",
       " [(96117, 2.2960280791438517),\n",
       "  (97597, 2.284521855669909),\n",
       "  (94001, 2.2840664900936307),\n",
       "  (97548, 2.231655031898324),\n",
       "  (96125, 2.2137954819206214),\n",
       "  (91483, 1.995002597744775),\n",
       "  (71382, 1.8593531140044024),\n",
       "  (9027, 1.8095725463176429),\n",
       "  (61017, 1.7701434665123836),\n",
       "  (64620, 1.764221944518105),\n",
       "  (61710, 1.7607398043128724),\n",
       "  (95685, 1.749262805742712),\n",
       "  (54399, 1.7242383793340963),\n",
       "  (71615, 1.7189385533549524),\n",
       "  (68553, 1.7139636811851913),\n",
       "  (42846, 1.7124627233806973),\n",
       "  (21211, 1.7008858102256725),\n",
       "  (2866, 1.7006555510956685),\n",
       "  (10022, 1.694281713735157),\n",
       "  (10332, 1.657737582427427),\n",
       "  (37276, 1.6368917564022323),\n",
       "  (95923, 1.6276230822119424),\n",
       "  (43995, 1.6251527465115119),\n",
       "  (2482, 1.6241215614729434),\n",
       "  (65514, 1.6177688555700827),\n",
       "  (25797, 1.6105298277850664),\n",
       "  (71605, 1.5896474215371252),\n",
       "  (44997, 1.5650608320138488),\n",
       "  (71229, 1.5629328847772506),\n",
       "  (80883, 1.5484338850665482)],\n",
       " [(64809, 10.099434071333286),\n",
       "  (49004, 9.955660672683678),\n",
       "  (65036, 9.673092981074209),\n",
       "  (4507, 9.369821354921944),\n",
       "  (12970, 8.421838149205673),\n",
       "  (59841, 8.412956343371139),\n",
       "  (54295, 8.23855675444461),\n",
       "  (47968, 8.13683879695423),\n",
       "  (9941, 7.965389832911753),\n",
       "  (68080, 7.963505119411758),\n",
       "  (81864, 7.944409114280355),\n",
       "  (59799, 7.805871188599197),\n",
       "  (59614, 7.774731313051485),\n",
       "  (52635, 7.580791855797919),\n",
       "  (15860, 7.173144553082676),\n",
       "  (64399, 6.888959762174023),\n",
       "  (60363, 6.53222240552617),\n",
       "  (18980, 6.224267511595034),\n",
       "  (64422, 6.180824064280392),\n",
       "  (64371, 5.086971506607356),\n",
       "  (55580, 4.998635387971917),\n",
       "  (25073, 4.157868110571995),\n",
       "  (27966, 4.08145153003081),\n",
       "  (27586, 4.000671393028785),\n",
       "  (31239, 2.4795356456205475),\n",
       "  (45447, 2.0704091785711576),\n",
       "  (68085, 2.0517206923705804),\n",
       "  (42011, 2.0472124971578376),\n",
       "  (46540, 1.9988442829470057),\n",
       "  (73027, 1.975561576202474)],\n",
       " [(63681, 5.351965285750349),\n",
       "  (79561, 4.841217854459054),\n",
       "  (46684, 2.183168121346983),\n",
       "  (33775, 2.1380176346360638),\n",
       "  (64205, 2.104938656547416),\n",
       "  (94158, 2.097561318863376),\n",
       "  (4013, 2.0935308636953613),\n",
       "  (71275, 2.089597329605445),\n",
       "  (91790, 2.0744631292119693),\n",
       "  (66050, 1.9516106388562064),\n",
       "  (39633, 1.9451004359044741),\n",
       "  (76126, 1.9446453599961298),\n",
       "  (44401, 1.9010026661981718),\n",
       "  (51633, 1.8634779023798833),\n",
       "  (38723, 1.8251919346067371),\n",
       "  (35892, 1.820403891958349),\n",
       "  (59696, 1.8196539203719628),\n",
       "  (21508, 1.801661296945877),\n",
       "  (39644, 1.8011966168897322),\n",
       "  (26485, 1.7850976978153688),\n",
       "  (51757, 1.7790852539928115),\n",
       "  (37927, 1.7748139877234235),\n",
       "  (39451, 1.7601119926357507),\n",
       "  (20664, 1.751859767271431),\n",
       "  (31857, 1.751673104454753),\n",
       "  (50970, 1.7440022211518527),\n",
       "  (60162, 1.7366230349915863),\n",
       "  (85992, 1.7355935876969757),\n",
       "  (39987, 1.725277314798432),\n",
       "  (33141, 1.6888454717023205)],\n",
       " [(44386, 99.84956145234864),\n",
       "  (77392, 99.80215232484987),\n",
       "  (3287, 0.759226608567857),\n",
       "  (27134, 0.7526981169248842),\n",
       "  (32273, 0.7356801391511224),\n",
       "  (38225, 0.6438588982304128),\n",
       "  (28879, 0.6081938546341031),\n",
       "  (78328, 0.5795582045778729),\n",
       "  (79703, 0.5746167733139538),\n",
       "  (92768, 0.5668631631888935),\n",
       "  (39628, 0.5199435245345894),\n",
       "  (11479, 0.5118177961346037),\n",
       "  (88443, 0.500099850233384),\n",
       "  (36954, 0.49601744699845574),\n",
       "  (77666, 0.4958527618109008),\n",
       "  (35219, 0.49470830380301867),\n",
       "  (46610, 0.4853798763124734),\n",
       "  (65288, 0.46714016577211503),\n",
       "  (38473, 0.46502943249189616),\n",
       "  (36992, 0.46217485019483306),\n",
       "  (26573, 0.4561843084096895),\n",
       "  (12149, 0.4487850403660327),\n",
       "  (10070, 0.44846487566254134),\n",
       "  (3028, 0.44255501134691844),\n",
       "  (39868, 0.43937077759408766),\n",
       "  (71921, 0.4382987173290161),\n",
       "  (94667, 0.43768090259187004),\n",
       "  (77703, 0.4354746864752001),\n",
       "  (6944, 0.4343450416151151),\n",
       "  (651, 0.4328204226788317)],\n",
       " [(7183, 0.3920187935593425),\n",
       "  (96256, 0.3443727183421706),\n",
       "  (97553, 0.3386466821367478),\n",
       "  (58145, 0.07582462507177574),\n",
       "  (98413, 0.0735162993836965),\n",
       "  (50294, 0.0686160135653632),\n",
       "  (71542, 0.06753769521359163),\n",
       "  (43227, 0.06423561815229233),\n",
       "  (87383, 0.06225726806276937),\n",
       "  (97849, 0.06166011861042681),\n",
       "  (33080, 0.06120668850033006),\n",
       "  (33036, 0.059890083044620145),\n",
       "  (46515, 0.05977237333849654),\n",
       "  (36522, 0.05878147442867178),\n",
       "  (92684, 0.05869212542984856),\n",
       "  (92726, 0.05834565842557829),\n",
       "  (58323, 0.05745117270838057),\n",
       "  (91948, 0.05660800470369344),\n",
       "  (59428, 0.05459133499271917),\n",
       "  (38255, 0.053726971021935736),\n",
       "  (45112, 0.050731518289742795),\n",
       "  (891, 0.050695739510413486),\n",
       "  (17141, 0.04893258861822164),\n",
       "  (61085, 0.04811670512527522),\n",
       "  (98232, 0.047378627845782915),\n",
       "  (47453, 0.04735592761222523),\n",
       "  (56182, 0.04495436149431626),\n",
       "  (58005, 0.0446547866300416),\n",
       "  (52729, 0.041513243707915946),\n",
       "  (41205, 0.04090124352361167)],\n",
       " [(56865, 101.62635251335705),\n",
       "  (7946, 101.28977928136557),\n",
       "  (94870, 101.23452448032981),\n",
       "  (79674, 101.11757109722072),\n",
       "  (91625, 101.11022191252762),\n",
       "  (91624, 100.9497348302216),\n",
       "  (76395, 100.90856865021888),\n",
       "  (67885, 100.88587674897633),\n",
       "  (91801, 100.69111647484196),\n",
       "  (1353, 100.68695348752716),\n",
       "  (6640, 100.59508497043595),\n",
       "  (8576, 100.58499509564679),\n",
       "  (90572, 100.20994709216285),\n",
       "  (5820, 100.19513157044004),\n",
       "  (4171, 100.18926486321587),\n",
       "  (97522, 100.18454854190786),\n",
       "  (8786, 100.177205351443),\n",
       "  (85592, 100.162187166659),\n",
       "  (90872, 100.16059764130614),\n",
       "  (19030, 100.1601333422737),\n",
       "  (30299, 100.15463918593454),\n",
       "  (9483, 100.14415844889022),\n",
       "  (71641, 100.13624347059013),\n",
       "  (91372, 100.13461239019735),\n",
       "  (93752, 100.1329625207338),\n",
       "  (8881, 100.12115964516016),\n",
       "  (90606, 100.0806116932943),\n",
       "  (23856, 100.02897858017775),\n",
       "  (71504, 100.01359631739405),\n",
       "  (55043, 3.03202657032959)],\n",
       " [(83463, 3.492018362942887),\n",
       "  (28465, 3.3884057430721715),\n",
       "  (29882, 3.3810000166711687),\n",
       "  (30052, 3.2651480845919694),\n",
       "  (30801, 3.244589492118865),\n",
       "  (30282, 3.205501542388083),\n",
       "  (30271, 3.197385034949687),\n",
       "  (1213, 3.1877897484031714),\n",
       "  (29900, 3.1835029387994975),\n",
       "  (25196, 3.1756703700506717),\n",
       "  (2422, 3.1633403387499555),\n",
       "  (28863, 3.129873863153521),\n",
       "  (63229, 3.105346570763749),\n",
       "  (25622, 3.1009151372763513),\n",
       "  (8680, 3.094326882734532),\n",
       "  (6286, 3.086161157898172),\n",
       "  (91223, 3.0259583298757624),\n",
       "  (1982, 2.9819061761874672),\n",
       "  (1809, 2.9233246699107114),\n",
       "  (24559, 2.9131268425915886),\n",
       "  (26874, 2.9023599856432125),\n",
       "  (30731, 2.8829548434726044),\n",
       "  (62489, 2.8788106776208258),\n",
       "  (68047, 2.8551884954452076),\n",
       "  (28728, 2.8451582380161002),\n",
       "  (29925, 2.7989711397692707),\n",
       "  (29939, 2.793717713499067),\n",
       "  (30286, 2.779391052986609),\n",
       "  (5762, 2.7672057503816956),\n",
       "  (29940, 2.742164864900385)],\n",
       " [(97076, 0.8222971516691486),\n",
       "  (52792, 0.8106821884054949),\n",
       "  (62807, 0.809884252803573),\n",
       "  (16896, 0.7985277300795381),\n",
       "  (67525, 0.7501779539571509),\n",
       "  (63914, 0.746497161748838),\n",
       "  (95765, 0.7258082104473049),\n",
       "  (54525, 0.690229675106449),\n",
       "  (95736, 0.6780359645085431),\n",
       "  (96570, 0.6730545030921575),\n",
       "  (52465, 0.6428485939941622),\n",
       "  (59048, 0.6152978920881028),\n",
       "  (61993, 0.6109754980686389),\n",
       "  (35486, 0.5992821603709831),\n",
       "  (97223, 0.5968366024003923),\n",
       "  (12887, 0.5883337055505737),\n",
       "  (12879, 0.5687404144861918),\n",
       "  (45083, 0.5531484694614178),\n",
       "  (11631, 0.5503927312910033),\n",
       "  (10288, 0.5474000033297592),\n",
       "  (97085, 0.5020323810857628),\n",
       "  (52686, 0.4997875104462759),\n",
       "  (54690, 0.49009321536985473),\n",
       "  (47674, 0.48897956186425473),\n",
       "  (97074, 0.4527842767156606),\n",
       "  (55208, 0.4271137302447623),\n",
       "  (93714, 0.41673655142345595),\n",
       "  (9853, 0.4136422229342528),\n",
       "  (55587, 0.3915773289706377),\n",
       "  (28173, 0.385680624839741)],\n",
       " [(60590, 3.2950339993151943),\n",
       "  (97021, 3.2030629844977403),\n",
       "  (3364, 3.0784393706866706),\n",
       "  (46949, 3.058209748719536),\n",
       "  (32973, 3.0156509340004445),\n",
       "  (37128, 2.9924048955458162),\n",
       "  (48114, 2.9788368569855255),\n",
       "  (42365, 2.8912495629463555),\n",
       "  (8006, 2.8656897606527263),\n",
       "  (32855, 2.7799959045083256),\n",
       "  (40810, 2.776706679168873),\n",
       "  (27362, 2.6979680136110114),\n",
       "  (40407, 2.6852747931330465),\n",
       "  (78359, 2.6804632167631275),\n",
       "  (11991, 2.6711756607766084),\n",
       "  (31756, 2.641715853437605),\n",
       "  (40230, 2.5454756326213177),\n",
       "  (48906, 2.537035156600014),\n",
       "  (26907, 2.5309187236199002),\n",
       "  (41238, 2.494526535895735),\n",
       "  (26756, 2.4812900746134714),\n",
       "  (54073, 2.4229872572461226),\n",
       "  (77312, 2.3460685188369115),\n",
       "  (91141, 2.129638045900164),\n",
       "  (92164, 2.117169158514701),\n",
       "  (48144, 2.090395806126161),\n",
       "  (92156, 1.9579704213436315),\n",
       "  (92181, 1.9548751820678614),\n",
       "  (92169, 1.9521705359489072),\n",
       "  (27959, 1.9277996343623467)],\n",
       " [(87614, 101.35228972144971),\n",
       "  (30182, 101.01059692435359),\n",
       "  (90985, 100.68074945510483),\n",
       "  (90948, 100.62958032312342),\n",
       "  (34981, 100.6228549690562),\n",
       "  (93076, 100.61434060545879),\n",
       "  (94987, 100.6064190217038),\n",
       "  (87134, 100.60359823956159),\n",
       "  (90915, 100.59912620794036),\n",
       "  (87423, 100.59885289311168),\n",
       "  (83994, 100.59670973450199),\n",
       "  (87463, 100.56118991353343),\n",
       "  (97830, 100.54393992317688),\n",
       "  (97079, 100.5376099956353),\n",
       "  (92151, 100.51660524933529),\n",
       "  (91160, 100.47097893128428),\n",
       "  (91921, 100.38419900613212),\n",
       "  (97721, 100.33157200760635),\n",
       "  (86965, 100.29793752296669),\n",
       "  (87425, 100.29778732405212),\n",
       "  (35906, 100.27638647019614),\n",
       "  (92185, 100.26604690996588),\n",
       "  (87521, 100.25975161830327),\n",
       "  (94116, 100.2563423347747),\n",
       "  (87574, 100.24143642601717),\n",
       "  (91212, 100.24069458421212),\n",
       "  (91152, 100.19685471554214),\n",
       "  (93078, 100.18621670626058),\n",
       "  (94957, 100.17976604164268),\n",
       "  (88261, 100.17616561942911)],\n",
       " [(51063, 5.0068020321424),\n",
       "  (65092, 4.869654557232587),\n",
       "  (46380, 4.8428459251335925),\n",
       "  (65293, 4.406719167059927),\n",
       "  (37430, 4.328278538174847),\n",
       "  (41860, 4.2862567452814595),\n",
       "  (29522, 4.25695368538489),\n",
       "  (64690, 4.210479419898092),\n",
       "  (42480, 4.169818091498568),\n",
       "  (23926, 4.081957462313421),\n",
       "  (56424, 4.043908279573786),\n",
       "  (29404, 4.004931772313543),\n",
       "  (63447, 3.9714081458873958),\n",
       "  (43338, 3.966689182727457),\n",
       "  (65176, 3.936985055099659),\n",
       "  (65151, 3.863013316104668),\n",
       "  (42893, 3.834181412887795),\n",
       "  (65069, 3.810849401706318),\n",
       "  (1786, 3.767585496438908),\n",
       "  (65063, 3.7635884544917078),\n",
       "  (65056, 3.759851536433884),\n",
       "  (65290, 3.736165381028403),\n",
       "  (4392, 3.6920720036941663),\n",
       "  (46533, 3.6701862895920705),\n",
       "  (65074, 3.6545530634966825),\n",
       "  (39562, 3.637182936196822),\n",
       "  (65190, 3.5986274863235472),\n",
       "  (65288, 3.5618367073913606),\n",
       "  (56701, 3.5522070107959163),\n",
       "  (39471, 3.5015874454213654)],\n",
       " [(19205, 7.366952784251957),\n",
       "  (818, 5.934713702528242),\n",
       "  (64631, 5.849707695870526),\n",
       "  (22064, 5.531661379986234),\n",
       "  (22246, 5.508020579686311),\n",
       "  (73078, 5.47502499084458),\n",
       "  (34725, 5.358575878999959),\n",
       "  (23139, 5.249996762088987),\n",
       "  (22777, 4.925480966527389),\n",
       "  (23040, 4.823738919150362),\n",
       "  (22386, 4.685431275086703),\n",
       "  (23501, 4.646070381395843),\n",
       "  (73054, 4.600124176519992),\n",
       "  (55054, 4.198857120424599),\n",
       "  (923, 2.8525998834392343),\n",
       "  (81862, 2.849171697412797),\n",
       "  (41216, 2.839085645831082),\n",
       "  (11262, 2.7825781009659667),\n",
       "  (60478, 2.703446476803926),\n",
       "  (39738, 2.7027001253807),\n",
       "  (39681, 2.6958445660198045),\n",
       "  (54210, 2.6723803620223805),\n",
       "  (33137, 2.66098508086895),\n",
       "  (58583, 2.6427794998184506),\n",
       "  (13969, 2.6374360800323218),\n",
       "  (42116, 2.594719735384752),\n",
       "  (39704, 2.5829657963744737),\n",
       "  (305, 2.5450301242624276),\n",
       "  (41984, 2.529277177115988),\n",
       "  (57360, 2.525293089021258)],\n",
       " [(97042, 20.661495996457752),\n",
       "  (64801, 20.612868022693714),\n",
       "  (24710, 20.55598370952004),\n",
       "  (65486, 20.54209541318245),\n",
       "  (48258, 20.476379712247155),\n",
       "  (73446, 20.455819516110534),\n",
       "  (35645, 20.44154848295052),\n",
       "  (48369, 20.420662216968886),\n",
       "  (3363, 20.416865021094743),\n",
       "  (1856, 20.41608767887673),\n",
       "  (17386, 20.349063164101484),\n",
       "  (24989, 20.332471194620144),\n",
       "  (97052, 20.295717954305843),\n",
       "  (63155, 20.283851187120383),\n",
       "  (26407, 20.276564294984347),\n",
       "  (3405, 20.257390558936372),\n",
       "  (78098, 20.25376118311732),\n",
       "  (62752, 20.25038679128544),\n",
       "  (53074, 20.24355898840607),\n",
       "  (47816, 20.231315675860962),\n",
       "  (29918, 20.224995515357485),\n",
       "  (68365, 20.224419636690467),\n",
       "  (3370, 20.191326723307853),\n",
       "  (47807, 20.118534691333274),\n",
       "  (47803, 20.117604876414443),\n",
       "  (48264, 20.11689584624176),\n",
       "  (26552, 20.102796932232703),\n",
       "  (17175, 20.10098389572537),\n",
       "  (51917, 20.100568268682807),\n",
       "  (26772, 20.098067910717443)],\n",
       " [(74999, 106.16590197925264),\n",
       "  (38427, 102.18632298556312),\n",
       "  (78753, 101.98156558934332),\n",
       "  (53215, 101.95976340215248),\n",
       "  (50639, 101.77866783276386),\n",
       "  (80065, 101.76398496008734),\n",
       "  (11190, 101.70831174563348),\n",
       "  (48315, 101.65615459810029),\n",
       "  (38399, 101.65158779300447),\n",
       "  (76693, 101.63727231666287),\n",
       "  (36013, 101.63058010959139),\n",
       "  (90982, 101.63017622287035),\n",
       "  (80102, 101.57142117244568),\n",
       "  (40873, 101.54881741862455),\n",
       "  (77751, 101.54530543097944),\n",
       "  (67258, 101.51475217805603),\n",
       "  (48300, 101.4882254369977),\n",
       "  (46260, 101.48665623760157),\n",
       "  (24435, 101.4596730253802),\n",
       "  (32433, 101.4375717760211),\n",
       "  (31759, 101.42287295320136),\n",
       "  (95222, 101.2961262431246),\n",
       "  (37322, 101.28857382416217),\n",
       "  (28445, 101.28828371767523),\n",
       "  (77099, 101.23116037502668),\n",
       "  (36226, 101.2228155025902),\n",
       "  (75978, 101.21114203119146),\n",
       "  (34277, 101.20467738779655),\n",
       "  (9843, 101.20387263526025),\n",
       "  (86538, 101.20185646767864)],\n",
       " [(24135, 5.052112850809198),\n",
       "  (23988, 4.798246872359906),\n",
       "  (24402, 4.4077414529786445),\n",
       "  (29, 4.296343228562601),\n",
       "  (53513, 4.295432644192006),\n",
       "  (24375, 4.277077588496633),\n",
       "  (17030, 4.263824784673104),\n",
       "  (23987, 4.238679771398711),\n",
       "  (24930, 4.106789635760612),\n",
       "  (23902, 4.061209455804922),\n",
       "  (24754, 4.026921866798026),\n",
       "  (24306, 4.023297075628451),\n",
       "  (24927, 3.9662418591656463),\n",
       "  (19474, 3.948206982156275),\n",
       "  (87758, 3.9438080921103427),\n",
       "  (24085, 3.942222117650002),\n",
       "  (24581, 3.894719766730444),\n",
       "  (24241, 3.8852807809114034),\n",
       "  (24673, 3.880380181100448),\n",
       "  (24205, 3.8710003864953184),\n",
       "  (24066, 3.8630549560390848),\n",
       "  (24376, 3.823863430216387),\n",
       "  (1559, 3.815822188877348),\n",
       "  (404, 3.814315225201706),\n",
       "  (15929, 3.8004918621527373),\n",
       "  (75175, 3.7985426249938037),\n",
       "  (41899, 3.785821096475824),\n",
       "  (23952, 3.7824356371726697),\n",
       "  (24852, 3.77129863689885),\n",
       "  (24401, 3.7206757595654665)],\n",
       " [(29983, 3.12726389242281),\n",
       "  (27425, 3.0501198174971584),\n",
       "  (80570, 2.981461429558234),\n",
       "  (27992, 2.827918238944988),\n",
       "  (25912, 2.7876931701213987),\n",
       "  (81154, 2.7853881725464973),\n",
       "  (82193, 2.7485373064742307),\n",
       "  (46989, 2.708420986921044),\n",
       "  (33488, 2.661447735495363),\n",
       "  (29370, 2.6538402630433624),\n",
       "  (23922, 2.630649865134892),\n",
       "  (923, 2.6300459208583016),\n",
       "  (25644, 2.5273253640865447),\n",
       "  (25217, 2.5234493183040794),\n",
       "  (34474, 2.512172465638076),\n",
       "  (25147, 2.5121598965319905),\n",
       "  (28364, 2.506788850180484),\n",
       "  (60572, 2.5043558119090434),\n",
       "  (25570, 2.4976690487895024),\n",
       "  (25704, 2.455599255044736),\n",
       "  (56521, 2.436849366570627),\n",
       "  (26132, 2.4221639402285113),\n",
       "  (31066, 2.416348433084555),\n",
       "  (81825, 2.411632851254977),\n",
       "  (28560, 2.3924443939061604),\n",
       "  (80521, 2.3851437298834686),\n",
       "  (24306, 2.3835928972468476),\n",
       "  (24552, 2.3792268809508044),\n",
       "  (29808, 2.3482606155331345),\n",
       "  (25215, 2.34148126713276)],\n",
       " [(92636, 2.2967135976765043),\n",
       "  (29325, 2.229272884875084),\n",
       "  (51756, 2.213557574223555),\n",
       "  (40191, 2.159408311165766),\n",
       "  (28282, 2.0732921980058054),\n",
       "  (29803, 1.9194135592844475),\n",
       "  (94213, 1.903091965616785),\n",
       "  (42584, 1.8813564815216297),\n",
       "  (64377, 1.8242017223750344),\n",
       "  (42240, 1.8222179085443042),\n",
       "  (77205, 1.744023043730878),\n",
       "  (29735, 1.7040443150785682),\n",
       "  (59048, 1.6949094471950268),\n",
       "  (90425, 1.5309792748515232),\n",
       "  (39377, 1.459450678073047),\n",
       "  (59009, 1.427765714552499),\n",
       "  (51235, 1.4078234105357457),\n",
       "  (65890, 1.4014249278142756),\n",
       "  (29764, 1.397478560777154),\n",
       "  (45783, 1.3892966035763574),\n",
       "  (55972, 1.3797730203780283),\n",
       "  (92451, 1.3574914533343965),\n",
       "  (57961, 1.3370918739446154),\n",
       "  (93086, 1.2874833424905354),\n",
       "  (27425, 1.2686819562097917),\n",
       "  (65844, 1.2604791321944122),\n",
       "  (67230, 1.2600081172578548),\n",
       "  (90308, 1.2566125475440009),\n",
       "  (28227, 1.2348068767827103),\n",
       "  (29777, 1.2338772596146435)],\n",
       " [(55580, 2.825295557836403),\n",
       "  (54412, 2.791209312969984),\n",
       "  (46668, 2.7546891484956704),\n",
       "  (38159, 2.6826640256285192),\n",
       "  (64604, 2.4120948718391246),\n",
       "  (38637, 2.345607003632652),\n",
       "  (31435, 2.2772511934297004),\n",
       "  (37976, 2.2558563552685644),\n",
       "  (39249, 2.24868788278632),\n",
       "  (58285, 2.2383197180371512),\n",
       "  (36675, 2.189600360222533),\n",
       "  (53514, 2.1255006505994394),\n",
       "  (56324, 2.1250212458863635),\n",
       "  (25603, 2.117939414384912),\n",
       "  (41496, 2.105731224704283),\n",
       "  (33992, 2.066720897893336),\n",
       "  (94197, 2.0605544387268004),\n",
       "  (47500, 2.0445704971450462),\n",
       "  (27586, 2.0327379305814617),\n",
       "  (48113, 2.019608939398972),\n",
       "  (37749, 2.01043438290012),\n",
       "  (37536, 1.9844341111780526),\n",
       "  (46605, 1.9737524899227548),\n",
       "  (42823, 1.972821150799564),\n",
       "  (56169, 1.9647704709341427),\n",
       "  (36088, 1.9396831476799226),\n",
       "  (39403, 1.9344488500755177),\n",
       "  (93981, 1.9329644661278784),\n",
       "  (58437, 1.9268161826506316),\n",
       "  (54885, 1.9222572986754045)],\n",
       " [(33389, 102.54457837296738),\n",
       "  (32804, 102.1432431945749),\n",
       "  (32849, 102.05391681108672),\n",
       "  (34930, 101.76131068635357),\n",
       "  (50434, 101.75972753301485),\n",
       "  (41593, 101.72956852327279),\n",
       "  (43335, 101.62625036397854),\n",
       "  (40119, 101.59242122267085),\n",
       "  (50119, 101.57509421799433),\n",
       "  (35290, 101.559165770373),\n",
       "  (77216, 101.50582096716722),\n",
       "  (36330, 101.479584783714),\n",
       "  (69798, 101.41387391853677),\n",
       "  (56429, 100.89961133977376),\n",
       "  (1915, 100.86486926276892),\n",
       "  (91634, 5.091308093486111),\n",
       "  (18890, 3.2584813813524756),\n",
       "  (31797, 3.2217358964025147),\n",
       "  (31346, 3.135873027478528),\n",
       "  (35418, 3.085978590811373),\n",
       "  (31412, 3.0725352041462903),\n",
       "  (31653, 3.066449768262148),\n",
       "  (31411, 3.057203335042141),\n",
       "  (31328, 3.0342271541170005),\n",
       "  (32725, 3.0208764195466005),\n",
       "  (31580, 3.0160769993062164),\n",
       "  (31383, 3.0063026380931435),\n",
       "  (31631, 2.9999624833346465),\n",
       "  (31392, 2.9968692664336487),\n",
       "  (33695, 2.986598946097994)],\n",
       " [(308, 4.995775737048071),\n",
       "  (88366, 4.922216394351044),\n",
       "  (3331, 4.616783525871646),\n",
       "  (918, 4.565307535932995),\n",
       "  (161, 4.416050968322853),\n",
       "  (87381, 4.364877260279731),\n",
       "  (76517, 4.363577273965077),\n",
       "  (84347, 4.3547350067569965),\n",
       "  (13399, 4.347360342877888),\n",
       "  (4382, 4.301965326777087),\n",
       "  (85095, 4.156159537043674),\n",
       "  (53113, 4.1113405825041065),\n",
       "  (43670, 4.013932897208083),\n",
       "  (55659, 3.9895201215144827),\n",
       "  (67417, 3.9845962277364086),\n",
       "  (58595, 3.956990224577006),\n",
       "  (90313, 3.9334823013141587),\n",
       "  (13411, 3.863971139652689),\n",
       "  (55812, 3.835511103024013),\n",
       "  (18785, 3.792709253384394),\n",
       "  (61726, 3.7862679480804733),\n",
       "  (27825, 3.7511315308091784),\n",
       "  (10494, 3.74449475726972),\n",
       "  (354, 3.7258035748069025),\n",
       "  (62352, 3.7234010206341384),\n",
       "  (61548, 3.718173070354845),\n",
       "  (60540, 3.7112868836597612),\n",
       "  (60816, 3.7013416599103524),\n",
       "  (21352, 3.6750109151014607),\n",
       "  (20926, 3.6740455081315035)],\n",
       " [(41642, 21.550756131786716),\n",
       "  (86598, 21.483239949701463),\n",
       "  (89931, 21.413350032899746),\n",
       "  (55482, 21.319150187000552),\n",
       "  (45544, 21.291278821373197),\n",
       "  (17927, 21.25684845043034),\n",
       "  (86608, 20.957299058670053),\n",
       "  (17914, 20.902108502334407),\n",
       "  (84802, 20.8880972495232),\n",
       "  (17434, 20.8719541817113),\n",
       "  (86831, 20.869969356802688),\n",
       "  (86597, 20.86594149570648),\n",
       "  (71181, 20.824666688809973),\n",
       "  (61586, 20.817974728440337),\n",
       "  (71356, 20.81154802155645),\n",
       "  (81738, 20.810614503999393),\n",
       "  (56358, 20.785093410029827),\n",
       "  (64442, 20.768307575495328),\n",
       "  (17688, 20.758989783271073),\n",
       "  (42485, 20.746037929041897),\n",
       "  (52353, 20.74549994029623),\n",
       "  (89563, 20.744987568471167),\n",
       "  (17686, 20.73999042294967),\n",
       "  (18210, 20.73672881642777),\n",
       "  (42174, 20.733241128838444),\n",
       "  (22254, 20.726299310739805),\n",
       "  (45065, 20.721990598955998),\n",
       "  (53994, 20.70993770571024),\n",
       "  (70305, 20.70590946347445),\n",
       "  (85089, 20.701716566135794)],\n",
       " [(98273, 100.77474987880252),\n",
       "  (67434, 100.44358663198143),\n",
       "  (82963, 100.37828940739773),\n",
       "  (5996, 100.31191030718959),\n",
       "  (18879, 100.30242164554204),\n",
       "  (83175, 100.29084404309897),\n",
       "  (89968, 100.28502744289491),\n",
       "  (17430, 100.27052230397261),\n",
       "  (84955, 100.26680555722409),\n",
       "  (18940, 100.23863546279584),\n",
       "  (57049, 100.19087851314174),\n",
       "  (73308, 100.19057643887037),\n",
       "  (40097, 100.18836682714947),\n",
       "  (29139, 100.13352123606387),\n",
       "  (21256, 100.13223717543623),\n",
       "  (19194, 100.12101011720695),\n",
       "  (27955, 100.10524723734305),\n",
       "  (89057, 100.0878119278008),\n",
       "  (49796, 100.08543822679833),\n",
       "  (18914, 100.08133495140054),\n",
       "  (7929, 100.05480134274794),\n",
       "  (9846, 100.04552275956853),\n",
       "  (77024, 100.03124875761536),\n",
       "  (91610, 100.0186603817535),\n",
       "  (17673, 100.00646355833592),\n",
       "  (73675, 99.988756415025),\n",
       "  (8082, 99.93615033317639),\n",
       "  (86198, 99.90466940080789),\n",
       "  (7542, 99.8747747211326),\n",
       "  (56271, 99.86220343511297)],\n",
       " [(94001, 2.1067150519986786),\n",
       "  (94480, 1.888726067080661),\n",
       "  (94517, 1.8773079080513766),\n",
       "  (93942, 1.876665484552109),\n",
       "  (93882, 1.8716464887109976),\n",
       "  (93933, 1.8682128662479092),\n",
       "  (94076, 1.863343578028193),\n",
       "  (94014, 1.8574816885290906),\n",
       "  (93476, 1.8453704584882844),\n",
       "  (93123, 1.8443992582651438),\n",
       "  (92874, 1.8418344063495713),\n",
       "  (94075, 1.8371886677803324),\n",
       "  (94142, 1.8344506506799956),\n",
       "  (93932, 1.8326186842778132),\n",
       "  (94306, 1.8234790606191345),\n",
       "  (94053, 1.8186342149685588),\n",
       "  (80622, 1.770316764423351),\n",
       "  (24647, 1.7700790202393317),\n",
       "  (24316, 1.691110415072517),\n",
       "  (26375, 1.6795557937632872),\n",
       "  (28462, 1.657193225523869),\n",
       "  (63417, 1.6546070475329209),\n",
       "  (94461, 1.647062164128177),\n",
       "  (41902, 1.6445434881274203),\n",
       "  (24308, 1.6243495405879038),\n",
       "  (25196, 1.6224221706376638),\n",
       "  (94089, 1.6086591487899358),\n",
       "  (80106, 1.6006672379573892),\n",
       "  (48898, 1.5853819700404803),\n",
       "  (84466, 1.5819708857206756)],\n",
       " [(63681, 5.534762254572961),\n",
       "  (79561, 5.076852362384264),\n",
       "  (28728, 3.481795612204359),\n",
       "  (25196, 3.2348858783833436),\n",
       "  (26874, 3.1318800754838905),\n",
       "  (83463, 3.0694079708495243),\n",
       "  (55633, 3.0597991517879253),\n",
       "  (30232, 3.0557630805935365),\n",
       "  (91223, 3.0259583298757624),\n",
       "  (1809, 3.0227474057481585),\n",
       "  (29882, 2.949223205706641),\n",
       "  (30052, 2.9099219100193623),\n",
       "  (6214, 2.776827397322568),\n",
       "  (29939, 2.7665698841349897),\n",
       "  (1213, 2.7290307485163003),\n",
       "  (57164, 2.6982487910970065),\n",
       "  (29900, 2.685631343271387),\n",
       "  (2422, 2.664626965697936),\n",
       "  (28863, 2.660522585893103),\n",
       "  (1982, 2.5879289748311014),\n",
       "  (63600, 2.54830708156941),\n",
       "  (29873, 2.520923753684087),\n",
       "  (29868, 2.5154649560455047),\n",
       "  (30282, 2.510179465860101),\n",
       "  (63229, 2.508813453755747),\n",
       "  (3544, 2.4920045015899888),\n",
       "  (29925, 2.490718247623741),\n",
       "  (30286, 2.4842004715809196),\n",
       "  (30271, 2.452558648632404),\n",
       "  (12279, 2.446380565354917)],\n",
       " [(42563, 2.1551918132488734),\n",
       "  (39063, 2.1534487482224396),\n",
       "  (6080, 2.0203956400314085),\n",
       "  (49546, 1.501920419281703),\n",
       "  (36420, 1.4917847735908427),\n",
       "  (43508, 1.4905979334525332),\n",
       "  (65844, 1.4891318538577303),\n",
       "  (66108, 1.4553329519871703),\n",
       "  (48833, 1.4426906877880195),\n",
       "  (45808, 1.4359464925521221),\n",
       "  (37548, 1.429838347756431),\n",
       "  (49437, 1.4104348800217235),\n",
       "  (42908, 1.3957909259148171),\n",
       "  (35294, 1.38502100442896),\n",
       "  (364, 1.3799646294564756),\n",
       "  (54456, 1.3779321106500815),\n",
       "  (6121, 1.3687213836904815),\n",
       "  (65757, 1.3673993125077955),\n",
       "  (38596, 1.3633320693650166),\n",
       "  (66089, 1.338505817393988),\n",
       "  (65974, 1.335672917414669),\n",
       "  (88104, 1.3128884710638857),\n",
       "  (89968, 1.29665844891271),\n",
       "  (65549, 1.2732480054600595),\n",
       "  (55656, 1.2707474676882204),\n",
       "  (66063, 1.2700850687847909),\n",
       "  (84094, 1.2538126557488691),\n",
       "  (80139, 1.2460558833608615),\n",
       "  (65758, 1.2384744155561753),\n",
       "  (65654, 1.2358419313107827)],\n",
       " [(25265, 105.37867118932303),\n",
       "  (25777, 103.68667755428524),\n",
       "  (25609, 103.25084032533749),\n",
       "  (27535, 103.24651268018673),\n",
       "  (95547, 103.20339637660274),\n",
       "  (28332, 102.68934624586882),\n",
       "  (25406, 102.48578517893425),\n",
       "  (87780, 101.7412930563098),\n",
       "  (25015, 5.605132116231783),\n",
       "  (25144, 5.5677321701319755),\n",
       "  (25476, 5.521946034454416),\n",
       "  (25039, 5.221413504634383),\n",
       "  (25032, 4.943331637960475),\n",
       "  (24629, 4.880186041475385),\n",
       "  (25167, 4.758337845902428),\n",
       "  (27161, 4.735573845668873),\n",
       "  (25267, 4.72441680002037),\n",
       "  (26565, 4.661123422527342),\n",
       "  (25295, 4.634381976860095),\n",
       "  (26730, 4.342473332558865),\n",
       "  (26406, 4.335828837157571),\n",
       "  (25320, 4.28930738151057),\n",
       "  (46949, 4.288688733937675),\n",
       "  (26720, 4.179118984277078),\n",
       "  (25380, 4.169897975550687),\n",
       "  (25031, 4.139068224831769),\n",
       "  (25119, 4.1381964366836765),\n",
       "  (25117, 4.0772571601512695),\n",
       "  (27163, 4.049627590204203),\n",
       "  (26775, 4.005252395741866)],\n",
       " [(24375, 4.7217209056729885),\n",
       "  (24135, 4.6832218973825945),\n",
       "  (23902, 4.611618487593562),\n",
       "  (23988, 4.515222208018981),\n",
       "  (53513, 4.475726024589477),\n",
       "  (29, 4.407908662540848),\n",
       "  (17030, 4.339464809396722),\n",
       "  (19474, 4.244687851644764),\n",
       "  (24241, 4.198235205580473),\n",
       "  (24358, 4.167765846623856),\n",
       "  (23911, 4.151449131035934),\n",
       "  (23952, 4.083172485841632),\n",
       "  (24754, 4.057637415636595),\n",
       "  (3653, 4.033420453010165),\n",
       "  (404, 3.9490467355905605),\n",
       "  (23987, 3.849061095239254),\n",
       "  (80414, 3.826120470955576),\n",
       "  (87758, 3.7661689831739102),\n",
       "  (24393, 3.764580421848084),\n",
       "  (24930, 3.7617340694649855),\n",
       "  (24870, 3.7515358899282414),\n",
       "  (1559, 3.7390128656768074),\n",
       "  (24581, 3.7332716025095745),\n",
       "  (24205, 3.6951108397459502),\n",
       "  (41899, 3.693679892343092),\n",
       "  (24825, 3.6637805980595046),\n",
       "  (24927, 3.6303003507909373),\n",
       "  (24673, 3.6207106748869333),\n",
       "  (24306, 3.6079599450506827),\n",
       "  (3159, 3.6038813527398155)],\n",
       " [(93790, 100.48027814295232),\n",
       "  (91072, 100.26149142115553),\n",
       "  (90565, 99.78454662259216),\n",
       "  (90978, 99.6989397769481),\n",
       "  (2009, 4.712076186077693),\n",
       "  (76413, 4.610597537859016),\n",
       "  (54377, 3.9231675062784026),\n",
       "  (46844, 3.657329105369856),\n",
       "  (41405, 3.077164648815026),\n",
       "  (67734, 2.9670179633422706),\n",
       "  (13288, 2.612838428089518),\n",
       "  (76154, 2.4816934222927913),\n",
       "  (41329, 2.44318946208129),\n",
       "  (14440, 2.3273673350513753),\n",
       "  (79555, 2.2312449114969244),\n",
       "  (16544, 2.1978872461845746),\n",
       "  (44602, 2.1897986734779287),\n",
       "  (75773, 2.1385781087161413),\n",
       "  (32928, 2.1242332274047238),\n",
       "  (67635, 2.0893499665044124),\n",
       "  (76170, 2.0884234162639617),\n",
       "  (16338, 2.0689606220997128),\n",
       "  (14439, 2.0423041835054896),\n",
       "  (15850, 2.030331258610925),\n",
       "  (16131, 2.0049257940607146),\n",
       "  (15035, 1.9953069212187962),\n",
       "  (76172, 1.9849445504750194),\n",
       "  (15685, 1.9249211186318642),\n",
       "  (69611, 1.9211717759888352),\n",
       "  (14063, 1.9202658916480286)],\n",
       " [(90422, 3.594000672475334),\n",
       "  (90513, 3.5740357963658402),\n",
       "  (90418, 3.3413940724211106),\n",
       "  (22750, 3.1982621989593407),\n",
       "  (23585, 3.1731473670929806),\n",
       "  (25316, 3.0891385958628033),\n",
       "  (43963, 3.0851984564894095),\n",
       "  (90379, 3.0234798827018734),\n",
       "  (90364, 3.0133834214074944),\n",
       "  (90355, 2.9494560895494937),\n",
       "  (29658, 2.9333237027921237),\n",
       "  (90368, 2.918577156388048),\n",
       "  (90336, 2.9086960503088046),\n",
       "  (90414, 2.9076399747285437),\n",
       "  (90335, 2.8841487529314533),\n",
       "  (90430, 2.8491008258050274),\n",
       "  (29752, 2.838188000345437),\n",
       "  (68660, 2.7719818658898117),\n",
       "  (90337, 2.7158772188543816),\n",
       "  (42530, 2.6569952404337536),\n",
       "  (29801, 2.653706763159191),\n",
       "  (29850, 2.6505148871173283),\n",
       "  (29853, 2.6494902209098816),\n",
       "  (1297, 2.6421215387543153),\n",
       "  (90543, 2.5948400361794506),\n",
       "  (22224, 2.5948028483964247),\n",
       "  (82539, 2.555724563367982),\n",
       "  (29669, 2.555200168757334),\n",
       "  (83415, 2.5544490060327973),\n",
       "  (25600, 2.5410341922366158)],\n",
       " [(47857, 22.304912438140683),\n",
       "  (77207, 22.112332033205636),\n",
       "  (41101, 22.074340077431934),\n",
       "  (52074, 22.013567060478373),\n",
       "  (414, 21.9254715588129),\n",
       "  (62673, 21.900072379916736),\n",
       "  (24494, 21.867874247300776),\n",
       "  (58755, 21.854748212353222),\n",
       "  (56358, 21.854201023060458),\n",
       "  (12491, 21.83011807669926),\n",
       "  (47705, 21.828556983384125),\n",
       "  (56035, 21.827788399855),\n",
       "  (43155, 21.82179569048312),\n",
       "  (35536, 21.821333809618878),\n",
       "  (45733, 21.81010871552411),\n",
       "  (29466, 21.79997973609456),\n",
       "  (17985, 21.788119738793295),\n",
       "  (18832, 21.784463018148205),\n",
       "  (90017, 21.783325872500118),\n",
       "  (18807, 21.780948824998447),\n",
       "  (73278, 21.778689091186816),\n",
       "  (77193, 21.77203430413446),\n",
       "  (17681, 21.76798793795851),\n",
       "  (34348, 21.761460998802782),\n",
       "  (17443, 21.758661193214614),\n",
       "  (57504, 21.736929126390688),\n",
       "  (54107, 21.72429874552343),\n",
       "  (11491, 21.722520183602295),\n",
       "  (54611, 21.711469217151297),\n",
       "  (30981, 21.707223447415377)],\n",
       " [(44359, 101.24009165242707),\n",
       "  (45910, 101.13800950632725),\n",
       "  (95348, 101.04150415951509),\n",
       "  (86067, 101.03231958718956),\n",
       "  (45582, 101.0310172388083),\n",
       "  (95049, 101.03058036700902),\n",
       "  (71231, 101.02085494858501),\n",
       "  (46025, 101.01594890757399),\n",
       "  (63825, 100.88634377802926),\n",
       "  (7972, 100.84530731900375),\n",
       "  (64521, 100.8019479049987),\n",
       "  (12491, 2.2963038072081017),\n",
       "  (29466, 2.2136149932449216),\n",
       "  (47857, 2.1786317194168223),\n",
       "  (71192, 2.101033949534997),\n",
       "  (59321, 2.0390107701540074),\n",
       "  (56225, 1.9503323632496916),\n",
       "  (414, 1.9254715588129003),\n",
       "  (28582, 1.903800054004737),\n",
       "  (46596, 1.8980716517227894),\n",
       "  (48294, 1.8747050286854967),\n",
       "  (58755, 1.8547482123532206),\n",
       "  (24494, 1.8493913170942595),\n",
       "  (82202, 1.7872663782097251),\n",
       "  (53266, 1.7178699530141308),\n",
       "  (56051, 1.692299744356651),\n",
       "  (44951, 1.6684785798285156),\n",
       "  (53912, 1.6641291778608762),\n",
       "  (76707, 1.6583968354303837),\n",
       "  (43742, 1.6488728209724612)],\n",
       " [(87, 121.25230309468304),\n",
       "  (13140, 100.81088651002477),\n",
       "  (21501, 100.7333777377209),\n",
       "  (80453, 22.086808447760706),\n",
       "  (7684, 21.9992625711754),\n",
       "  (27134, 21.879536917171812),\n",
       "  (78220, 21.856741531393705),\n",
       "  (58755, 21.854748212353222),\n",
       "  (87098, 21.82605534516975),\n",
       "  (46325, 21.7530499614429),\n",
       "  (41932, 21.708941203835195),\n",
       "  (86665, 21.686719921285093),\n",
       "  (46702, 21.673656684529202),\n",
       "  (71431, 21.65400871943504),\n",
       "  (37354, 21.651807622401883),\n",
       "  (19404, 21.645764002810562),\n",
       "  (35324, 21.639363049664162),\n",
       "  (63870, 21.635618460567144),\n",
       "  (47231, 21.591182629915707),\n",
       "  (7621, 21.560997265526538),\n",
       "  (39454, 21.560736161897122),\n",
       "  (79173, 21.54893575477953),\n",
       "  (39460, 21.50435421867842),\n",
       "  (37809, 21.494548887306973),\n",
       "  (18120, 21.48641178747912),\n",
       "  (569, 21.48604361617615),\n",
       "  (16232, 21.477818382223926),\n",
       "  (10498, 21.466948682854785),\n",
       "  (8477, 21.4659272613967),\n",
       "  (24633, 21.463554687818636)]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59e320-51bf-43af-9e7c-f633143e70c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35b7556-c957-4cc4-9e7d-31681348202f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d30251-39fa-4b3e-afd2-679456197dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c1b167-2859-4d5a-ae90-bce795d26429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e6875-01c3-4303-8117-762e2f16f1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578591f8-dd93-48eb-88b7-5e196105f770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "030315e3-648d-4693-8c04-bdd150ce3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "\n",
    "import main\n",
    "reload(main)\n",
    "from main import IR_Ret_Sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a3ff89-6c66-4135-a5ed-f610928b9ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Index\n",
      "Index loaded from all_papers_index\n",
      "Loading Title Index\n",
      "Index loaded from all_papers_title_index\n",
      "Setting up Llama Pipeline.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2c18a9674b475081631fd23d83fc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization Complete.\n"
     ]
    }
   ],
   "source": [
    "from main import IR_Ret_Sys\n",
    "IR = IR_Ret_Sys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cce286e-4fd9-4017-bce7-dbb4b881cbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96636/96636 [00:01<00:00, 89034.07it/s]\n"
     ]
    }
   ],
   "source": [
    "docid_score = IR.TFIDFRanker.query('Show me papers by Ashish Vaswani on Machine Learning before 2020')\n",
    "results = [\n",
    "    {\n",
    "    'title': IR.docid_title_map.get(i[0]),\n",
    "    'link' : IR.docid_link_map.get(i[0]),\n",
    "    'snippet': IR.docid_abstract_map.get(i[0])\n",
    "    }\n",
    "    for i in docid_score[:100]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec3ae323-1147-4146-b01c-5cf59e5106da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96636/96636 [00:01<00:00, 88260.10it/s]\n",
      "100%|██████████| 23670/23670 [00:00<00:00, 121522.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match!\n",
      "match!\n",
      "match!\n"
     ]
    }
   ],
   "source": [
    "docid_score = IR.TFIDFRanker.query_augmented('Show me papers by Ashish Vaswani on Machine Learning before 2020')\n",
    "results = [\n",
    "    {\n",
    "    'title': IR.docid_title_map.get(i[0]),\n",
    "    'link' : IR.docid_link_map.get(i[0]),\n",
    "    'snippet': IR.docid_abstract_map.get(i[0])\n",
    "    }\n",
    "    for i in docid_score[:100]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00c5df3b-1206-4e1f-9dfc-118231c84c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA QUERY\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42526ca91c034b908b764598aa9b41d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_dict: \t  {'author': ['Ashish Vaswani'], 'year': 'until 2019', 'keywords': ['machine', 'learning']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96636/96636 [00:01<00:00, 88443.47it/s]\n",
      "100%|██████████| 23670/23670 [00:00<00:00, 122050.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "authors_in_query: \t ['ashish vaswani']\n",
      "relevant_years: \t [1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
      "match!\n",
      "match!\n",
      "match!\n"
     ]
    }
   ],
   "source": [
    "docid_score = IR.TFIDFRanker.query_llama(text_gen_pipeline=IR.text_gen_pipeline, \n",
    "                                         query='Show me papers by Ashish Vaswani on Machine Learning before 2020')\n",
    "results = [\n",
    "    {\n",
    "    'title': IR.docid_title_map.get(i[0]),\n",
    "    'link' : IR.docid_link_map.get(i[0]),\n",
    "    'snippet': IR.docid_abstract_map.get(i[0])\n",
    "    }\n",
    "    for i in docid_score[:100]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1069eb73-cec1-454c-9397-ce859677d559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c2a8a5-4572-44b8-9f55-48ec1e5b6045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b1f8f6-f052-4b92-82db-ad22d325495f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(279, 23.480281645675486),\n",
       " (93599, 23.474080091697076),\n",
       " (70, 23.42833686948632),\n",
       " (72, 23.35001358139103),\n",
       " (1048, 23.309322188168373),\n",
       " (530, 23.276161844372716),\n",
       " (108, 23.25085107288236),\n",
       " (9632, 23.221275037704043),\n",
       " (7624, 23.164203391071013),\n",
       " (265, 23.1493951399747),\n",
       " (373, 23.147874390200545),\n",
       " (196, 23.145867572115918),\n",
       " (251, 23.10318257268457),\n",
       " (70944, 23.10105273747341),\n",
       " (432, 23.097123759460064),\n",
       " (90216, 23.088221615338576),\n",
       " (1095, 23.084631404967244),\n",
       " (11383, 23.08114225249954),\n",
       " (598, 23.079643403870303),\n",
       " (1071, 23.077019103744046),\n",
       " (12, 23.06839845015798),\n",
       " (93721, 23.06601719354665),\n",
       " (96, 23.044412731061502),\n",
       " (1119, 23.035020944233516),\n",
       " (1150, 23.02720526812874),\n",
       " (12291, 23.02634746609553),\n",
       " (469, 23.017074226927853),\n",
       " (94111, 23.01525432610678),\n",
       " (737, 23.012321148997998),\n",
       " (69797, 23.011472985318793),\n",
       " (93065, 23.003806680873506),\n",
       " (10643, 22.98452164393269),\n",
       " (862, 22.98069719212997),\n",
       " (27, 22.97954741724126),\n",
       " (43263, 22.978955200351056),\n",
       " (1093, 22.96819027049804),\n",
       " (90124, 22.95861457179726),\n",
       " (543, 22.95500774000331),\n",
       " (280, 22.947941851153317),\n",
       " (34, 22.92217817582111),\n",
       " (3345, 22.921840717211726),\n",
       " (234, 22.913281405533372),\n",
       " (805, 22.909033130615754),\n",
       " (249, 22.90470343621992),\n",
       " (9601, 22.900948962642634),\n",
       " (157, 22.89884621424443),\n",
       " (62, 22.897444541043242),\n",
       " (93, 22.896078608703686),\n",
       " (707, 22.892696211656055),\n",
       " (95519, 22.886214709583502),\n",
       " (580, 22.884223971157972),\n",
       " (672, 22.880578156958148),\n",
       " (391, 22.879787792161686),\n",
       " (240, 22.87561589605221),\n",
       " (297, 22.873606204758083),\n",
       " (93597, 22.871870936445625),\n",
       " (1903, 22.868910332217034),\n",
       " (93064, 22.86667976279388),\n",
       " (82875, 22.862931731367233),\n",
       " (235, 22.86150482886842),\n",
       " (374, 22.860943698460108),\n",
       " (93062, 22.83780683503647),\n",
       " (871, 22.836428827184154),\n",
       " (532, 22.829486581253096),\n",
       " (303, 22.82924461684327),\n",
       " (10732, 22.82599698024851),\n",
       " (8720, 22.821851563929044),\n",
       " (1094, 22.821768908602834),\n",
       " (66707, 22.819676800970967),\n",
       " (19435, 22.815329322522228),\n",
       " (221, 22.810959716133322),\n",
       " (91257, 22.807251309723732),\n",
       " (90617, 22.80308381005677),\n",
       " (744, 22.80056356563734),\n",
       " (91823, 22.798217065505945),\n",
       " (45764, 22.79370807319645),\n",
       " (42, 22.790760972483568),\n",
       " (555, 22.787483802328648),\n",
       " (94349, 22.78269098630394),\n",
       " (97428, 22.780751598703475),\n",
       " (5040, 22.77037695899487),\n",
       " (527, 22.762264306817883),\n",
       " (45907, 22.76198660999994),\n",
       " (69, 22.75566004758275),\n",
       " (360, 22.75551226394818),\n",
       " (90963, 22.75211513706462),\n",
       " (4159, 22.74811909789456),\n",
       " (533, 22.74540254146511),\n",
       " (216, 22.74316021049299),\n",
       " (93574, 22.742370130342252),\n",
       " (285, 22.74077904801554),\n",
       " (841, 22.736115042696788),\n",
       " (83048, 22.734691897812677),\n",
       " (837, 22.73005190814558),\n",
       " (713, 22.729372934286925),\n",
       " (3, 22.727675103690707),\n",
       " (401, 22.725810705422337),\n",
       " (40047, 22.71812821285304),\n",
       " (4842, 22.71753694355807),\n",
       " (2, 22.712859252402538),\n",
       " (725, 22.712406954718357),\n",
       " (90837, 22.710069012402606),\n",
       " (43255, 22.70673364183394),\n",
       " (43246, 22.705199237452007),\n",
       " (529, 22.701226648686983),\n",
       " (8656, 22.700707288811206),\n",
       " (233, 22.699580163661185),\n",
       " (43202, 22.695093257908553),\n",
       " (13238, 22.691554836740075),\n",
       " (43990, 22.686897232762924),\n",
       " (515, 22.68508909857067),\n",
       " (305, 22.684447770241164),\n",
       " (75813, 22.683896989497534),\n",
       " (70960, 22.681226308939088),\n",
       " (723, 22.681175974628243),\n",
       " (1047, 22.68086207744638),\n",
       " (43251, 22.67679661130672),\n",
       " (75957, 22.668223548473595),\n",
       " (74944, 22.665935114304588),\n",
       " (579, 22.665308930933588),\n",
       " (109, 22.663225254366736),\n",
       " (843, 22.662723636846692),\n",
       " (15959, 22.662190154698003),\n",
       " (361, 22.65997920924231),\n",
       " (70932, 22.659089129004997),\n",
       " (358, 22.658107296935835),\n",
       " (76722, 22.657262938275153),\n",
       " (43752, 22.656011072373662),\n",
       " (44437, 22.655706195830888),\n",
       " (43357, 22.65266600940742),\n",
       " (73101, 22.64846886250465),\n",
       " (8642, 22.64736290822939),\n",
       " (68779, 22.64335985701658),\n",
       " (1151, 22.64056736707192),\n",
       " (140, 22.64000179935971),\n",
       " (32819, 22.639671354869595),\n",
       " (64689, 22.632044367436862),\n",
       " (94931, 22.62855160592027),\n",
       " (789, 22.627785623117006),\n",
       " (376, 22.62770728350344),\n",
       " (75670, 22.624983725523425),\n",
       " (83205, 22.622544242457497),\n",
       " (1084, 22.62197013635289),\n",
       " (9820, 22.61975919089719),\n",
       " (96482, 22.61871605477716),\n",
       " (344, 22.615523664667165),\n",
       " (365, 22.61240278954428),\n",
       " (75116, 22.612314104668865),\n",
       " (44237, 22.611338219810033),\n",
       " (86078, 22.609154008281195),\n",
       " (74943, 22.60897082746249),\n",
       " (93081, 22.608927470371917),\n",
       " (82818, 22.608248844159533),\n",
       " (141, 22.60780475190922),\n",
       " (4906, 22.599031329013815),\n",
       " (726, 22.597596987641392),\n",
       " (1133, 22.593287653298837),\n",
       " (12640, 22.59051848968094),\n",
       " (43824, 22.588714745011252),\n",
       " (107, 22.585579735659348),\n",
       " (90234, 22.58465179306282),\n",
       " (720, 22.582018809140344),\n",
       " (371, 22.577594545770534),\n",
       " (273, 22.575602259408885),\n",
       " (70265, 22.574637305886995),\n",
       " (1110, 22.57372205560064),\n",
       " (93592, 22.570519221953695),\n",
       " (12174, 22.570158469724912),\n",
       " (61, 22.568807636975542),\n",
       " (26121, 22.56798254573539),\n",
       " (95031, 22.56756805475586),\n",
       " (90210, 22.561742776026858),\n",
       " (9372, 22.558552524758515),\n",
       " (890, 22.556354864975525),\n",
       " (218, 22.555852597233397),\n",
       " (451, 22.55499114232471),\n",
       " (8160, 22.554703456994037),\n",
       " (94897, 22.554625117380468),\n",
       " (75552, 22.546593223420658),\n",
       " (23163, 22.54476181670607),\n",
       " (86352, 22.544398221145848),\n",
       " (1174, 22.54351551245797),\n",
       " (66, 22.54134602863912),\n",
       " (806, 22.538063965497745),\n",
       " (67221, 22.53667247890504),\n",
       " (39746, 22.535888661339516),\n",
       " (24173, 22.53285038085615),\n",
       " (68641, 22.532811552502483),\n",
       " (5907, 22.53027178942646),\n",
       " (7712, 22.528424439530987),\n",
       " (95098, 22.525227943513134),\n",
       " (6, 22.52488084646266),\n",
       " (69676, 22.524794194125228),\n",
       " (75563, 22.52451482151842),\n",
       " (839, 22.52445630266234),\n",
       " (8637, 22.521232328055188),\n",
       " (809, 22.520205487175865),\n",
       " (70045, 22.519841586219037),\n",
       " (43715, 22.519412456749013),\n",
       " (68181, 22.519299991900283),\n",
       " (156, 22.51653870736324),\n",
       " (75087, 22.51578122380606),\n",
       " (808, 22.51550421023664),\n",
       " (172, 22.514436322760808),\n",
       " (1132, 22.513010382232807),\n",
       " (93089, 22.507378696118693),\n",
       " (363, 22.507040824092105),\n",
       " (45966, 22.506976988457215),\n",
       " (43540, 22.505503932041098),\n",
       " (43317, 22.505474605385796),\n",
       " (43981, 22.504812651609804),\n",
       " (68657, 22.504543600038193),\n",
       " (19934, 22.501753149394318),\n",
       " (43700, 22.499707632076717),\n",
       " (490, 22.498602250191855),\n",
       " (76299, 22.49703534816925),\n",
       " (40016, 22.494056759162866),\n",
       " (75333, 22.49378126659352),\n",
       " (3638, 22.493619560102296),\n",
       " (91829, 22.484574175780114),\n",
       " (69799, 22.484267508042677),\n",
       " (67346, 22.483309475012554),\n",
       " (43061, 22.48224780243829),\n",
       " (142, 22.482243206568427),\n",
       " (392, 22.480851719975725),\n",
       " (83770, 22.48028918689048),\n",
       " (41513, 22.47932654406181),\n",
       " (1441, 22.47922366212935),\n",
       " (5878, 22.47541998697542),\n",
       " (10361, 22.47506628488005),\n",
       " (75534, 22.468258636485757),\n",
       " (37534, 22.465583701497298),\n",
       " (1036, 22.463509264240244),\n",
       " (91001, 22.462806495216547),\n",
       " (98276, 22.4612635790092),\n",
       " (69056, 22.460004190918845),\n",
       " (72915, 22.455124366396543),\n",
       " (45322, 22.455005021212724),\n",
       " (4514, 22.453916308421213),\n",
       " (569, 22.451781964104473),\n",
       " (75362, 22.45032870107436),\n",
       " (9085, 22.448943890663763),\n",
       " (66891, 22.447247983015522),\n",
       " (7572, 22.44406641835079),\n",
       " (146, 22.44396905341822),\n",
       " (73422, 22.442356526269975),\n",
       " (90995, 22.440926316911792),\n",
       " (6586, 22.440317323316208),\n",
       " (44, 22.440069168545367),\n",
       " (76767, 22.43995657108742),\n",
       " (143, 22.43919486162673),\n",
       " (94160, 22.438738423209195),\n",
       " (43054, 22.438515696023842),\n",
       " (2424, 22.437545972250952),\n",
       " (93152, 22.43731699053623),\n",
       " (94711, 22.4364541576832),\n",
       " (3590, 22.434516738031604),\n",
       " (44844, 22.434296529995724),\n",
       " (73082, 22.430823921577222),\n",
       " (81899, 22.429029419541873),\n",
       " (9227, 22.427362174778732),\n",
       " (15023, 22.427339816343228),\n",
       " (2330, 22.42431720353133),\n",
       " (83299, 22.419729437890577),\n",
       " (719, 22.41372189908652),\n",
       " (7777, 22.412354888515072),\n",
       " (92993, 22.41214366605652),\n",
       " (40427, 22.411956982770498),\n",
       " (39501, 22.410776301745802),\n",
       " (82953, 22.410484190922432),\n",
       " (158, 22.403109641546926),\n",
       " (43455, 22.40290814877565),\n",
       " (4837, 22.401144289939676),\n",
       " (80665, 22.40073896360776),\n",
       " (372, 22.39984915020025),\n",
       " (67256, 22.39749721300445),\n",
       " (7002, 22.39616863199296),\n",
       " (93200, 22.395391968896668),\n",
       " (73681, 22.394477668565372),\n",
       " (41742, 22.393364414232256),\n",
       " (16036, 22.39334243215436),\n",
       " (44279, 22.392432559284373),\n",
       " (46022, 22.38616408612378),\n",
       " (135, 22.385646741581997),\n",
       " (44868, 22.381879746427852),\n",
       " (7592, 22.37963570624191),\n",
       " (8508, 22.379007951568397),\n",
       " (75382, 22.3775524511895),\n",
       " (79491, 22.373123947609034),\n",
       " (44067, 22.370232684283813),\n",
       " (67276, 22.36988961452991),\n",
       " (83754, 22.369818364299046),\n",
       " (93539, 22.36958102788444),\n",
       " (42847, 22.368460598065457),\n",
       " (63, 22.367250058672752),\n",
       " (6883, 22.36698030261182),\n",
       " (10205, 22.365438204955943),\n",
       " (98399, 22.36149362331755),\n",
       " (896, 22.360291043995847),\n",
       " (91927, 22.35649905263314),\n",
       " (319, 22.356322907127204),\n",
       " (119, 22.355916931792084),\n",
       " (35857, 22.355441347538108),\n",
       " (9191, 22.352910933078114),\n",
       " (91441, 22.349770744467627),\n",
       " (1795, 22.349144561096626),\n",
       " (62562, 22.348606714494892),\n",
       " (788, 22.347996528139582),\n",
       " (25657, 22.341489810093687),\n",
       " (400, 22.34034912551234),\n",
       " (893, 22.340288161084874),\n",
       " (145, 22.338125157874636),\n",
       " (40077, 22.33618074360532),\n",
       " (95171, 22.33470936980768),\n",
       " (478, 22.332539519165245),\n",
       " (90227, 22.331366478868773),\n",
       " (92103, 22.32721098920652),\n",
       " (70253, 22.324228402900033),\n",
       " (37653, 22.32374527531352),\n",
       " (68972, 22.320282248109287),\n",
       " (487, 22.319884342364713),\n",
       " (30087, 22.31930338210005),\n",
       " (38433, 22.317728405823768),\n",
       " (4116, 22.313355530465707),\n",
       " (74825, 22.312564575459024),\n",
       " (91801, 22.311973623321137),\n",
       " (4844, 22.31057281190176),\n",
       " (13267, 22.310426726287385),\n",
       " (13063, 22.310387846958598),\n",
       " (6306, 22.30801425306182),\n",
       " (91592, 22.30767077838449),\n",
       " (452, 22.304470285066532),\n",
       " (684, 22.304290481716635),\n",
       " (6778, 22.304094631358232),\n",
       " (8690, 22.303638242066828),\n",
       " (9194, 22.30133356727265),\n",
       " (93610, 22.296881401295373),\n",
       " (96484, 22.2959607252602),\n",
       " (8695, 22.29570188115854),\n",
       " (93164, 22.294794649706937),\n",
       " (8401, 22.294156991728755),\n",
       " (139, 22.29193550917705),\n",
       " (6293, 22.291416892268906),\n",
       " (4153, 22.289827475484124),\n",
       " (269, 22.28878711090288),\n",
       " (6843, 22.288497716444628),\n",
       " (5172, 22.286439319078493),\n",
       " (90237, 22.2856778177737),\n",
       " (6034, 22.282700747560284),\n",
       " (22648, 22.28027755109655),\n",
       " (67325, 22.27580417542412),\n",
       " (96325, 22.27458672629506),\n",
       " (91704, 22.27322339858902),\n",
       " (91143, 22.273082446153488),\n",
       " (94308, 22.271170938228106),\n",
       " (73219, 22.27034185034897),\n",
       " (15381, 22.268962457373437),\n",
       " (56, 22.26810654810413),\n",
       " (65363, 22.267450760039377),\n",
       " (946, 22.267211567968786),\n",
       " (5304, 22.265623995622185),\n",
       " (8707, 22.262624350714336),\n",
       " (7730, 22.262003487033457),\n",
       " (359, 22.260787930297223),\n",
       " (74917, 22.2589870394938),\n",
       " (4460, 22.258919413426213),\n",
       " (6073, 22.25767590301594),\n",
       " (5568, 22.25412882308355),\n",
       " (92448, 22.251767585875726),\n",
       " (9737, 22.251333075999174),\n",
       " (91253, 22.2474305019423),\n",
       " (32916, 22.245513767675952),\n",
       " (70751, 22.245297896100173),\n",
       " (25534, 22.2446462397008),\n",
       " (9060, 22.243022804715523),\n",
       " (11390, 22.239784312308018),\n",
       " (93745, 22.239634866688448),\n",
       " (181, 22.235356156663975),\n",
       " (91440, 22.234588612261522),\n",
       " (9740, 22.233910788312773),\n",
       " (1152, 22.233003380243908),\n",
       " (71, 22.232785507261156),\n",
       " (804, 22.231388118943563),\n",
       " (8504, 22.23117897854142),\n",
       " (43607, 22.229697155515975),\n",
       " (362, 22.22961472193652),\n",
       " (8704, 22.227639971142796),\n",
       " (4522, 22.226778709704696),\n",
       " (20555, 22.225655765417983),\n",
       " (13392, 22.22554298417588),\n",
       " (69583, 22.223881668751854),\n",
       " (3568, 22.22313200997896),\n",
       " (44268, 22.222377388610283),\n",
       " (4234, 22.22197091017974),\n",
       " (11118, 22.220496237866882),\n",
       " (18922, 22.21910236969662),\n",
       " (8413, 22.2174604192273),\n",
       " (79560, 22.217320206080252),\n",
       " (96001, 22.217261687224173),\n",
       " (786, 22.215414337328703),\n",
       " (6047, 22.211406439635713),\n",
       " (9213, 22.209949434479118),\n",
       " (12226, 22.20864949685998),\n",
       " (70504, 22.207320046936207),\n",
       " (4065, 22.207275998082377),\n",
       " (4095, 22.206461493922205),\n",
       " (18048, 22.204426221355682),\n",
       " (61062, 22.20371737020499),\n",
       " (4133, 22.20360717305533),\n",
       " (8514, 22.20250434825079),\n",
       " (9830, 22.201541921731916),\n",
       " (34403, 22.200141232466052),\n",
       " (5224, 22.199788526950734),\n",
       " (13325, 22.1996512262997),\n",
       " (9157, 22.19765491862285),\n",
       " (67110, 22.19367886852332),\n",
       " (4310, 22.188045093357275),\n",
       " (94270, 22.188020252771466),\n",
       " (976, 22.185912343623205),\n",
       " (9056, 22.182832562527604),\n",
       " (41748, 22.181150758183236),\n",
       " (4630, 22.177810684340116),\n",
       " (13287, 22.17778707207927),\n",
       " (4077, 22.175891194750125),\n",
       " (8919, 22.17530338418511),\n",
       " (5306, 22.173469947595084),\n",
       " (11009, 22.172919119520298),\n",
       " (8503, 22.17272166254872),\n",
       " (63073, 22.172426952435757),\n",
       " (5687, 22.17218782432009),\n",
       " (97121, 22.170300521909052),\n",
       " (92659, 22.167871669441883),\n",
       " (7228, 22.167447500568525),\n",
       " (4287, 22.16691476028017),\n",
       " (6449, 22.166554805886758),\n",
       " (13288, 22.166341151649736),\n",
       " (52802, 22.165608998200785),\n",
       " (11644, 22.16490067815102),\n",
       " (7722, 22.16467243996003),\n",
       " (1352, 22.162351301523326),\n",
       " (138, 22.161836801363474),\n",
       " (43151, 22.161625990688535),\n",
       " (335, 22.161179678443435),\n",
       " (43186, 22.160521670958474),\n",
       " (9159, 22.160066748398627),\n",
       " (45529, 22.15805401673582),\n",
       " (5837, 22.15777497714603),\n",
       " (92511, 22.156614989393002),\n",
       " (83173, 22.154288510831933),\n",
       " (5131, 22.152275024486652),\n",
       " (1083, 22.151545190585292),\n",
       " (20081, 22.14814765796985),\n",
       " (90923, 22.147523174347498),\n",
       " (94341, 22.146642125379586),\n",
       " (8428, 22.14538876209835),\n",
       " (5511, 22.145005808630525),\n",
       " (9070, 22.14377624775348),\n",
       " (1021, 22.14330235618205),\n",
       " (5331, 22.14290720977109),\n",
       " (75277, 22.13727478553552),\n",
       " (11500, 22.137081256333957),\n",
       " (93627, 22.136087796778064),\n",
       " (45901, 22.135955223759453),\n",
       " (8518, 22.135500436598264),\n",
       " (4171, 22.135176304928557),\n",
       " (10052, 22.132203658319607),\n",
       " (96329, 22.131964325884958),\n",
       " (12287, 22.130388239263997),\n",
       " (83218, 22.130272310013943),\n",
       " (47443, 22.129322634466874),\n",
       " (4521, 22.128543950117514),\n",
       " (13241, 22.12633235576317),\n",
       " (90602, 22.12537717871609),\n",
       " (43471, 22.12342105510597),\n",
       " (76603, 22.12257405337223),\n",
       " (4537, 22.12065738777116),\n",
       " (93186, 22.11950857769546),\n",
       " (82802, 22.11939746327414),\n",
       " (9215, 22.1192342841747),\n",
       " (5904, 22.117109150522644),\n",
       " (11405, 22.11639497104789),\n",
       " (8692, 22.11575208125784),\n",
       " (12442, 22.11478883236077),\n",
       " (4139, 22.114446166899185),\n",
       " (4080, 22.113465030232376),\n",
       " (36727, 22.11336342766669),\n",
       " (8934, 22.1125282259518),\n",
       " (95371, 22.109378596108602),\n",
       " (5285, 22.10843357219625),\n",
       " (9051, 22.1082284151144),\n",
       " (92708, 22.108138862083287),\n",
       " (95530, 22.107580308268254),\n",
       " (8458, 22.106609511849562),\n",
       " (5083, 22.105435992648644),\n",
       " (5225, 22.104859090584103),\n",
       " (44092, 22.10473114360688),\n",
       " (10670, 22.103569031136153),\n",
       " (43086, 22.10267661055491),\n",
       " (37160, 22.102467271331626),\n",
       " (14946, 22.10060499008303),\n",
       " (8031, 22.09981344921075),\n",
       " (299, 22.09891565684276),\n",
       " (8138, 22.096651784567506),\n",
       " (8680, 22.09599648776946),\n",
       " (6063, 22.09279347010638),\n",
       " (4734, 22.091878489943994),\n",
       " (4281, 22.091797026514932),\n",
       " (23233, 22.090830133059594),\n",
       " (4472, 22.09018654920073),\n",
       " (10327, 22.09017847666022),\n",
       " (90498, 22.08820532281662),\n",
       " (4976, 22.088130486782312),\n",
       " (8521, 22.085753945412247),\n",
       " (8462, 22.084351103967446),\n",
       " (40, 22.08199404363198),\n",
       " (4278, 22.08188148740735),\n",
       " (94399, 22.080758750837344),\n",
       " (92707, 22.080205869837688),\n",
       " (5569, 22.080120849220943),\n",
       " (43053, 22.079672845061665),\n",
       " (14918, 22.079375446020734),\n",
       " (5686, 22.077880497715622),\n",
       " (5345, 22.076920355902985),\n",
       " (339, 22.074847584835407),\n",
       " (9055, 22.07382785085819),\n",
       " (19383, 22.072228854807598),\n",
       " (30289, 22.07099966194462),\n",
       " (11212, 22.068233757239412),\n",
       " (8700, 22.0678285164655),\n",
       " (87564, 22.065637435115374),\n",
       " (4730, 22.063986532726844),\n",
       " (655, 22.063789636789654),\n",
       " (25220, 22.06235667965709),\n",
       " (97417, 22.061029158414975),\n",
       " (8678, 22.060762977461),\n",
       " (91564, 22.05819588654854),\n",
       " (75091, 22.05684498283468),\n",
       " (8917, 22.05663682393715),\n",
       " (4553, 22.055312388255654),\n",
       " (375, 22.055307374013378),\n",
       " (12227, 22.055187885304328),\n",
       " (90212, 22.054834048586535),\n",
       " (4417, 22.054284364377267),\n",
       " (75230, 22.053940376713193),\n",
       " (5190, 22.053383296238245),\n",
       " (44228, 22.05306399036713),\n",
       " (43645, 22.051746456662855),\n",
       " (5403, 22.051348003754057),\n",
       " (23161, 22.051327966422075),\n",
       " (4377, 22.05052138032547),\n",
       " (93628, 22.045677069850477),\n",
       " (68163, 22.04489240829857),\n",
       " (8699, 22.043321571534243),\n",
       " (6580, 22.042352645222547),\n",
       " (666, 22.042213584285463),\n",
       " (1170, 22.04135783603242),\n",
       " (4304, 22.039891685479173),\n",
       " (7351, 22.039331208001688),\n",
       " (463, 22.03833392714006),\n",
       " (208, 22.038050120213356),\n",
       " (8517, 22.035407536527334),\n",
       " (6597, 22.03540727987485),\n",
       " (43147, 22.034794421722967),\n",
       " (12475, 22.03450621173583),\n",
       " (95332, 22.033826592514863),\n",
       " (90269, 22.033558569452044),\n",
       " (20357, 22.03339218576148),\n",
       " (63422, 22.033154510074176),\n",
       " (8914, 22.03284018896239),\n",
       " (9086, 22.032518440375938),\n",
       " (13237, 22.03050756689813),\n",
       " (13740, 22.029385105208654),\n",
       " (10485, 22.029193887366162),\n",
       " (46026, 22.028229913645923),\n",
       " (35105, 22.02789103771826),\n",
       " (67248, 22.027522823960062),\n",
       " (45896, 22.027134207401772),\n",
       " (4672, 22.026835147693856),\n",
       " (8502, 22.026374219100802),\n",
       " (43358, 22.024606629753094),\n",
       " (4079, 22.0243133224839),\n",
       " (91556, 22.023613928838323),\n",
       " (18883, 22.02273836246059),\n",
       " (92579, 22.022422914933028),\n",
       " (13247, 22.021651677497147),\n",
       " (90804, 22.021203055227037),\n",
       " (68683, 22.020964824939067),\n",
       " (259, 22.018694248693077),\n",
       " (92994, 22.017770928816173),\n",
       " (6244, 22.015620973853345),\n",
       " (46257, 22.01523191226888),\n",
       " (95107, 22.015067455646772),\n",
       " (83227, 22.015018928213102),\n",
       " (4468, 22.013522137340132),\n",
       " (75105, 22.012880014377654),\n",
       " (5776, 22.012399956404913),\n",
       " (8412, 22.011714573161033),\n",
       " (7138, 22.011339207867497),\n",
       " (45904, 22.011293020126686),\n",
       " (60732, 22.01096850645227),\n",
       " (67211, 22.008723589521367),\n",
       " (91582, 22.008009287261306),\n",
       " (94767, 22.00798211005261),\n",
       " (129, 22.007580905476555),\n",
       " (87188, 22.004889438276813),\n",
       " (5803, 22.004513690116898),\n",
       " (3025, 22.00436920212263),\n",
       " (94479, 22.003968533324716),\n",
       " (48006, 22.001644063957976),\n",
       " (97463, 22.001401823152214),\n",
       " (62469, 22.001273519764307),\n",
       " (4112, 22.00006600974154),\n",
       " (4549, 21.997368388610113),\n",
       " (47652, 21.997282033193294),\n",
       " (2302, 21.996719853367992),\n",
       " (90271, 21.994660676182207),\n",
       " (43158, 21.99451554689864),\n",
       " (8639, 21.992453334891596),\n",
       " (84048, 21.991968327290863),\n",
       " (664, 21.98986594268843),\n",
       " (7209, 21.989041203450878),\n",
       " (5318, 21.988422086947118),\n",
       " (23287, 21.987565534772283),\n",
       " (83230, 21.986786388447353),\n",
       " (84042, 21.986116616879393),\n",
       " (97389, 21.98509546432434),\n",
       " (22193, 21.9847877705791),\n",
       " (45011, 21.984655114349543),\n",
       " (6065, 21.98269177895625),\n",
       " (4222, 21.9826019595937),\n",
       " (92575, 21.97979635542324),\n",
       " (94474, 21.978777225635742),\n",
       " (43929, 21.977145502251652),\n",
       " (13249, 21.975890460627255),\n",
       " (12793, 21.97520629674866),\n",
       " (67268, 21.9749359170249),\n",
       " (4486, 21.973798552275426),\n",
       " (70682, 21.972657113396526),\n",
       " (92341, 21.972507812243002),\n",
       " (97120, 21.971868848295276),\n",
       " (12792, 21.97139690962628),\n",
       " (568, 21.97099900388171),\n",
       " (90921, 21.970021498399802),\n",
       " (92564, 21.969854715135273),\n",
       " (8522, 21.96953976776592),\n",
       " (4848, 21.968240321861362),\n",
       " (613, 21.968218398386217),\n",
       " (7691, 21.96694301771051),\n",
       " (43755, 21.966658296405587),\n",
       " (4473, 21.96651494593774),\n",
       " (92719, 21.96564321164098),\n",
       " (8718, 21.96499191402223),\n",
       " (19901, 21.964254360391397),\n",
       " (8519, 21.964053324960755),\n",
       " (9175, 21.96329671611428),\n",
       " (68156, 21.963070530638657),\n",
       " (90226, 21.962918778917842),\n",
       " (43542, 21.962751948315763),\n",
       " (956, 21.962498787985677),\n",
       " (31, 21.96234807563143),\n",
       " (4447, 21.961848792333726),\n",
       " (9220, 21.959861951100365),\n",
       " (94938, 21.958870203327344),\n",
       " (8507, 21.957297892089013),\n",
       " (44012, 21.955793955870295),\n",
       " (4055, 21.955524074854512),\n",
       " (94744, 21.95539570620287),\n",
       " (5746, 21.954727779643733),\n",
       " (74641, 21.954433990007875),\n",
       " (4606, 21.954113890141976),\n",
       " (9723, 21.95348846595113),\n",
       " (94941, 21.952939266668285),\n",
       " (74923, 21.952857481029522),\n",
       " (10902, 21.95171068344159),\n",
       " (4446, 21.95146015358701),\n",
       " (4462, 21.949660337070977),\n",
       " (6920, 21.94958021943875),\n",
       " (7713, 21.94947859220222),\n",
       " (11538, 21.94934074881006),\n",
       " (603, 21.948375376827496),\n",
       " (92447, 21.948354294494496),\n",
       " (85999, 21.948085177629025),\n",
       " (4213, 21.948047272666365),\n",
       " (76919, 21.945817401909153),\n",
       " (95318, 21.945802022609847),\n",
       " (97388, 21.945612082570108),\n",
       " (8697, 21.94468642236422),\n",
       " (13248, 21.943476007913013),\n",
       " (10546, 21.943385238876726),\n",
       " (10168, 21.942979227568514),\n",
       " (9166, 21.942104042826713),\n",
       " (73003, 21.94141185710633),\n",
       " (640, 21.94075184955001),\n",
       " (54328, 21.940321740278335),\n",
       " (10584, 21.93905991991053),\n",
       " (6921, 21.93897705251386),\n",
       " (4552, 21.9388709356001),\n",
       " (44824, 21.93799303541072),\n",
       " (87408, 21.93793080709829),\n",
       " (77810, 21.936632575962282),\n",
       " (6296, 21.936319646446307),\n",
       " (872, 21.936094370231334),\n",
       " (6068, 21.934880546993572),\n",
       " (20871, 21.933979478854553),\n",
       " (4260, 21.9339482551291),\n",
       " (36575, 21.933938043699257),\n",
       " (611, 21.933242842263667),\n",
       " (83973, 21.932906594308125),\n",
       " (21321, 21.932798797829854),\n",
       " (6520, 21.931541288432538),\n",
       " (68310, 21.930949770584924),\n",
       " (8696, 21.93077898553659),\n",
       " (13630, 21.930723169744272),\n",
       " (8030, 21.9301665435319),\n",
       " (8789, 21.92832842363857),\n",
       " (10414, 21.926870934407415),\n",
       " (75109, 21.92629207978396),\n",
       " (94945, 21.925886548989393),\n",
       " (12237, 21.92564051608304),\n",
       " (4236, 21.925469919048222),\n",
       " (43034, 21.925452727498314),\n",
       " (4177, 21.92513213763098),\n",
       " (21236, 21.924907682980155),\n",
       " (41232, 21.924651438643963),\n",
       " (4305, 21.924554991151403),\n",
       " (9084, 21.924435744311157),\n",
       " (92386, 21.92395687292525),\n",
       " (66826, 21.923159271863096),\n",
       " (19666, 21.92292078231656),\n",
       " (571, 21.92285051229354),\n",
       " (93425, 21.922621443463733),\n",
       " (75081, 21.920948326407398),\n",
       " (4971, 21.92051385332454),\n",
       " (92105, 21.919544287565607),\n",
       " (75810, 21.9195197090885),\n",
       " (97424, 21.91949036338943),\n",
       " (97226, 21.919461853986157),\n",
       " (93561, 21.91884993752054),\n",
       " (9153, 21.9187557088911),\n",
       " (4478, 21.918650184982226),\n",
       " (76023, 21.918265080965448),\n",
       " (67109, 21.918143272206414),\n",
       " (75342, 21.918126080656506),\n",
       " (6384, 21.918122258115147),\n",
       " (62966, 21.917592342230005),\n",
       " (40786, 21.916980189538755),\n",
       " (19386, 21.916945972927692),\n",
       " (75770, 21.91679460770437),\n",
       " (41285, 21.916687053313687),\n",
       " (4286, 21.916652553817205),\n",
       " (91152, 21.916478601032203),\n",
       " (9600, 21.91619814130369),\n",
       " (4642, 21.916026947571908),\n",
       " (10900, 21.91542761435666),\n",
       " (84052, 21.913399138956418),\n",
       " (8505, 21.913299671331465),\n",
       " (4520, 21.913228853581316),\n",
       " (69449, 21.91303445075642),\n",
       " (5284, 21.912819819403847),\n",
       " (94062, 21.912696459328917),\n",
       " (697, 21.91216849587324),\n",
       " (83213, 21.912013298201366),\n",
       " (9594, 21.91179677309968),\n",
       " (5777, 21.911744181335894),\n",
       " (43992, 21.91129401028904),\n",
       " (9180, 21.910239042568747),\n",
       " (62294, 21.91007458594664),\n",
       " (9052, 21.90965742974746),\n",
       " (83970, 21.909320174588224),\n",
       " (6379, 21.90930912549678),\n",
       " (13268, 21.908636131121384),\n",
       " (4752, 21.908610187386614),\n",
       " (8635, 21.908592557542953),\n",
       " (8560, 21.90832344067748),\n",
       " (83748, 21.90830982017802),\n",
       " (81335, 21.907621109947534),\n",
       " (9367, 21.907583652192297),\n",
       " (44680, 21.906869400607626),\n",
       " (9140, 21.906798289217573),\n",
       " (93156, 21.906714189300267),\n",
       " (8708, 21.906003122515603),\n",
       " (4322, 21.905682387041985),\n",
       " (5582, 21.90543470244304),\n",
       " (43751, 21.90454769552865),\n",
       " (19029, 21.904064696357626),\n",
       " (4384, 21.9038298896765),\n",
       " (45194, 21.903701100797402),\n",
       " (91580, 21.90367145279384),\n",
       " (86059, 21.903374420102736),\n",
       " (4599, 21.90316522053161),\n",
       " (75229, 21.902819541208306),\n",
       " (2316, 21.90127055570982),\n",
       " (4427, 21.900963670664616),\n",
       " (6213, 21.90038069448581),\n",
       " (3611, 21.89933018219608),\n",
       " (44408, 21.899324112177606),\n",
       " (83035, 21.89921419504477),\n",
       " (5986, 21.899019167689488),\n",
       " (14301, 21.898869815410844),\n",
       " (8701, 21.898215338547757),\n",
       " (4970, 21.897885614033285),\n",
       " (4954, 21.897861021966417),\n",
       " (82749, 21.897488126749387),\n",
       " (11898, 21.896740661778296),\n",
       " (82353, 21.896383033908066),\n",
       " (44230, 21.89637776771022),\n",
       " (5632, 21.896200662829585),\n",
       " (10002, 21.896031962970135),\n",
       " (44586, 21.89597087638832),\n",
       " (29968, 21.895252759031692),\n",
       " (12235, 21.894738326518674),\n",
       " (12567, 21.894157366254007),\n",
       " (8712, 21.893729812505875),\n",
       " (11499, 21.89336906027709),\n",
       " (9276, 21.892595271741513),\n",
       " (91505, 21.892236311492038),\n",
       " (95448, 21.891692843373548),\n",
       " (9158, 21.8915291572229),\n",
       " (6011, 21.891355375087134),\n",
       " (714, 21.890792118207074),\n",
       " (44302, 21.89034437393463),\n",
       " (9162, 21.89034340416427),\n",
       " (11339, 21.890074287298802),\n",
       " (5637, 21.889629505068683),\n",
       " (75743, 21.889272554756804),\n",
       " (8407, 21.88869318997293),\n",
       " (76713, 21.888086178629543),\n",
       " (83211, 21.887945226194013),\n",
       " (4984, 21.887881068164102),\n",
       " (68126, 21.887606242309698),\n",
       " (92061, 21.886698818595995),\n",
       " (987, 21.886243672362884),\n",
       " (43699, 21.88526393287865),\n",
       " (264, 21.88452943812706),\n",
       " (82852, 21.884446145922027),\n",
       " (91515, 21.88439935925458),\n",
       " (9958, 21.884083825149837),\n",
       " (5680, 21.882632067836326),\n",
       " (13256, 21.882319704823693),\n",
       " (9210, 21.8823135400799),\n",
       " (6243, 21.881529809535127),\n",
       " (37994, 21.880850477919108),\n",
       " (90830, 21.88043752355361),\n",
       " (93550, 21.880406299828156),\n",
       " (97266, 21.879687864780717),\n",
       " (352, 21.879667528286532),\n",
       " (93099, 21.879590188204535),\n",
       " (44485, 21.878984381522816),\n",
       " (31012, 21.878519289388343),\n",
       " (19486, 21.87669948380945),\n",
       " (96434, 21.876467034968574),\n",
       " (69040, 21.87627207671376),\n",
       " (92053, 21.876258582687086),\n",
       " (37173, 21.8762150988838),\n",
       " (33779, 21.87620932855416),\n",
       " (13557, 21.875282999274685),\n",
       " (9593, 21.874176401472933),\n",
       " (66236, 21.87408155330317),\n",
       " (11717, 21.873876060882008),\n",
       " (86358, 21.873582735344804),\n",
       " (93899, 21.873581164943936),\n",
       " (86081, 21.872886244007965),\n",
       " (94834, 21.872820239264133),\n",
       " (91609, 21.871963890962782),\n",
       " (79420, 21.87160659657023),\n",
       " (97116, 21.871535346339368),\n",
       " (94816, 21.871167057180678),\n",
       " (83270, 21.87080930501599),\n",
       " (45911, 21.870804163112602),\n",
       " (77668, 21.870155226299016),\n",
       " (19956, 21.870019024223634),\n",
       " (10413, 21.869566726539453),\n",
       " (44588, 21.86940897637713),\n",
       " (10160, 21.8687736961126),\n",
       " (97893, 21.868050651347897),\n",
       " (43654, 21.867707501872882),\n",
       " (38406, 21.867698631060577),\n",
       " (9170, 21.867674295664084),\n",
       " (8562, 21.86766968342704),\n",
       " (45261, 21.86743362604856),\n",
       " (9771, 21.867119690817603),\n",
       " (8716, 21.866291190324997),\n",
       " (5010, 21.865994419006043),\n",
       " (44068, 21.86569249833117),\n",
       " (86361, 21.865197837501103),\n",
       " (97133, 21.865142463169647),\n",
       " (5269, 21.865093350867024),\n",
       " (5335, 21.864993818251868),\n",
       " (6394, 21.864795017907184),\n",
       " (1631, 21.863876372161148),\n",
       " (1000, 21.86363952465597),\n",
       " (45965, 21.863051487445357),\n",
       " (94259, 21.862512574031925),\n",
       " (8561, 21.862293281982826),\n",
       " (31995, 21.86221988991267),\n",
       " (94830, 21.861382544157316),\n",
       " (45034, 21.86134031577414),\n",
       " (75009, 21.859852833782266),\n",
       " (4883, 21.85950901974132),\n",
       " (70512, 21.859393032590013),\n",
       " (44087, 21.858953691690147),\n",
       " (4604, 21.85845912230957),\n",
       " (9804, 21.85749594644765),\n",
       " (40897, 21.85728831179255),\n",
       " (10136, 21.85673993548228),\n",
       " (12092, 21.85673186589173),\n",
       " (38404, 21.856368971823656),\n",
       " (4214, 21.85596943712978),\n",
       " (45491, 21.855383460566074),\n",
       " (43316, 21.855282666402065),\n",
       " (940, 21.85527304380996),\n",
       " (13286, 21.855240763180785),\n",
       " (2402, 21.85520824115859),\n",
       " (5466, 21.855002746371408),\n",
       " (134, 21.854958048657235),\n",
       " (83287, 21.854835844749385),\n",
       " (90696, 21.854825174961334),\n",
       " (91798, 21.85441914226501),\n",
       " (63734, 21.85433585005998),\n",
       " (43348, 21.852816369653613),\n",
       " (8706, 21.852387752925253),\n",
       " (9177, 21.850001128841257),\n",
       " (12234, 21.849479892342487),\n",
       " (477, 21.849370440426),\n",
       " (5898, 21.849233485290572),\n",
       " (91000, 21.848432422391568),\n",
       " (37078, 21.8478887975261),\n",
       " (604, 21.84786616028443),\n",
       " (8705, 21.847829132014088),\n",
       " (6216, 21.847346671504585),\n",
       " (43723, 21.84709389539377),\n",
       " (94492, 21.846985919275802),\n",
       " (94681, 21.846913167128697),\n",
       " (44285, 21.846777516179174),\n",
       " (8460, 21.846590990357733),\n",
       " (4974, 21.846430897717184),\n",
       " (6038, 21.846032991972614),\n",
       " (4469, 21.846003343969052),\n",
       " (82809, 21.845767771397572),\n",
       " (44666, 21.845172703412636),\n",
       " (570, 21.844477501977043),\n",
       " (67137, 21.844179340909466),\n",
       " (60385, 21.843898023415782),\n",
       " (76014, 21.843863806804723),\n",
       " (75548, 21.84298079946588),\n",
       " (69324, 21.842775948145917),\n",
       " (91684, 21.842172843377135),\n",
       " (9203, 21.84187473117256),\n",
       " (11139, 21.84105676131863),\n",
       " (93412, 21.84059179868785),\n",
       " (70032, 21.840408723050988),\n",
       " (82077, 21.84031697283345),\n",
       " (95168, 21.83975090190782),\n",
       " (96704, 21.83944750994142),\n",
       " (4477, 21.839118536777114),\n",
       " (83248, 21.839086329750266),\n",
       " (25578, 21.83860298910296),\n",
       " (9171, 21.837741339177754),\n",
       " (4978, 21.837563999802036),\n",
       " (92441, 21.837372850491562),\n",
       " (5050, 21.83729307852116),\n",
       " (5692, 21.83699241982367),\n",
       " (90232, 21.83655040858986),\n",
       " (4839, 21.836282569527143),\n",
       " (82833, 21.83605876773025),\n",
       " (442, 21.83592181729836),\n",
       " (34058, 21.83582645147677),\n",
       " (94911, 21.83578965086478),\n",
       " (5556, 21.83576467098998),\n",
       " (44397, 21.835730599946128),\n",
       " (8643, 21.83555396499841),\n",
       " (5456, 21.83479829400163),\n",
       " (9826, 21.83457900195253),\n",
       " (93567, 21.8344776988369),\n",
       " (37804, 21.834008412302822),\n",
       " (9190, 21.833935415143703),\n",
       " (48387, 21.833597994104),\n",
       " (45995, 21.83338008709253),\n",
       " (90079, 21.83303652641365),\n",
       " (986, 21.83251904397636),\n",
       " (76902, 21.83241034304265),\n",
       " (8520, 21.832182986120777),\n",
       " (10344, 21.8321244672647),\n",
       " (63605, 21.831907030676412),\n",
       " (8506, 21.831401867739796),\n",
       " (70646, 21.830650158399884),\n",
       " (7598, 21.83058928667087),\n",
       " (7749, 21.829737375085337),\n",
       " (44121, 21.829629857235354),\n",
       " (6763, 21.828973015272627),\n",
       " (38431, 21.82892797124684),\n",
       " (9206, 21.82848286001734),\n",
       " (44396, 21.828266997968896),\n",
       " (93818, 21.82810804245093),\n",
       " (68130, 21.828004478484644),\n",
       " (39936, 21.82798607967284),\n",
       " (8500, 21.827581791878316),\n",
       " (20724, 21.827315433616022),\n",
       " (9155, 21.826856376710808),\n",
       " (87330, 21.826621570029683),\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c4ba84c-eaf3-4638-9c51-fb0625482093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arxiv_id': '1707.06742v3', 'title': 'Machine Teaching: A New Paradigm for Building Machine Learning Systems', 'authors': \"['Patrice Y. Simard', 'Saleema Amershi', 'David M. Chickering', 'Alicia Edelman Pelton', 'Soroush Ghorashi', 'Christopher Meek', 'Gonzalo Ramos', 'Jina Suh', 'Johan Verwey', 'Mo Wang', 'John Wernsing']\", 'summary': 'The current processes for building machine learning systems require\\npractitioners with deep knowledge of machine learning. This significantly\\nlimits the number of machine learning systems that can be created and has led\\nto a mismatch between the demand for machine learning systems and the ability\\nfor organizations to build them. We believe that in order to meet this growing\\ndemand for machine learning systems we must significantly increase the number\\nof individuals that can teach machines. We postulate that we can achieve this\\ngoal by making the process of teaching machines easy, fast and above all,\\nuniversally accessible.\\n  While machine learning focuses on creating new algorithms and improving the\\naccuracy of \"learners\", the machine teaching discipline focuses on the efficacy\\nof the \"teachers\". Machine teaching as a discipline is a paradigm shift that\\nfollows and extends principles of software engineering and programming\\nlanguages. We put a strong emphasis on the teacher and the teacher\\'s\\ninteraction with data, as well as crucial components such as techniques and\\ndesign principles of interaction and visualization.\\n  In this paper, we present our position regarding the discipline of machine\\nteaching and articulate fundamental machine teaching principles. We also\\ndescribe how, by decoupling knowledge about machine learning algorithms from\\nthe process of teaching, we can accelerate innovation and empower millions of\\nnew uses for machine learning models.', 'published': '2017-07-21T02:37:04Z', 'text': 'Machine Teaching\\nA New Paradigm for Building Machine Learning Systems\\nPatrice Y. Simard\\npatrice@microsoft.com\\nSaleema Amershi\\nsamershi@microsoft.com\\nDavid M. Chickering\\ndmax@microsoft.com\\nAlicia Edelman Pelton\\naliciaep@microsoft.com\\nSoroush Ghorashi\\nsorgh@microsoft.com\\nChristopher Meek\\nmeek@microsoft.com\\nGonzalo Ramos\\ngoramos@microsoft.com\\nJina Suh\\njinsuh@microsoft.com\\nJohan Verwey\\njoverwey@microsoft.com\\nMo Wang\\nmowan@microsoft.com\\nJohn Wernsing\\njohnwer@microsoft.com\\nMicrosoft Research, One Microsoft Way, Redmond, WA 98052, USA\\nAbstract\\nThe current processes for building machine\\nlearning systems require practitioners with\\ndeep knowledge of machine learning.\\nThis\\nsigniﬁcantly limits the number of machine\\nlearning systems that can be created and has\\nled to a mismatch between the demand for\\nmachine learning systems and the ability for\\norganizations to build them. We believe that\\nin order to meet this growing demand for ma-\\nchine learning systems we must signiﬁcantly\\nincrease the number of individuals that can\\nteach machines. We postulate that we can\\nachieve this goal by making the process of\\nteaching machines easy, fast and above all,\\nuniversally accessible.\\nWhile machine learning focuses on creating\\nnew algorithms and improving the accuracy\\nof “learners”, the machine teaching discipline\\nfocuses on the eﬃcacy of the “teachers”. Ma-\\nchine teaching as a discipline is a paradigm\\nshift that follows and extends principles of\\nsoftware engineering and programming lan-\\nguages.\\nWe put a strong emphasis on the\\nteacher and the teacher’s interaction with\\ndata, as well as crucial components such as\\ntechniques and design principles of interac-\\ntion and visualization.\\nIn this paper, we present our position re-\\ngarding the discipline of machine teaching\\nand articulate fundamental machine teaching\\nprinciples. We also describe how, by decou-\\npling knowledge about machine learning al-\\ngorithms from the process of teaching, we can\\naccelerate innovation and empower millions\\nof new uses for machine learning models.\\n1. Introduction\\nThe demand for machine learning (ML) models far\\nexceeds the supply of “machine teachers” that can\\nbuild those models. Categories of common-sense un-\\nderstanding tasks that we would like to automate with\\ncomputers include interpreting commands, customer\\nsupport, or agents that perform tasks on our behalf.\\nThe combination of categories, domains, and tasks\\nleads to millions of opportunities for building special-\\nized, high-accuracy machine learning models. For ex-\\nample, we might be interested in building a model to\\nunderstand voice commands for controlling a televi-\\nsion or building an agent for making restaurant reser-\\nvations. The key to opening up the large space of solu-\\ntions to is to increase the number of machine teachers\\nby making the process of teaching machines easy, fast\\nand universally accessible.\\nA large fraction of the machine learning community is\\nfocused on creating new algorithms to improve the ac-\\ncuracy of the “learners” (machine learning algorithms)\\non given labeled data sets.\\nThe machine teaching\\n(MT) discipline is focused on the eﬃcacy of the teach-\\ners given the learners. The metrics of machine teaching\\nmeasure performance relative to human costs, such as\\narXiv:1707.06742v3  [cs.LG]  11 Aug 2017\\nMachine Teaching\\nproductivity, interpretability, robustness, and scaling\\nwith the complexity of the problem or the number of\\ncontributors.\\nMany problems that aﬀect model building productiv-\\nity are not addressed by traditional machine learning.\\nOne such problem is concept evolution, a process in\\nwhich the teacher’s underlying notion of the target\\nclass is deﬁned and reﬁned over time (Kulesza et al.,\\n2014). Label noise or inconsistencies can be detrimen-\\ntal to traditional machine learning because it assumes\\nthat the target concept is ﬁxed and is deﬁned by the\\nlabels. In practice, concept deﬁnitions, schemas, and\\nlabels can change as new sets of rare positives are dis-\\ncovered or when teachers simply change their minds.\\nConsider a binary classiﬁcation task for gardening web\\npages where the machine learner and feature set is\\nﬁxed. The teacher may initially label botanical gar-\\nden web pages as positive examples for the gardening\\nconcept, but then later decide that these are negative\\nexamples. Relabeling the examples when the target\\nconcept evolves is a huge burden on the teacher. From\\na teacher’s perspective, concepts should be decompos-\\nable into sub-concepts and the manipulation of the\\nrelationship between sub-concepts should be easy, in-\\nterpretable, and reversible. At the onset, the teacher\\ncould decompose gardening into sub-concepts (that in-\\nclude botanical gardens) and label the web page ac-\\ncording to this concept schema.\\nIn this scenario, labeling for sub-concepts has no ben-\\neﬁts to the machine learning algorithm, but it beneﬁts\\nthe teacher by enabling concept manipulation. Manip-\\nulation of sub-concepts can be done in constant time\\n(i.e., not dependent on the number of labels), and the\\nteacher’s semantic decisions can be documented for\\ncommunication and collaboration. Addressing concept\\nevolution is but one example of where the emphasis on\\nthe teacher’s perspective can make a large diﬀerence\\nin model building productivity.\\nMachine teaching is a paradigm shift away from ma-\\nchine learning, akin to how other ﬁelds in program-\\nming language have shifted from optimizing perfor-\\nmance to optimizing productivity with the notions of\\nfunctional programming, programming interfaces, ver-\\nsion control, etc. The discipline of machine teaching\\nfollows and extends principles of software engineering\\nand languages that are fundamental to software pro-\\nductivity. Machine teaching places a strong empha-\\nsis on the teacher and the teacher’s interaction with\\ndata, and techniques and design principles of interac-\\ntion and visualization are crucial components.\\nMa-\\nchine teaching is also directly connected to machine\\nlearning fundamentals as it deﬁnes abstractions and\\ninterfaces between the underlying algorithm and the\\nteaching language. Therefore, machine teaching lives\\nat the interaction of the human-computer interaction,\\nmachine learning, visualization, systems and engineer-\\ning ﬁelds. The goal of this paper is to explore machine\\nlearning model building from the teacher’s perspec-\\ntive.\\n2. The need for a new discipline\\nIn 2016, at one of Microsoft’s internal conferences\\n(TechFest) during a panel titled “How Do We Build\\nand Maintain Machine Learning Systems?”, the host\\nstarted the discussion by asking the audience “What is\\nyour worst nightmare?” in the context of building ma-\\nchine learning models for production. A woman raised\\nher hand and gave the ﬁrst answer:\\n“[...]\\nManage versions.\\nManage data ver-\\nsions. Being able to reproduce the models.\\nWhat if, you know, the data disappears, the\\nperson disappears, the model disappears...\\nAnd we cannot reproduce this. I have seen\\nthis hundreds of times in Bing. I have seen\\nit every day. Like... Oh yeah, we had a good\\nmodel.\\nOk, I need to tweak it.\\nI need to\\nunderstand it. And then... Now we cannot\\nreproduce it. That is my biggest nightmare!”\\nTo put context to this testimony, we review what\\nbuilding a machine learning model may look like in\\na product group:\\n1. A problem owner collects data, writes labeling\\nguidelines, and optionally contributes some labels.\\n2. The problem owner outsources the task of labeling\\na large portion of the data (e.g., 50,000 examples).\\n3. The problem owner examines the labels and may\\ndiscover that the guidelines are incorrect or that\\nthe sampled examples are inappropriate or in-\\nadequate for the problem. When that happens,\\nGOTO step 1.\\n4. An ML expert is consulted to select the algorithm\\n(e.g., deep neural network), the architecture (e.g.,\\nnumber of layers, units per layer, etc.), the objec-\\ntive function, the regularizers, the cross-validation\\nsets, etc.\\n5. Engineers adjust existing features or create new\\nfeatures to improve performance.\\nModels are\\ntrained and deployed on a fraction of traﬃc for\\ntesting.\\nMachine Teaching\\n6. If the system does not perform well on test traﬃc,\\nGOTO step 1\\n7. The model is deployed on full traﬃc. Performance\\nof the model is monitored, and if that performance\\ngoes below a critical level, the model is modiﬁed\\nby returning to step 1.\\nAn iteration through steps 1 to 6 typically takes weeks.\\nThe system can be stable at step 7 for months. When\\nit eventually breaks, it can be for a variety of reasons:\\nthe data distribution has changed, the competition\\nhas improved and the requirements have increased,\\nnew features are available and some old features are\\nno longer available, the deﬁnition of the problem has\\nchanged, or a security update or other change has bro-\\nken the code. At various steps, the problem owner,\\nthe machine learning expert, or the key engineer may\\nhave moved on to another group or another company.\\nThe features or the labels were not versioned or doc-\\numented. No one understands how the data was col-\\nlected because it was done in an ad hoc and organic\\nfashion.\\nBecause multiple players with diﬀerent ex-\\npertise are involved, it takes a signiﬁcant amount of\\neﬀort and coordination to understand why the model\\ndoes not perform as well as expected after being re-\\ntrained. In the worst case, the model is operating but\\nno one can tell if it is performing as expected, and\\nno one wants the responsibility of turning it oﬀ. Ma-\\nchine learning “litter” starts accumulating everywhere.\\nThese problems are not new to machine learning in\\npractice (Sculley et al., 2014).\\nThe example above illustrates the fact that building a\\nmachine learning model involves more than just col-\\nlecting data and applying learning algorithms, and\\nthat the management process of building machine\\nlearning solutions can be fraught with ineﬃciencies.\\nThere are other forms of ineﬃciencies that are deeply\\nembedded in the current machine learning paradigm.\\nFor instance, machine learning projects typically con-\\nsist of a single monolithic model trained on a large\\nlabeled data set. If the model’s summary performance\\nmetrics (e.g., accuracy, F1 score) were the only re-\\nquirements and the performance remained unchanged,\\nadding examples would not be a problem even if the\\nnew model errs on the examples that were previously\\npredicted correctly. However, for many problems for\\nwhich predictability and quality control are important,\\nany negative progress on the model quality leads to\\nlaborious testing of the entire model and incurs high\\nmaintenance cost. A single monolithic model lacks the\\nmodularity required for most people to isolate and ad-\\ndress the root cause of a regression problem.\\n2.1. Deﬁnitions of machine learning and\\nmachine teaching\\nIt is diﬃcult to argue that the challenges discussed\\nabove are given a high priority in the world’s best\\nmachine learning conferences.\\nThese problems and\\nineﬃciencies do not stem from the machine learning\\nalgorithm, which is the central topic of the machine\\nlearning ﬁeld; they come from the processes that use\\nmachine learning, from the interaction between people\\nand machine learning algorithms, and from people’s\\nown limitations.\\nTo give more weight to this assertion, we will deﬁne\\nthe machine learning research ﬁeld narrowly as:\\nDeﬁnition 2.1 (Machine learning research)\\nMachine\\nLearning\\nresearch\\naims\\nat\\nmaking\\nthe\\nlearner better by improving ML algorithms.\\nThis ﬁeld covers, for instance, any new variations or\\nbreakthroughs in deep learning, unsupervised learning,\\nrecurrent networks, convex optimization, and so on.\\nConversely, we see version control, concept decomposi-\\ntion, semantic data exploration, expressiveness of the\\nteaching language, interpretability of the model, and\\nproductivity as having more in common with program-\\nming and human-computer interaction than with ma-\\nchine learning.\\nThese “machine teaching” concepts,\\nhowever, are extraordinarily important to any practi-\\ntioners of machine learning. Hence, we deﬁne a disci-\\npline aimed at improving these concept as:\\nDeﬁnition 2.2 (Machine teaching research)\\nMachine teaching research aims at making the teacher\\nmore productive at building machine learning models.\\nWe have chosen these deﬁnitions to minimize the inter-\\nsection between the two ﬁelds and thus provide clarity\\nand scoping. The two disciplines are complementary\\nand can evolve independently. Of course, like any gen-\\neralization, there are limitations. Curriculum learn-\\ning (Bengio et al., 2009), for instance, could be seen\\nas belonging squarely in the intersection because it in-\\nvolves both a learning algorithm and teacher behavior.\\nNevertheless, we have found these deﬁnitions useful to\\ndecide what to work on and what not to work on.\\n2.2. Decoupling machine teaching from\\nmachine learning\\nMachine teaching solutions require one or more ma-\\nchine learning algorithms to produce models through-\\nout the teaching process (and for the ﬁnal output).\\nThis requirement can make things complex for teach-\\ners. Diﬀerent deployment environments may support\\nMachine Teaching\\ndiﬀerent runtime functions, depending on what re-\\nsources are available (e.g., DSPs, GPUs, FPGAs, tight\\nmemory or CPU constraints) or what has been imple-\\nmented and green-lighted for deployment.\\nMachine\\nlearning algorithms can be understood as “compilers”\\nthat convert the teaching information to an instance\\nof the set of functions available at runtime. For ex-\\nample, each such instance might be characterized by\\nthe weights in a neural network, the “means” in K-\\nmeans, the support vectors in SVMs, or the decisions\\nin decision trees. For each set of runtime functions,\\ndiﬀerent machine learning compilers may be available\\n(e.g., LBFGS, stochastic gradient descent), each with\\nits own set of parameters (e.g., history size, regulariz-\\ners, k-folds, learning rates schedule, batch size, etc.)\\nMachine teaching aims at shielding the teacher from\\nboth the variability of the runtime and the complex-\\nity of the optimization. This has a performance cost:\\noptimizing for a target runtime with expert control of\\nthe optimization parameters will always outperform\\ngeneric parameter-less optimization. It is akin to in-\\nlining assembly code. But like high-level programming\\nlanguages, our goal with machine teaching is to reduce\\nthe human cost in terms of both maintenance time\\nand required expertise. The teaching language should\\nbe “write once, compile anywhere”, following the ISO\\nC++ philosophy.\\nUsing well-deﬁned interfaces describing the inputs\\n(feature values) and outputs (label value predictions)\\nof machine learning algorithms, the teaching solution\\ncan leverage any machine learning algorithms that\\nsupport these interfaces. We impose three additional\\nsystem requirements:\\n1. The featuring language available to the teacher\\nshould be expressive enough to enable examples\\nto be distinguished in meaningful ways (a hash of\\na text document has distinguishing power, but it\\nis not considered meaningful). This enables the\\nteacher to remove feature blindness without nec-\\nessarily increasing concept complexity.\\n2. The complexity (VC dimension) of the set of func-\\ntions that the system can return increases with\\nthe dimension of the feature space. This enables\\nthe teacher to decrease the approximation error\\nby adding features.\\n3. The available ML algorithms must satisfy the\\nclassical deﬁnition of learning consistency (Vap-\\nnik, 2013). This enables the teacher to decrease\\nthe estimation error by adding labeled examples.\\nThe aim of these requirements is to enable teachers to\\ncreate and debug any concept function to an arbitrary\\nlevel of accuracy without being required to understand\\nthe runtime function space, learning algorithms, or op-\\ntimization.\\n3. Analogy to programming\\nIn this section, we argue that teaching machines is a\\nform of programming. We ﬁrst describe what machine\\nteaching and programming have in common. Next, we\\nhighlight several tools developed to support software\\ndevelopment that we argue are likely to provide valu-\\nable guidance and inspiration to the machine teaching\\ndiscipline. We conclude this section with a discussion\\nof the history of the discipline of programming and\\nhow it might be predictive of the trajectory of the dis-\\ncipline of machine teaching.\\n3.1. Commonalities and diﬀerences between\\nprogramming and teaching\\nAssume that a software engineer needs to create a\\nstateless target function (e.g., as in functional pro-\\ngramming) that returns value Y given input X. While\\nnot strictly sequential, we can describe the program-\\nming process as a set of steps as follows:\\n1. The target function needs to be speciﬁed\\n2. The target function can be decomposed into sub-\\nfunctions\\n3. Functions (including sub-functions) need to be\\ntested and debugged\\n4. Functions can be documented\\n5. Functions can be shared\\n6. Functions can be deployed\\n7. Functions need to be maintained (scheduled and\\nunscheduled debug cycles)\\nFurther assume that a teacher wants to build a target\\nclassiﬁcation function that returns class Y given in-\\nput X. The process for machine teaching presented in\\nthe previous section is similar to the set of program-\\nming steps above. While there are strong similarities,\\nthere are also signiﬁcant diﬀerences, especially in the\\ndebugging step (Table 1).\\nIn order to strengthen the analogy between teaching\\nand programming, we need a machine teaching lan-\\nguage that lets us express these steps in the context\\nof a machine learning model building task. For pro-\\ngramming, the examples of languages include C++,\\nMachine Teaching\\nTable 1. Comparison of debugging steps in programming and machine teaching\\nDebugging in programming\\nDebugging in machine teaching\\n(3) Repeat:\\n(3) Repeat:\\n(a) Inspect\\n(a) Inspect\\n(b) Edit code\\n(b) Edit/add knowledge (e.g., labels, features, ...)\\n(c) Compile\\n(c) Train\\n(d) Test\\n(d) Test\\nPython, JavaScript, etc. which can be compiled into\\nmachine language for execution. For teaching, the lan-\\nguage is a means of expressing teacher knowledge into\\na form that a machine learning algorithm can leverage\\nfor training. Teacher knowledge does not need to be\\nlimited to providing labels but can be a combination\\nof schema constraints (e.g., mutually exclusive labels\\nfor classiﬁcation, state transition constraints in entity\\nextraction1), labeled examples, and features. Just as\\nnew programing languages are being developed to ad-\\ndress current limitations, we expect that new teaching\\nlanguages will be developed that allow the teacher to\\ncommunicate diﬀerent types of knowledge and to com-\\nmunicate knowledge more eﬀectively.\\n3.2. Programming paving the way forward\\nAs we have illustrated in the previous sections, cur-\\nrent machine learning processes require multiple peo-\\nple of diﬀerent expertise and strong knowledge depen-\\ndency among them, there are no standards or tooling\\nfor versioning of data and models, and there is a strong\\nco-dependency between problem formulation, training\\nand the underlying machine learning algorithms. For-\\ntunately, the emerging discipline of machine teaching\\ncan leverage lessons learned from the programming,\\nsoftware engineering and related disciplines.\\nThese\\ndisciplines have developed over the last half century\\nand addressed many analogous problems that machine\\nteaching aims to solve. This is not surprising given\\ntheir strong commonalities. In this section, we high-\\nlight several lessons and relate them to machine teach-\\ning.\\n3.2.1. Solving complex problems\\nThe programming discipline has developed and im-\\nproved a set of tools, techniques and principles that\\nallow software engineers to solve complex problems in\\nways that allow for eﬃcient, maintainable and under-\\nstandable solutions. These principles include problem\\ndecomposition, encapsulation, abstraction, and design\\n1In an address recognizer, we might want to require that\\nthe zip code appears after the state.\\npatterns. Rather than discussing each of these, we con-\\ntrast the diﬀering expectations between software engi-\\nneers solving a complex problem and machine teachers\\nsolving a complex problem. One of the most power-\\nful concepts that allowed software engineers to write\\nsystems that solve complex problems is that of decom-\\nposition. The next anecdote illustrates its importance\\nand power.\\nWe asked dozens of software engineers the following:\\n1. Can you write a program that correctly imple-\\nments the game Tetris?\\n2. Can you do it in a month?\\nThe answer to the ﬁrst question is universally “yes”.\\nThe answer to the second question varies from “I think\\nso” to “why would it take more than 2 days?”. The\\nﬁrst question is arguably related to the Church-Turing\\nthesis which states that all computable functions are\\ncomputable by a Turing machine. If a human can com-\\npute the function, there exists a program that can per-\\nform the same computation on a Turing machine. In\\nother words, given that there is an algorithm to imple-\\nment the Tetris game, most respectable software en-\\ngineers believe they can also implement the game on\\nwhatever machine they have access to and in whatever\\nprogramming language they are familiar with.\\nThe\\nanswer to the second question is more puzzling. The\\nstate space in a Tetris game (informally the number of\\nconﬁgurations of the pieces on the screen) is very large,\\nin fact, far larger than can be examined by the software\\nengineer. Indeed, one might expect that the complex-\\nity of the program should grow exponentially with the\\nsize of the representation of a state in the state space.\\nYet, the software engineers seem conﬁdent that they\\ncan implement the game in under a month. The most\\nlikely explanation is that they consider the complexity\\nof the implementation and the debugging to be poly-\\nnomial in both the representation of the state space\\nand the input.\\nLet us examine how machine learning experts react\\nto similar questions asked about teaching a complex\\nMachine Teaching\\nproblem:\\n1. Can you teach a machine to recognize kitchen\\nutensils in an image as well as you do?\\n2. Can you do it in a month?\\nWhen these questions were asked to another handful of\\nmachine learning experts, the answers were quite var-\\nied. While one person answered “yes” to both ques-\\ntions without hesitation, most machine learning ex-\\nperts were less conﬁdent about both questions with\\nanswers including “probably”, “I think so”, “I am not\\nsure”, and “probably not”. Implementing the Tetris\\ngame and recognizing simple non-deformable objects\\nseem like fairly basic functions in either ﬁelds, thus it\\nis surprising that the answers to both sets of questions\\nare so diﬀerent.\\nThe goal of both programming and teaching is to cre-\\nate a function. In that respect, the two activities have\\nfar more in common than they have diﬀerences.\\nIn\\nboth cases we are writing functions, so there is no rea-\\nson to think that the Church-Turing thesis is not true\\nfor teaching. Despite the similarities, the expectations\\nof success for creating, debugging, and maintaining\\nsuch function diﬀer widely between software engineers\\nand teachers. While the programming languages and\\nteaching languages are diﬀerent, the answers to the\\nquestions were the same for all software engineers re-\\ngardless of the programming languages. Yet, most ma-\\nchine learning experts did not give upper bounds on\\nhow long it would take to solve a teaching problem,\\neven when they thought the problem was solvable.\\nSoftware engineers have the conﬁdence of being able\\nto complete the task in a reasonable time because\\nthey have learned to decompose problems into smaller\\nproblems.\\nEach smaller problem can be further de-\\ncomposed until coding and debugging can be done in\\nconstant or polynomial time.\\nFor instance, to code\\nTetris, one can create a state module, a state transfor-\\nmation module, an input module, a scoring module, a\\nshape display module, an animation module, and so\\non. Each of these modules can be further decomposed\\ninto smaller modules. The smaller modules can then\\nbe composed and debugged in polynomial time. Given\\nthat each module can be built eﬃciently, software en-\\ngineers have conﬁdence that they can code Tetris in\\nless than a month’s time.\\nIt is interesting to observe that that the ability to de-\\ncompose a problem is a learned skill and is not easy to\\nlearn. A smart student could understand and learn all\\nthe functions of a programming language (variables,\\narrays, conditional statements, for loops, etc.)\\nin a\\nweek or two. If the same student was asked to code\\nTetris after two weeks, they would not know where to\\nstart. After 6 to 12 months of learning how to pro-\\ngram, most software engineers would be able to ac-\\ncommodate the task of programming the Tetris game\\nin under a month.\\nAkin to how decomposition brings conﬁdence to soft-\\nware engineers2 and an upper bound to solving com-\\nplex problems, machine teachers can learn to decom-\\npose complex machine learning problems with the\\nright tools and experiences, and the machine teach-\\ning discipline can bring the expectations of success for\\nteaching a machine to a level comparable to that of\\nprogramming.\\n3.2.2. Scaling to multiple contributors\\nThe complexity of the problems that software engi-\\nneers can solve has increased signiﬁcantly over the past\\nhalf century, but there are limits to the scale of prob-\\nlems that one software engineer can solve. To address\\nthis, many tools and techniques have been developed\\nto enable multiple engineers to contribute to the solu-\\ntion of a problem. In this section, we focus on three\\nconcepts - programming languages, interfaces (APIs),\\nand version control.\\nOne of the key developments that enables scaling with\\nthe number of contributors is the creation of standard-\\nized programming languages. The use of a standard-\\nized programming language along with design pat-\\nterns and documentation enables other collaborators\\nto read, understand and maintain the software. The\\nanalog to programming languages for machine teach-\\ning is the expressions of a teacher’s domain knowledge\\nwhich include labels, features and schemas. Currently,\\nthere is no standardization of the programming lan-\\nguages for machine teaching.\\nAnother key development that enables scaling with the\\nnumber of contributors is the use of componentization\\nand interfaces, which are closely related to the idea of\\nproblem decomposition discussed above. Componen-\\ntization allows for a separation of concerns that re-\\nduces development complexity, and clear interfaces al-\\nlow for independent development and innovation. For\\ninstance, a software engineer does not need to consider\\nthe details of the hardware upon which the solution\\nwill run. For machine teaching, the development of\\nclear interfaces for services required for teaching, such\\nas training, sampling and featuring, would enable inde-\\npendent teaching. In addition, having clear interfaces\\n2For similar reasons, the ability to decompose also bring\\nconﬁdence to professional instructors and animal trainers.\\nMachine Teaching\\nfor models, features, labels, and schemas enables com-\\nposing these constituent parts to solve more complex\\nproblems, and thus, allowing for their use in problem\\ndecomposition.\\nThe ﬁnal development that enables scaling with the\\nnumber of contributors is the development of version\\ncontrol systems. Modern version control systems sup-\\nport merging contributions by multiple software engi-\\nneers, speculative development, isolation of bug ﬁxes\\nand independent feature development, and rolling back\\nto previous versions among many other beneﬁts. The\\nprimary role of a version control system is to track and\\nmanage changes to the source code rather than keeping\\ntrack of the compiled binaries. Similarly, in machine\\nteaching, a version control system could support man-\\naging the changes of the labels, features, schemas, and\\nlearners used for building the model and enable re-\\nproducibility and branching for experimentation while\\nproviding documentation and transparency necessary\\nfor collaboration.\\n3.2.3. Supporting the development of\\nproblem solutions\\nIn the past few decades, there has been an explosion\\nof tools and processes aimed at increasing program-\\nming productivity. These include the development of\\nhigh-level programming languages, innovations in inte-\\ngrated development environments, and the creation of\\ndevelopment processes. Some of these tools and pro-\\ncesses have a direct analog in machine teaching, and\\nsome are yet to be developed and adapted. Table 2\\npresents a mapping of many of these tools and con-\\ncepts to machine teaching.\\n3.3. The trajectory of the machine teaching\\ndiscipline\\nWe conclude this section with a brief review of the\\nhistory of programming and how that might inform\\nthe trajectory of the machine teaching discipline. The\\nhistory of programming is inexorably linked to the de-\\nvelopment of computers. Programming started with\\nscientiﬁc and engineering tasks (1950s) with few pro-\\ngrams and programming languages like FORTRAN\\nthat focused on compute performance. In the 1960s,\\nthe range of problems expanded to include manage-\\nment information systems and the range of program-\\nming languages expanded to target speciﬁc application\\ndomains (e.g., COBOL). The explosion of the number\\nof software engineers led to the realization that scal-\\ning with contributors was diﬃcult (Brooks Jr, 1995).\\nIn the 1980s, the scope of problems to which pro-\\ngramming was applied exploded with the advent of\\nthe personal computer as did the number of software\\nengineers solving the problems (e.g., with Basic). Fi-\\nnally, in the 1990s, another explosive round of growth\\nbegan with the advent of web programming and pro-\\ngramming languages like JavaScript and Java. As of\\nwriting this paper, the number of software engineers\\nin the world is approaching 20 million!\\nMachine teaching is undergoing a similar explosion.\\nCurrently, much of the machine teaching eﬀort is un-\\ndertaken by experts in machine learning and statistics.\\nLike the story of programming, the range of problems\\nto which machine learning has been applied has been\\nexpanding. With the deep-learning breakthroughs in\\nperceptual tasks in the 2010s (e.g., speech, vision, self-\\ndriving cars), there has been an incredible eﬀort to\\nbroaden the range of problems addressed by teaching\\nmachines to solve the problems. Similar to the expand-\\ning population of software engineers, the advent of ser-\\nvices like LUIS.ai3 and Wit.ai4 have enabled domain\\nexperts to build their own machine learning models\\nwith no machine learning knowledge. The discipline of\\nmachine teaching is young and in its formative stages.\\nOne can only expect that this growth will continue\\nat an even quicker pace.\\nIn fact, machine teaching\\nmight be the path to bringing machine learning to the\\nmasses.\\n4. The role of teachers\\nThe role of the teacher is to transfer knowledge to the\\nlearning machine so that it can generate a useful model\\nthat can approximate a concept. Let’s deﬁne what we\\nmean by this.\\nDeﬁnition 4.1 (Concept) A concept is a mapping\\nfrom any example to a label value.\\nFor example, the concept of a recipe web page can\\nbe represented by a function that returns zero or\\none, based on whether a web page contains a cooking\\nrecipe. In another example, an address concept can\\nbe represented by a function that, given a document,\\nreturns a list of token ranges, each labeled “address”,\\n“street”, “zip”, “state”, etc. Label values for a binary\\nconcept could be “Is” and “Is Not”.\\nWe may also\\nallow a “Undecided” label which allows a teacher to\\npostpone labeling decisions or ignore ambiguous ex-\\namples. Postponing a decision is important because\\nthe concept may be evolving in the teacher’s head.\\nAn example of this is in (Kulesza et al., 2014).\\nDeﬁnition 4.2 (Feature) A feature is a concept that\\n3https://www.luis.ai/\\n4https://wit.ai/\\nMachine Teaching\\nTable 2. Mapping between programming and machine teaching\\nProgramming\\nMachine teaching\\nCompiler\\nML Algorithms (Neural Networks, SVMs)\\nOperating System/Services/IDEs\\nTraining, Sampling, Featuring Services,\\netc.\\nFrameworks\\nImageNet, word2vec, etc.\\nProgramming Languages (Fortran,\\nPython, C#)\\nLabels, Features, Schemas, etc.\\nProgramming Expertise\\nTeaching Expertise\\nVersion Control\\nVersion Control\\nDevelopment Processes (speciﬁcations,\\nunit testing, deployment, monitoring,\\netc.)\\nTeaching Processes (data collection,\\ntesting, publishing, etc.)\\nassigns each example a scalar value.\\nWe usually use feature to denote a concept when em-\\nphasizing its use in a machine learning model.\\nFor\\nexample, the concept corresponding to the presence or\\nabsence of the word “recipe” in text examples might\\nbe a useful feature when teaching the recipe concept.\\nDeﬁnition 4.3 (Teacher) A teacher is the person\\nwho transfers concept knowledge to a learning ma-\\nchine.\\nTo clarify this deﬁnition of a teacher, the methods of\\nknowledge transfer need to be deﬁned. At this point,\\nthey include a) example selection (biased), b) label-\\ning, c) schema deﬁnition (relationship between labels),\\nd) featuring, and e) concept decomposition (where\\nfeatures are recursively deﬁned as sub-models). The\\nteachers are expected to make mistakes in all the forms\\nof knowledge transfer. These teaching “bugs” are com-\\nmon occurrences.\\nFigure 1 illustrates how concepts, labels, features, and\\nteachers are related. We assume that every concept is\\na computable function of a representation of examples.\\nThe representation is assumed to include all available\\ninformation about each example. The horizontal axis\\nrepresents the (inﬁnite) space of examples. The verti-\\ncal axis represents the (inﬁnite) space of programs or\\nconcepts. In computer science theory, programs and\\nexamples can be represented as (long) integers. Us-\\ning that convention, each integer number on the ver-\\ntical axis could be interpreted as a program, and each\\ninteger number on the horizontal axis could be inter-\\npreted as an example. We ignore the programs that\\ndo not compile and the examples that are nonsensical.\\nWe now use Figure 1 to refer to the diﬀerent ways a\\nteacher can pass information to a learning system.\\nDeﬁnition 4.4 (Selection) Selection is the process\\nby which teachers gain access to an example that ex-\\nempliﬁes useful aspects of a concept.\\nTeachers can select speciﬁc examples by ﬁltering the\\nset of unlabeled examples.\\nBy choosing these ﬁl-\\nters deliberately, they can systematically explore the\\nspace and discover information relevant to concepts.\\nFor example, a teacher may discover insect recipes\\nwhile building a recipe classiﬁer by issuing a query on\\n“source of proteins”. We note that uniform sampling\\nand uncertainty sampling, which have no explicit input\\nfrom a teacher, are likely of little use for discovering\\nrare clusters of positive examples. Combinations of se-\\nmantic ﬁlters involving trained models are even more\\npowerful (e.g., “nutrition proteins” and low score with\\ncurrent classiﬁer). This ability to ﬁnd examples con-\\ntaining useful aspects of a concept enables the teacher\\nto ﬁnd useful features and provide the labels to train\\nthem. Furthermore, the selection choices themselves\\ncan be valuable documentation of the teaching pro-\\ncess.\\nDeﬁnition 4.5 (Label) A label is a (example, con-\\ncept value) pair created by a teacher in relation to a\\nconcept.\\nMachine Teaching\\nFigure 1. Representation of examples and concepts. Each column represents an example and contains all concept values\\nfor that example. A teacher looks in that direction to “divine” a label. The teacher has access to feature concepts not\\navailable to the training set (it is part of the teaching power). However, the teacher does not know his/her own program.\\nEach row represents a concept and contains the value of that concept for all examples. A teacher looks in that direction\\nto “divine” the usefulness of a feature concept. A teacher can guess the values over the space of examples (it is part of\\nthe teaching power). Features selected by the teacher looking horizontally are immune to over-training.\\nTeachers can provide labels by “looking at a column”\\nin Figure 1. It is important to realize that the teachers\\ndo not know which programs are running in their heads\\nwhen they evaluate the target concept values. If they\\nknew the programs, they would transfer their knowl-\\nedge in programmatic form to the machine and would\\nnot need machine learning. Teachers instead look at\\nthe available data of an example and “divine” its label.\\nThey do this by unconsciously evaluating sub-features\\nand combining them to make labeling decisions. The\\nfeature spaces and the combination functions available\\nto the teachers are beyond what is available through\\nthe training sets. This power is what makes the teach-\\ners valuable for the purpose of creating labels.\\nDeﬁnition 4.6 (Schema) A schema is a relation-\\nship graph between concepts.\\nWhen multiple concepts are involved, a teacher can\\nexpress relationship between them. For instance, the\\nteacher could express that the concepts “Tennis” and\\n“Soccer” are mutually exclusive, or that concept “Ten-\\nnis” implies the concept “Sport”. These concept con-\\nstraints are relationships between lines on the dia-\\ngram (true across all examples).\\nSeparating knowl-\\nedge captured by the schema from the knowledge cap-\\ntured by the labels allows information to be conveyed\\nand edited at a high level.\\nThe implied labels can\\nbe changed simply by changing the concept relation-\\nship. For instance, “Golf” could be moved from being\\na sub-concept of “Sport” to being mutually exclusive\\nor vice versa.\\nTeachers can understand and change\\nthe semantics of a concept by reviewing its schema.\\nSemantic decisions can be reversed without editing in-\\ndividual labels.\\nDeﬁnition 4.7 (Generic feature) A\\ngeneric\\nfea-\\nture is a set of related feature functions.\\nGeneric\\nfeatures\\nare\\ncreated\\nby\\nengineers\\nin\\nparametrizable form, and teachers instantiate individ-\\nual features by providing useful and semantic parame-\\nters. For instance, a generic feature could be: “Log(1\\n+ number of instances of words in list X in a docu-\\nment)” and an instantiation would be setting X to a\\nlist of car brands (useful for an automotive classiﬁer).\\nGiven a set of generic features, teachers have the abil-\\nity to evaluate diﬀerent (instantiated) features by look-\\ning along the corresponding horizontal lines in Fig-\\nure 1. Given two features, the teachers can “divine”\\nthat one is better than the other on a large unlabeled\\nset. For instance, a teacher may choose a feature that\\nmeasures the presence of the word “recipe” over a fea-\\nture that measures the presence of the word “the”,\\nMachine Teaching\\neven though the latter feature might yield better re-\\nsults on the training set. This ability to estimate the\\nvalue of a feature over estimated distributions of the\\ntest set is essential to feature engineering, and is prob-\\nably the most useful capability of a teacher. Features\\nselected by the teacher in this manner are immune to\\nover-training because they are created independently\\nof the training set. Note the contrast to “automatic\\nfeature selection”, which only looks at the training set\\nand concept-independent statistics and is susceptible\\nto over-training.\\nDeﬁnition 4.8 (Decomposition) Decomposition is\\nthe act of using simpler concepts to express more com-\\nplex ones.\\nWhereas teachers do not have direct access to the pro-\\ngram implementing their concept, they sometimes can\\ninfer how these programs work. Socrates used to teach\\nby asking the right questions. The “right question” is\\nakin to providing a useful sub-concept, whose value\\nmakes evaluating the overall concept easier. In other\\nwords, Socrates was teaching by decomposition rather\\nthan by examples. This ability is not equally available\\nto teachers. It is learned. It is essential to scaling with\\ncomplexity and with the number of teachers. It is the\\nsame ability that helps software engineers decompose\\nfunctions into sub-functions. Software engineers also\\nacquire this ability with experience. As in program-\\nming, teaching decompositions are not unique (in soft-\\nware engineering, switching from one decomposition to\\nanother is called refactoring).\\nThe knowledge provided by the teacher through con-\\ncept decomposition is high level and modular. Each\\nconcept implementation might provide its own exam-\\nple selection, labels, schema, and features. These can\\nbe viewed as documentation of interfaces and con-\\ntracts. Each concept implementation may be a black\\nbox, but the concept hierarchy is transparent and in-\\nterpretable.\\nConcept decomposition is the highest\\nform of knowledge provided by the teacher.\\nNow that we have deﬁned some of the key roles of the\\n(machine) teacher, we turn to the question of how do\\nwe meet the demand for them.\\nMeeting the demand for teachers\\nWe postulate that the right solution to satisfy the in-\\ncreasing demand for machine learning models is to in-\\ncrease the number of people that can teach machines\\nthese models. But how do we do that and who are\\nthey?\\nThe current ML-focused work ﬂows put the machine\\nlearning or data scientist on the driver’s seat. While\\ntraining more scientists is a way to increase the number\\nof teachers, we believe that that is not the right path\\nto follow. For starters, machine learning and data sci-\\nentists are a scarce and expensive resource. Secondly,\\nmachine learning scientists can serve a better purpose\\ninventing and optimizing learning algorithms. In the\\nsame way, data scientists are indispensable applying\\ntheir expertise to make sense of data and transform it\\ninto a usable form.\\nThe machine teaching process that we envision does\\nnot require the skills of a ML expert or data scientist.\\nMachine teachers use their domain knowledge to pick\\nthe right examples and counterexamples for a concept\\nand explain why they diﬀer.\\nThey do this through\\nan interactive information exchange with a learning\\nsystem. It is within the ranks of the domain experts\\nwhere we will ﬁnd the large population of machine\\nteachers that will increase, by orders of magnitude,\\nthe number of ML models used to solve problems. We\\ncan transform domain experts by making a machine\\nteaching language universally accessible.\\nA key characteristic of domain experts is that they\\nunderstand the semantics of a problem. To this point,\\nwe argue that if a problem’s data does not need to be\\ninterpreted by a person to be useful, machine teaching\\nis not needed. For example, problems for which the\\nlabeled data is abundant or practically limitless; e.g.\\nComputer Vision, Speech Understanding, Genomics\\nAnalysis, Click-Prediction, Financial Forecasting. For\\nthese, powerful learning algorithms or hardware may\\nbe the better strategy to arrive at an eﬀective solution.\\nIn other problems like the above, feature selection us-\\ning cross validation can be used to arrive at a good\\nsolution without the need of a machine teacher.\\nThere is nonetheless, an ever-growing set of prob-\\nlems for which machine teaching is the right approach;\\nproblems where unlabeled data is plentiful and domain\\nknowledge to articulate a concept is essential. Exam-\\nples of these include controlling Internet-of-Things ap-\\npliances through spoken dialogs and the environment’s\\ncontext, or routing customer feedback for a brand new\\nproduct of a start-up to the right department, build-\\ning a one-time assistant to help a paralegal sift through\\nhundreds of thousands of briefs, etc.\\nWe aim at reaching the same number of machine teach-\\ners as there are software engineers, a set counted in\\nthe tens of millions. Table 3 illustrates the diﬀerences\\nin numbers between machine learning scientists, data\\nscientists, and domain experts. By enabling domain\\nexperts to teach, we will enable them to apply their\\nknowledge to solve directly millions of meaningful, per-\\nMachine Teaching\\nsonal, shared, “one-oﬀ” and recurrent problems at a\\nscale that we have never seen.\\n5. Teaching process\\nA teaching or programming language can be applied in\\nmany diﬀerent ways, some more eﬀective than others.\\nWe propose the following principles for the language\\nand process of machine teaching:\\nUniversal teaching language\\nWe do not rely on\\nthe power of speciﬁc machine learning algorithms. The\\nteaching interface is the same for all algorithms.\\nIf\\na machine learning algorithm is swapped for another\\none, more teaching may be necessary, but the teach-\\ning language and the model building experience is not\\nchanged. Machine learning algorithms should be inter-\\nchangeable. Conversely, the teaching language should\\nbe simple and easy to learn given the domain (e.g.,\\ntext, signal, images).\\nIdeally, we aim at designing\\nan ANSI or ISO standard per domain. Teachers that\\nspeak the same language should be interchangeable.\\nFeature completeness (or realizability)\\nWe as-\\nsume that all the target concepts that a teacher may\\nwant to implement are “realizable” through a recur-\\nsive composition of models and existing features. This\\nimplies a property on the feature set, which we call\\n“feature completeness”. Feature completeness is the\\nresponsibility of the teaching tool, not the teachers.\\nTeachers achieve realizability through the following ac-\\ntions:\\n1. Add missing features: If a teacher can distin-\\nguish two documents belonging to two diﬀerent\\nclasses in a meaningful way, there must be a (cor-\\nresponding) feature expressible in the system that\\ncan make an equivalent semantically meaningful\\ndistinction. By adding such a feature, the teacher\\ncan correct feature blindness errors. If no such\\nfeature exists, the language is not feature com-\\nplete for distinguishing the desired classes.\\n2. Create features through decomposition: If\\nthe concept function cannot be learned from the\\nexisting set of features due to limitations of the\\nmodel class, the teacher can circumvent this prob-\\nlem by creating features that are themselves mod-\\nels; we call this process “model decomposition”.\\nTo illustrate the point, suppose there are two bi-\\nnary features A and B, and the teacher would like\\nto produce a model for A⊕B (where ⊕stands for\\nXOR) using logistic regression.\\nBecause of the\\ncapacity limitations of logistic regression, it is im-\\npossible to represent A ⊕B without additional\\nfeatures. If the teacher adds a third AND feature\\nA∧B, however, logistic regression can work. Note\\nthat A∧B is itself learnable via logistic regression\\nin the A and B feature space.\\n3. Explicitly ignore ambiguous patterns: Am-\\nbiguous patterns can be marked as “don’t care”\\nto avoid wasting features, labels, and the teacher’s\\ntime on diﬃcult examples. Areas of “don’t care”\\nare used as a coping mechanism to keep the real-\\nizability assumption despite the Bayes error rate.\\nThis action does not constrain the feature set.\\nFeature completeness of a teaching language does not\\nimply that the language can be used to eﬃciently teach\\nconcepts. If a feature complete language is not very\\nexpressive, realizability can require a large number of\\nmodel compositions. If a feature complete language\\nis too expressive (e.g. a features can be speciﬁed as\\nprograms), the teachers have to become engineers.\\nRich and diverse sampling set\\nWe call the set of\\nunlabeled documents accessible to the teacher when\\nbuilding models the “sampling distribution”.\\nWe\\ncall the set of documents for which models are built\\nthe “deployment distribution”. The rich-and-diverse-\\nsampling-set principle is that the sampling distribution\\ncaptures the richness and diversity of examples in the\\ndeployment distribution.\\nThe sampling distribution\\nand the deployment distributions are preferably simi-\\nlar, but they do not have to be perfectly matched. The\\nmost important requirement of the sampling distribu-\\ntion is that all important types of documents be repre-\\nsented (rich and diverse). If important documents are\\nmissing from the sampling distribution, performance\\ncould be impacted in unpredictable ways. As a rule of\\nthumb, unlabeled data should be collected indiscrim-\\ninately because the cost of storing data is negligible\\ncompared to the cost of teaching; we view selectively\\ncollecting only the data that is meant to be labeled\\nas both risky and limiting5. A rich and diverse data\\nset allows the teacher to explore it to express knowl-\\nedge through selection. It also allows the teacher to\\nﬁnd examples that can be used to train sub-concepts\\nthat are more speciﬁc than the original concept. For\\ninstance, a teacher could decide to build classiﬁers for\\nbonsai gardening (sub-concept) and botanical garden-\\ning (excluded concept) to be used as features to a gar-\\ndening classiﬁer. The sampling set needs to be rich\\nenough to contain suﬃcient examples to successfully\\nlearn the sub-concepts. The sampling distribution can\\n5For example, the collected set may not contain im-\\nportant examples that would otherwise be found via the\\nmachine teaching process.\\nMachine Teaching\\nTable 3. Where to ﬁnd machine teachers\\nPotential teacher\\nQuantities\\nCharacteristics\\nMachine learning experts\\nTens of thousands\\nHas profound understanding\\nof machine learning. Can\\nmodify a machine learning\\nalgorithm or architecture to\\nimprove performance.\\nData Scientist / Analyst\\nHundreds of thousands\\nCan analyze big data, detect\\ntrend and correlations using\\nmachine learning. Can train\\nmachine learning models on\\nexisting values to extract\\nvalue for a business.\\nDomain expert\\nTens of millions\\nUnderstands the semantics of\\na problem. Can provide\\nexamples and counter\\nexamples, and explain the\\ndiﬀerence between them.\\nbe updated or re-collected. Examples that have been\\nlabeled by teachers, however, are kept forever because\\nlabels always retain some semantic value.\\nDistribution robustness\\nThe assumption that the\\ntraining distribution matches the sampling or deploy-\\nment distribution is unrealistic in practice. The role\\nof the teacher is to create a model that is correct for\\nany example, regardless of the deployment distribu-\\ntion. Given our assumption of feature completeness\\nand a rich and diverse sampling set, the result of a suc-\\ncessful teaching process should be robust to not know-\\ning the deployment distributions. Imagine program-\\nming a “Sort” function.\\nWe expect “Sort” to work\\nregardless of the distribution of the data it is sorting.\\nThanks to realizability, we have the same correctness\\nexpectation for teaching. Because the training data is\\ndiscovered and labeled for the training set in an ad hoc\\nway using ﬁltering, distribution robustness is a critical\\nassumption and we therefore favor machine learning\\nalgorithms that are robust to covariate shifts. Warn-\\ning: having a mismatch between train and sampling\\n(or deployment) distributions complicates evaluation.\\nModular development\\nDecomposition is a central\\nprinciple of both programming and machine teach-\\ning.\\nThe machine teaching process should support\\nthe modular development of concept implementation.\\nThis includes the decomposition of concepts into sub-\\nconcepts, and the use of models as features for other\\nmodels. We can achieve this by standardizing model\\nand feature interfaces. Similar to a programming in-\\ntegrated development environment (IDE), within our\\nteaching IDE, concept implementation is done through\\n“projects” that are grouped into “solutions”. Projects\\nin a solution are trained together because their retrain-\\ning can aﬀect each other. Dependencies across diﬀer-\\nent solutions are treated as versioned packages, which\\nmeans that retraining a project in one solution does\\nnot aﬀect a project in a diﬀerent solution (the teacher\\nmust update the package reference to incorporate such\\nchanges). The modular development principle encour-\\nages the sharing of explicit concept implementations.\\nVersion control\\nAll teacher actions (e.g., labels,\\nfeatures, label constraints, schema and dependency\\ngraph, and even programming code if necessary) are\\nequivalent to a concept “program”. They are saved\\nin the same “commit”. Like programming code, the\\nteacher’s actions relevant to a concept are saved in a\\nversion control system. Diﬀerent type of actions are\\nkept in diﬀerent ﬁles to facilitate merge operations be-\\ntween contributions from diﬀerent teachers.\\nThe combination of these principles suggests a teach-\\ning process that is diﬀerent from the standard teach-\\ning process. The universal teaching language implies\\nthat the machine learning expert can be left out of the\\nteaching loop.\\nThe featuring completeness principle\\nimplies that the engineers can be left out of the teach-\\ning loop as well. The teaching tool should provide the\\nteacher with all that is needed to build models eﬀec-\\ntively. The engineers can update the data pipeline and\\nthe programming language, but neither are concept-\\ndependent so the engineer is out of the teaching loop.\\nThese two principles imply that a single person with\\ndomain and teaching knowledge can own the whole\\nMachine Teaching\\nrepeat\\nwhile training set is realizable do\\nif quality criteria is met then\\nexit\\nend\\n// Actively and semantically explore sampling set using concept based ﬁlters.\\nFind a test error (i.e., an incorrectly predicted (example, label) pair);\\nAdd example to training set.;\\nend\\n// Fix training set error\\nif training error is caused by labeling error(s) then\\nCorrect labeling error(s);\\nelse\\n// Fix feature blindness. This may entail one or more of the following actions:\\nAdd or edit basic features;\\nCreate a new concept/project for a new feature (decomposition);\\nChange label constraints or schema (high level knowledge);\\nend\\nuntil forever;\\nAlgorithm 1: A machine teaching process\\nprocess. The availability of a rich and diverse sam-\\npling set means that the traditional data collection for\\nlabeling step is not part of the concept teaching pro-\\ncess. The distribution robustness principle allows the\\nteacher to explore and label freely throughout the pro-\\ncess without worrying about balancing classes or ex-\\nample types. Concept modularity and version control\\nguarantee that a function created in a project is repro-\\nducible provided that (1) all of its features are deter-\\nministic and (2) training is deterministic. The concept\\nmodularity principle enables interpretability and scal-\\ning with complexity. The interpretability comes from\\nbeing able to explain what each sub-concept does by\\nlooking at the labels, features, or schema. Even if each\\nsub-concept is a black box inside, their interfaces are\\ntransparent. The merge functionality in version con-\\ntrol enables easy collaboration between multiple teach-\\ners.\\nBased on the above, we propose a skeleton for a teach-\\ning process in Algorithm 1. Note that this process is\\nnot unique.\\nEvaluating the quality criteria in a distribution-robust\\nsetting is diﬃcult and beyond the scope of this pa-\\nper.\\nA simple criteria could be to pause when the\\nteacher’s cost or time invested reaches a given limit.\\nFinding test error eﬀectively is also diﬃcult and be-\\nyond the scope of this paper.\\nThe idea is to query\\nover the large sample set by leveraging query-speciﬁc\\nteacher-created concepts and sub-concepts. The art is\\nto maximize the semantic expressiveness of querying\\nand the diversity of results. Uncertainty sampling is\\na trivial and uninteresting case (ambiguous examples\\nare not useful for coming up with new decomposition\\nconcepts).\\nThere are a few striking diﬀerences between the teach-\\ning process above and the standard model building\\nprocess. The most important aspect is that it can be\\ndone by a single actor operating on the true distribu-\\ntion. Knowledge transfer from teacher to learner has\\nmultiple modalities (selection, labels, features, con-\\nstraints, schema). The process is a never-ending loop\\nreminiscent of Tom Mitchell’s NELL (Carlson et al.,\\n2010). Capacity is increased on demand, so there is no\\nneed for traditional regularization because the teacher\\ncontrols the capacity of the learning system by adding\\nfeatures only when necessary.\\n6. Conclusion\\nOver the past two decades, the machine learning ﬁeld\\nhas devoted most of its energy to developing and im-\\nproving learning algorithms. For problems in which\\ndata is plentiful and statistical guarantees are suﬃ-\\ncient, this approach has paid oﬀhandsomely.\\nThe\\nﬁeld is now evolving toward addressing a larger set\\nof simpler and more ephemeral problems. While the\\ndemand to solve these problems eﬀectively grows, the\\naccess to teachers that can build corresponding solu-\\ntions is limited by their scarcity and cost. To truly\\nmeet this demand, we need to advance the discipline\\nof machine teaching. This shift is identical to the shift\\nin the programming ﬁeld in the 1980s and 1990s. This\\nMachine Teaching\\nparallel yields a wealth of beneﬁts. This paper takes\\ninspiration from three lessons from the history of pro-\\ngramming. The ﬁrst one is problem decomposition and\\nmodularity. They have allowed programming to scale\\nwith complexity.\\nWe argue that a similar approach\\nhas the same beneﬁts for machine teaching. The sec-\\nond lesson is the standardization of programming lan-\\nguages: write once, run everywhere. This paper is not\\nproposing a standard machine teaching language, but\\nwe enumerated the most important machine-learning-\\nagnostic knowledge channels available to the teacher.\\nThe ﬁnal lesson is the process discipline, which in-\\ncludes separation of concerns and the building of stan-\\ndard tools and libraries. This addresses the same limi-\\ntations to productivity and scaling with the number of\\ncontributors that plagued programming (as described\\nin the ”Mythical Man Month” (Brooks Jr, 1995)). We\\nhave proposed a set of principles that lead to a better\\nteaching process discipline. Some of the tools of pro-\\ngramming, such as version control, can be used as is.\\nSome of these principles have been successfully applied\\nin services such as LUIS.ai and by product groups in-\\nside Microsoft such as Bing Local. We are in the early\\nstages of building a teaching interactive development\\nenvironment.\\nOn a more philosophical note, large monolithic sys-\\ntems, as epitomized by deep learning, are a popular\\ntrend in artiﬁcial intelligence. We see this as a form of\\nmachine learning behaviorism. It is the idea that com-\\nplex concepts can be learned from a large set of (in-\\nput, output) pairs. With the aid of regularizers and/or\\ndeep representations computed using unsupervised or\\nsemi-supervised learning, the monolithic learning ap-\\nproach has yielded impressive results. This has been\\nthe case in several ﬁelds where labeled data is abun-\\ndant (speech, vision, machine translation). The mono-\\nlithic approach, however, has limitations when labeled\\ndata is hard to come by. Deep representations built\\nfrom unlabeled data optimize where the data is. Rare\\nmisspellings that are domain speciﬁc are likely to be ig-\\nnored or misinterpreted if they appear more frequently\\nin a diﬀerent domain context. Corner cases with little\\nor no labels for autonomous-driving may be ignored\\nat great perils. Large (amorphic) models are hard to\\ninterpret. These limitations can be overcome by in-\\njecting semantic knowledge via active teaching (e.g.,\\nlabels, features, structure). For this reason, we believe\\nthat both large monolithic systems and systems more\\nactively supervised by teaching have important roles\\nto play in machine learning.\\nAs a bonus, they can\\neasily be combined to complement each other.\\n7. Acknowledgements\\nWe thank Jason Williams for his support and contri-\\nbutions to Machine Teaching.\\nJason is the creator\\nof the LUIS (Language Understanding Internet Ser-\\nvices) project, a service for building language under-\\nstanding models, based on the principles mentioned\\nin this paper.\\nWe thank Riham Mansour and the\\nMicrosoft Cairo team for co-building, maintaining,\\nand improving the www.LUIS.ai service. Finally, we\\nthank Matthew Hurst and his team for building high-\\nperforming web page entity extractors leveraging our\\nmachine teaching tools.\\nThese entity extractors are\\ndeployed in Bing Local.\\nReferences\\nYoshua Bengio, J´erˆome Louradour, Ronan Collobert,\\nand Jason Weston. 2009. Curriculum Learning. In\\nProceedings of the 26th Annual International Con-\\nference on Machine Learning (ICML ’09). ACM,\\nNew York, NY, USA, 41–48.\\nhttps://doi.org/\\n10.1145/1553374.1553380\\nFrederick P Brooks Jr. 1995.\\nThe Mythical Man-\\nMonth: Essays on Software Engineering, Anniver-\\nsary Edition, 2/E. Pearson Education India.\\nAndrew Carlson, Justin Betteridge, Bryan Kisiel,\\nBurr\\nSettles,\\nEstevam\\nR.\\nHruschka,\\nJr.,\\nand\\nTom M. Mitchell. 2010.\\nToward an Architec-\\nture for Never-ending Language Learning. In Pro-\\nceedings of the Twenty-Fourth AAAI Conference\\non Artiﬁcial Intelligence (AAAI’10). AAAI Press,\\n1306–1313.\\nhttp://dl.acm.org/citation.cfm?\\nid=2898607.2898816\\nTodd Kulesza,\\nSaleema Amershi,\\nRich Caruana,\\nDanyel Fisher, and Denis Charles. 2014. Structured\\nlabeling for facilitating concept evolution in machine\\nlearning. In Proceedings of the SIGCHI Conference\\non Human Factors in Computing Systems. ACM,\\n3075–3084.\\nD. Sculley, Gary Holt, Daniel Golovin, Eugene Davy-\\ndov, Todd Phillips, Dietmar Ebner, Vinay Chaud-\\nhary, and Michael Young. 2014. Machine Learning:\\nThe High Interest Credit Card of Technical Debt. In\\nSE4ML: Software Engineering for Machine Learn-\\ning (NIPS 2014 Workshop).\\nVladimir Vapnik. 2013.\\nThe Nature of Statistical\\nLearning Theory. Springer science & business me-\\ndia.\\n'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('all_papers.jsonl', 'r') as file:\n",
    "    idx=0\n",
    "    for line_num, line in enumerate(file):\n",
    "        idx+=1\n",
    "        if idx==279:\n",
    "            document = json.loads(line.strip())\n",
    "            print(document)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407fb44f-e333-4d9c-be83-d59d2189eca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72234a-5d25-4e78-b68a-2c6d0bdeec33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30ad62e2-0fc3-4500-9a06-55abe26ee6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 documents done.\n",
      "20000 documents done.\n",
      "30000 documents done.\n",
      "40000 documents done.\n",
      "50000 documents done.\n",
      "60000 documents done.\n",
      "70000 documents done.\n",
      "80000 documents done.\n",
      "90000 documents done.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "docid_title_map = {}\n",
    "with open('all_papers.jsonl', 'r') as file:\n",
    "    idx=0\n",
    "    for line_num, line in enumerate(file):\n",
    "        idx+=1\n",
    "        # if idx>90643:\n",
    "        document = json.loads(line.strip())\n",
    "        t = document.get('title', '')\n",
    "        # print(isinstance(a, list))\n",
    "        # if isinstance(a, list):\n",
    "        docid_title_map[idx] = t\n",
    "        # else:\n",
    "        #     docid_title_map[idx] = [author.lower() for author in ast.literal_eval(a)]\n",
    "        if idx%10000 == 0:\n",
    "            print(f'{idx} documents done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbe7dff8-2493-48ae-b430-5d62baa3f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('docid_title_map.pkl', 'wb') as file:\n",
    "    pickle.dump(docid_title_map, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e695dcc-63af-40db-b7fd-c56d9c2ae05e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "499537d8-6bd8-43db-967a-26246d750c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 documents done.\n",
      "20000 documents done.\n",
      "30000 documents done.\n",
      "40000 documents done.\n",
      "50000 documents done.\n",
      "60000 documents done.\n",
      "70000 documents done.\n",
      "80000 documents done.\n",
      "90000 documents done.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "docid_abstract_map = {}\n",
    "with open('all_papers.jsonl', 'r') as file:\n",
    "    idx=0\n",
    "    for line_num, line in enumerate(file):\n",
    "        idx+=1\n",
    "        # if idx>90643:\n",
    "        document = json.loads(line.strip())\n",
    "        if document.get('abstract', '') != '':\n",
    "            a = document.get('abstract', '')\n",
    "        else:\n",
    "            a = document.get('summary', '')\n",
    "        # print(isinstance(a, list))\n",
    "        # if isinstance(a, list):\n",
    "        docid_abstract_map[idx] = a\n",
    "        # else:\n",
    "        #     docid_title_map[idx] = [author.lower() for author in ast.literal_eval(a)]\n",
    "        if idx%10000 == 0:\n",
    "            print(f'{idx} documents done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d680d746-be4b-4c80-8d56-fe149b463c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('docid_abstract_map.pkl', 'wb') as file:\n",
    "    pickle.dump(docid_abstract_map, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e76804-9da9-4c5f-9f02-ea6869eca6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cf09308-2e0f-4b87-aae4-5810270e95f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 documents done.\n",
      "20000 documents done.\n",
      "30000 documents done.\n",
      "40000 documents done.\n",
      "50000 documents done.\n",
      "60000 documents done.\n",
      "70000 documents done.\n",
      "80000 documents done.\n",
      "90000 documents done.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "docid_link_map = {}\n",
    "with open('all_papers.jsonl', 'r') as file:\n",
    "    idx=0\n",
    "    for line_num, line in enumerate(file):\n",
    "        idx+=1\n",
    "        # if idx>90643:\n",
    "        document = json.loads(line.strip())\n",
    "        if document.get('global_link_openable', '') != '':\n",
    "            l = document.get('global_link_openable')\n",
    "        else:\n",
    "            l = 'https://arxiv.org/abs/'+document.get('arxiv_id')\n",
    "        # print(isinstance(a, list))\n",
    "        # if isinstance(a, list):\n",
    "        docid_link_map[idx] = l\n",
    "        # else:\n",
    "        #     docid_title_map[idx] = [author.lower() for author in ast.literal_eval(a)]\n",
    "        if idx%10000 == 0:\n",
    "            print(f'{idx} documents done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c8878a4-f150-4c7e-8585-85e6975bb1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'https://arxiv.org/abs/1909.03550v1',\n",
       " 2: 'https://arxiv.org/abs/1811.04422v1',\n",
       " 3: 'https://arxiv.org/abs/1707.04849v1',\n",
       " 4: 'https://arxiv.org/abs/1909.09246v1',\n",
       " 5: 'https://arxiv.org/abs/2301.09753v1',\n",
       " 6: 'https://arxiv.org/abs/0904.3664v1',\n",
       " 7: 'https://arxiv.org/abs/2012.04105v1',\n",
       " 8: 'https://arxiv.org/abs/1911.06612v1',\n",
       " 9: 'https://arxiv.org/abs/1909.01866v1',\n",
       " 10: 'https://arxiv.org/abs/1903.08801v1',\n",
       " 11: 'https://arxiv.org/abs/2108.07915v1',\n",
       " 12: 'https://arxiv.org/abs/1707.09562v3',\n",
       " 13: 'https://arxiv.org/abs/1907.08908v1',\n",
       " 14: 'https://arxiv.org/abs/2312.03120v1',\n",
       " 15: 'https://arxiv.org/abs/1507.02188v1',\n",
       " 16: 'https://arxiv.org/abs/1212.2686v1',\n",
       " 17: 'https://arxiv.org/abs/2001.04942v2',\n",
       " 18: 'https://arxiv.org/abs/2007.01503v1',\n",
       " 19: 'https://arxiv.org/abs/1906.06821v2',\n",
       " 20: 'https://arxiv.org/abs/1911.00776v1',\n",
       " 21: 'https://arxiv.org/abs/2011.11819v1',\n",
       " 22: 'https://arxiv.org/abs/2004.00993v2',\n",
       " 23: 'https://arxiv.org/abs/2009.11087v1',\n",
       " 24: 'https://arxiv.org/abs/2303.18087v1',\n",
       " 25: 'https://arxiv.org/abs/2401.11351v2',\n",
       " 26: 'https://arxiv.org/abs/2003.05155v2',\n",
       " 27: 'https://arxiv.org/abs/1706.08001v1',\n",
       " 28: 'https://arxiv.org/abs/2405.03720v1',\n",
       " 29: 'https://arxiv.org/abs/2007.05479v1',\n",
       " 30: 'https://arxiv.org/abs/2007.14206v1',\n",
       " 31: 'https://arxiv.org/abs/1603.02185v1',\n",
       " 32: 'https://arxiv.org/abs/1910.12387v2',\n",
       " 33: 'https://arxiv.org/abs/1908.04710v3',\n",
       " 34: 'https://arxiv.org/abs/1509.00913v3',\n",
       " 35: 'https://arxiv.org/abs/2202.10564v1',\n",
       " 36: 'https://arxiv.org/abs/2407.05526v1',\n",
       " 37: 'https://arxiv.org/abs/2001.09608v1',\n",
       " 38: 'https://arxiv.org/abs/2110.12773v1',\n",
       " 39: 'https://arxiv.org/abs/1510.00633v1',\n",
       " 40: 'https://arxiv.org/abs/1802.03830v1',\n",
       " 41: 'https://arxiv.org/abs/2106.07032v1',\n",
       " 42: 'https://arxiv.org/abs/1808.00033v3',\n",
       " 43: 'https://arxiv.org/abs/1702.08608v2',\n",
       " 44: 'https://arxiv.org/abs/1705.07538v2',\n",
       " 45: 'https://arxiv.org/abs/1911.08587v1',\n",
       " 46: 'https://arxiv.org/abs/2007.01977v1',\n",
       " 47: 'https://arxiv.org/abs/2007.07981v1',\n",
       " 48: 'https://arxiv.org/abs/2108.08712v1',\n",
       " 49: 'https://arxiv.org/abs/1612.04858v1',\n",
       " 50: 'https://arxiv.org/abs/2212.12303v1',\n",
       " 51: 'https://arxiv.org/abs/2305.15410v1',\n",
       " 52: 'https://arxiv.org/abs/2306.14624v2',\n",
       " 53: 'https://arxiv.org/abs/2407.19890v1',\n",
       " 54: 'https://arxiv.org/abs/2102.05639v1',\n",
       " 55: 'https://arxiv.org/abs/1909.09248v1',\n",
       " 56: 'https://arxiv.org/abs/1810.03548v1',\n",
       " 57: 'https://arxiv.org/abs/2004.05366v2',\n",
       " 58: 'https://arxiv.org/abs/1711.06552v1',\n",
       " 59: 'https://arxiv.org/abs/1807.06722v2',\n",
       " 60: 'https://arxiv.org/abs/2201.06921v1',\n",
       " 61: 'https://arxiv.org/abs/1812.01410v1',\n",
       " 62: 'https://arxiv.org/abs/1707.03184v1',\n",
       " 63: 'https://arxiv.org/abs/1611.03969v1',\n",
       " 64: 'https://arxiv.org/abs/1810.11383v2',\n",
       " 65: 'https://arxiv.org/abs/2103.11249v1',\n",
       " 66: 'https://arxiv.org/abs/1612.04251v1',\n",
       " 67: 'https://arxiv.org/abs/1910.02544v1',\n",
       " 68: 'https://arxiv.org/abs/2407.05520v1',\n",
       " 69: 'https://arxiv.org/abs/1501.04309v1',\n",
       " 70: 'https://arxiv.org/abs/1602.00198v1',\n",
       " 71: 'https://arxiv.org/abs/1605.07805v2',\n",
       " 72: 'https://arxiv.org/abs/1803.10311v2',\n",
       " 73: 'https://arxiv.org/abs/2006.15680v1',\n",
       " 74: 'https://arxiv.org/abs/2105.03726v4',\n",
       " 75: 'https://arxiv.org/abs/1912.09630v1',\n",
       " 76: 'https://arxiv.org/abs/2209.02057v3',\n",
       " 77: 'https://arxiv.org/abs/2312.14050v1',\n",
       " 78: 'https://arxiv.org/abs/1902.04622v1',\n",
       " 79: 'https://arxiv.org/abs/2103.03122v1',\n",
       " 80: 'https://arxiv.org/abs/1907.03010v1',\n",
       " 81: 'https://arxiv.org/abs/2206.13446v1',\n",
       " 82: 'https://arxiv.org/abs/2310.11470v1',\n",
       " 83: 'https://arxiv.org/abs/2011.03733v1',\n",
       " 84: 'https://arxiv.org/abs/2304.01316v1',\n",
       " 85: 'https://arxiv.org/abs/1302.0406v1',\n",
       " 86: 'https://arxiv.org/abs/1708.07826v1',\n",
       " 87: 'https://arxiv.org/abs/2009.06410v2',\n",
       " 88: 'https://arxiv.org/abs/2106.03015v1',\n",
       " 89: 'https://arxiv.org/abs/2106.09756v1',\n",
       " 90: 'https://arxiv.org/abs/1904.03259v1',\n",
       " 91: 'https://arxiv.org/abs/2202.13608v2',\n",
       " 92: 'https://arxiv.org/abs/2403.02432v1',\n",
       " 93: 'https://arxiv.org/abs/1612.07640v1',\n",
       " 94: 'https://arxiv.org/abs/1904.00001v2',\n",
       " 95: 'https://arxiv.org/abs/2201.01288v2',\n",
       " 96: 'https://arxiv.org/abs/1711.01431v1',\n",
       " 97: 'https://arxiv.org/abs/1907.07543v1',\n",
       " 98: 'https://arxiv.org/abs/2107.11277v3',\n",
       " 99: 'https://arxiv.org/abs/2205.14136v1',\n",
       " 100: 'https://arxiv.org/abs/2405.16159v1',\n",
       " 101: 'https://arxiv.org/abs/2409.04923v1',\n",
       " 102: 'https://arxiv.org/abs/1303.2104v1',\n",
       " 103: 'https://arxiv.org/abs/1807.10681v1',\n",
       " 104: 'https://arxiv.org/abs/2102.11274v1',\n",
       " 105: 'https://arxiv.org/abs/1905.07822v2',\n",
       " 106: 'https://arxiv.org/abs/2305.00520v1',\n",
       " 107: 'https://arxiv.org/abs/1607.00279v1',\n",
       " 108: 'https://arxiv.org/abs/1812.10422v1',\n",
       " 109: 'https://arxiv.org/abs/1404.6674v1',\n",
       " 110: 'https://arxiv.org/abs/1805.07072v1',\n",
       " 111: 'https://arxiv.org/abs/1805.11959v2',\n",
       " 112: 'https://arxiv.org/abs/1902.06789v2',\n",
       " 113: 'https://arxiv.org/abs/1903.00092v2',\n",
       " 114: 'https://arxiv.org/abs/1911.07679v1',\n",
       " 115: 'https://arxiv.org/abs/1911.07749v1',\n",
       " 116: 'https://arxiv.org/abs/1911.12593v1',\n",
       " 117: 'https://arxiv.org/abs/2006.00700v1',\n",
       " 118: 'https://arxiv.org/abs/2008.07758v1',\n",
       " 119: 'https://arxiv.org/abs/1801.04016v1',\n",
       " 120: 'https://arxiv.org/abs/2111.04439v1',\n",
       " 121: 'https://arxiv.org/abs/2203.06430v2',\n",
       " 122: 'https://arxiv.org/abs/2203.16797v1',\n",
       " 123: 'https://arxiv.org/abs/2208.10896v2',\n",
       " 124: 'https://arxiv.org/abs/2305.10472v2',\n",
       " 125: 'https://arxiv.org/abs/2307.02071v1',\n",
       " 126: 'https://arxiv.org/abs/2408.12655v1',\n",
       " 127: 'https://arxiv.org/abs/2409.03669v1',\n",
       " 128: 'https://arxiv.org/abs/2410.10523v1',\n",
       " 129: 'https://arxiv.org/abs/1703.01977v1',\n",
       " 130: 'https://arxiv.org/abs/2001.07278v1',\n",
       " 131: 'https://arxiv.org/abs/1802.00382v1',\n",
       " 132: 'https://arxiv.org/abs/2201.04703v1',\n",
       " 133: 'https://arxiv.org/abs/2211.14401v2',\n",
       " 134: 'https://arxiv.org/abs/1807.06689v1',\n",
       " 135: 'https://arxiv.org/abs/1811.00542v1',\n",
       " 136: 'https://arxiv.org/abs/2310.10368v1',\n",
       " 137: 'https://arxiv.org/abs/2005.09428v2',\n",
       " 138: 'https://arxiv.org/abs/1706.05749v1',\n",
       " 139: 'https://arxiv.org/abs/1505.06614v1',\n",
       " 140: 'https://arxiv.org/abs/1703.10121v1',\n",
       " 141: 'https://arxiv.org/abs/1804.11238v1',\n",
       " 142: 'https://arxiv.org/abs/1504.03874v1',\n",
       " 143: 'https://arxiv.org/abs/1706.08519v1',\n",
       " 144: 'https://arxiv.org/abs/1805.04272v2',\n",
       " 145: 'https://arxiv.org/abs/1807.04162v3',\n",
       " 146: 'https://arxiv.org/abs/1807.05351v1',\n",
       " 147: 'https://arxiv.org/abs/1904.01957v1',\n",
       " 148: 'https://arxiv.org/abs/1911.09052v1',\n",
       " 149: 'https://arxiv.org/abs/2005.00478v3',\n",
       " 150: 'https://arxiv.org/abs/2006.01387v2',\n",
       " 151: 'https://arxiv.org/abs/2006.02619v1',\n",
       " 152: 'https://arxiv.org/abs/2006.07237v1',\n",
       " 153: 'https://arxiv.org/abs/2006.12270v1',\n",
       " 154: 'https://arxiv.org/abs/2103.00366v2',\n",
       " 155: 'https://arxiv.org/abs/1606.02767v2',\n",
       " 156: 'https://arxiv.org/abs/1606.05386v1',\n",
       " 157: 'https://arxiv.org/abs/1811.04871v1',\n",
       " 158: 'https://arxiv.org/abs/1811.11669v1',\n",
       " 159: 'https://arxiv.org/abs/1912.07323v1',\n",
       " 160: 'https://arxiv.org/abs/2011.04328v1',\n",
       " 161: 'https://arxiv.org/abs/2101.12097v1',\n",
       " 162: 'https://arxiv.org/abs/2201.12428v1',\n",
       " 163: 'https://arxiv.org/abs/2205.09488v1',\n",
       " 164: 'https://arxiv.org/abs/2207.13596v2',\n",
       " 165: 'https://arxiv.org/abs/2208.10463v1',\n",
       " 166: 'https://arxiv.org/abs/2212.13988v1',\n",
       " 167: 'https://arxiv.org/abs/2303.13735v1',\n",
       " 168: 'https://arxiv.org/abs/2403.02467v1',\n",
       " 169: 'https://arxiv.org/abs/2009.08497v1',\n",
       " 170: 'https://arxiv.org/abs/2004.11149v7',\n",
       " 171: 'https://arxiv.org/abs/1606.08531v1',\n",
       " 172: 'https://arxiv.org/abs/1705.10201v2',\n",
       " 173: 'https://arxiv.org/abs/1912.05796v1',\n",
       " 174: 'https://arxiv.org/abs/2306.16156v2',\n",
       " 175: 'https://arxiv.org/abs/1911.11374v1',\n",
       " 176: 'https://arxiv.org/abs/2002.03123v1',\n",
       " 177: 'https://arxiv.org/abs/1908.09788v1',\n",
       " 178: 'https://arxiv.org/abs/2211.02263v1',\n",
       " 179: 'https://arxiv.org/abs/2205.15104v1',\n",
       " 180: 'https://arxiv.org/abs/1506.01709v1',\n",
       " 181: 'https://arxiv.org/abs/1703.10444v1',\n",
       " 182: 'https://arxiv.org/abs/2004.13598v1',\n",
       " 183: 'https://arxiv.org/abs/1803.08118v3',\n",
       " 184: 'https://arxiv.org/abs/2009.12999v1',\n",
       " 185: 'https://arxiv.org/abs/2012.00152v1',\n",
       " 186: 'https://arxiv.org/abs/2012.15505v1',\n",
       " 187: 'https://arxiv.org/abs/2208.04707v1',\n",
       " 188: 'https://arxiv.org/abs/2303.03181v1',\n",
       " 189: 'https://arxiv.org/abs/2104.02466v2',\n",
       " 190: 'https://arxiv.org/abs/2007.09982v1',\n",
       " 191: 'https://arxiv.org/abs/1606.01487v1',\n",
       " 192: 'https://arxiv.org/abs/2010.07744v1',\n",
       " 193: 'https://arxiv.org/abs/1811.06622v1',\n",
       " 194: 'https://arxiv.org/abs/2112.12181v2',\n",
       " 195: 'https://arxiv.org/abs/2204.13291v3',\n",
       " 196: 'https://arxiv.org/abs/1611.00379v1',\n",
       " 197: 'https://arxiv.org/abs/2006.05604v1',\n",
       " 198: 'https://arxiv.org/abs/1901.03678v1',\n",
       " 199: 'https://arxiv.org/abs/2011.08450v2',\n",
       " 200: 'https://arxiv.org/abs/1803.02388v1',\n",
       " 201: 'https://arxiv.org/abs/2103.07802v1',\n",
       " 202: 'https://arxiv.org/abs/1901.05353v3',\n",
       " 203: 'https://arxiv.org/abs/2009.06093v1',\n",
       " 204: 'https://arxiv.org/abs/1103.3095v1',\n",
       " 205: 'https://arxiv.org/abs/1204.2477v1',\n",
       " 206: 'https://arxiv.org/abs/1212.3900v2',\n",
       " 207: 'https://arxiv.org/abs/1408.6618v1',\n",
       " 208: 'https://arxiv.org/abs/1502.02704v1',\n",
       " 209: 'https://arxiv.org/abs/1803.06586v1',\n",
       " 210: 'https://arxiv.org/abs/1809.07904v2',\n",
       " 211: 'https://arxiv.org/abs/2012.03130v1',\n",
       " 212: 'https://arxiv.org/abs/1805.07938v1',\n",
       " 213: 'https://arxiv.org/abs/1610.06072v1',\n",
       " 214: 'https://arxiv.org/abs/2008.01171v1',\n",
       " 215: 'https://arxiv.org/abs/2008.07739v1',\n",
       " 216: 'https://arxiv.org/abs/1706.00066v1',\n",
       " 217: 'https://arxiv.org/abs/1904.09644v2',\n",
       " 218: 'https://arxiv.org/abs/1502.02127v2',\n",
       " 219: 'https://arxiv.org/abs/1511.03198v1',\n",
       " 220: 'https://arxiv.org/abs/1511.03643v3',\n",
       " 221: 'https://arxiv.org/abs/1510.00772v1',\n",
       " 222: 'https://arxiv.org/abs/1909.13316v1',\n",
       " 223: 'https://arxiv.org/abs/2001.04601v1',\n",
       " 224: 'https://arxiv.org/abs/2011.04890v1',\n",
       " 225: 'https://arxiv.org/abs/2106.05466v2',\n",
       " 226: 'https://arxiv.org/abs/2306.09624v1',\n",
       " 227: 'https://arxiv.org/abs/2309.02532v1',\n",
       " 228: 'https://arxiv.org/abs/2403.10175v2',\n",
       " 229: 'https://arxiv.org/abs/2405.20620v1',\n",
       " 230: 'https://arxiv.org/abs/2409.09537v1',\n",
       " 231: 'https://arxiv.org/abs/2107.11921v4',\n",
       " 232: 'https://arxiv.org/abs/1904.05061v2',\n",
       " 233: 'https://arxiv.org/abs/1711.02038v1',\n",
       " 234: 'https://arxiv.org/abs/1812.03057v1',\n",
       " 235: 'https://arxiv.org/abs/1805.05409v2',\n",
       " 236: 'https://arxiv.org/abs/1904.05961v3',\n",
       " 237: 'https://arxiv.org/abs/2006.09271v2',\n",
       " 238: 'https://arxiv.org/abs/2006.16789v2',\n",
       " 239: 'https://arxiv.org/abs/1907.00678v1',\n",
       " 240: 'https://arxiv.org/abs/1802.05351v3',\n",
       " 241: 'https://arxiv.org/abs/2009.04661v1',\n",
       " 242: 'https://arxiv.org/abs/2112.01998v1',\n",
       " 243: 'https://arxiv.org/abs/2211.10708v1',\n",
       " 244: 'https://arxiv.org/abs/2302.02926v2',\n",
       " 245: 'https://arxiv.org/abs/2306.04646v1',\n",
       " 246: 'https://arxiv.org/abs/2402.14694v1',\n",
       " 247: 'https://arxiv.org/abs/1910.07012v1',\n",
       " 248: 'https://arxiv.org/abs/2404.19370v1',\n",
       " 249: 'https://arxiv.org/abs/1711.04708v1',\n",
       " 250: 'https://arxiv.org/abs/1911.02621v3',\n",
       " 251: 'https://arxiv.org/abs/1811.01315v2',\n",
       " 252: 'https://arxiv.org/abs/2107.01238v1',\n",
       " 253: 'https://arxiv.org/abs/2208.02030v1',\n",
       " 254: 'https://arxiv.org/abs/2107.02378v3',\n",
       " 255: 'https://arxiv.org/abs/2004.11694v1',\n",
       " 256: 'https://arxiv.org/abs/1607.01354v1',\n",
       " 257: 'https://arxiv.org/abs/2002.10619v2',\n",
       " 258: 'https://arxiv.org/abs/2202.03070v1',\n",
       " 259: 'https://arxiv.org/abs/1801.07883v2',\n",
       " 260: 'https://arxiv.org/abs/1802.08250v2',\n",
       " 261: 'https://arxiv.org/abs/1809.02591v2',\n",
       " 262: 'https://arxiv.org/abs/2111.12867v1',\n",
       " 263: 'https://arxiv.org/abs/2401.16407v1',\n",
       " 264: 'https://arxiv.org/abs/1804.00709v1',\n",
       " 265: 'https://arxiv.org/abs/1705.02908v1',\n",
       " 266: 'https://arxiv.org/abs/2010.04430v1',\n",
       " 267: 'https://arxiv.org/abs/2307.05232v2',\n",
       " 268: 'https://arxiv.org/abs/1210.8353v1',\n",
       " 269: 'https://arxiv.org/abs/1409.4044v1',\n",
       " 270: 'https://arxiv.org/abs/1903.05202v2',\n",
       " 271: 'https://arxiv.org/abs/1909.13340v2',\n",
       " 272: 'https://arxiv.org/abs/2010.01040v1',\n",
       " 273: 'https://arxiv.org/abs/1801.06275v1',\n",
       " 274: 'https://arxiv.org/abs/1910.01612v1',\n",
       " 275: 'https://arxiv.org/abs/2104.05569v1',\n",
       " 276: 'https://arxiv.org/abs/2211.04254v2',\n",
       " 277: 'https://arxiv.org/abs/2305.07801v1',\n",
       " 278: 'https://arxiv.org/abs/2409.12100v1',\n",
       " 279: 'https://arxiv.org/abs/1707.06742v3',\n",
       " 280: 'https://arxiv.org/abs/1811.03392v1',\n",
       " 281: 'https://arxiv.org/abs/2012.04863v2',\n",
       " 282: 'https://arxiv.org/abs/2002.05518v1',\n",
       " 283: 'https://arxiv.org/abs/1802.01034v1',\n",
       " 284: 'https://arxiv.org/abs/1806.02865v1',\n",
       " 285: 'https://arxiv.org/abs/1402.3382v1',\n",
       " 286: 'https://arxiv.org/abs/1908.10714v1',\n",
       " 287: 'https://arxiv.org/abs/2112.01088v1',\n",
       " 288: 'https://arxiv.org/abs/2009.12783v2',\n",
       " 289: 'https://arxiv.org/abs/2409.07632v1',\n",
       " 290: 'https://arxiv.org/abs/2012.03661v1',\n",
       " 291: 'https://arxiv.org/abs/0508073v1',\n",
       " 292: 'https://arxiv.org/abs/1204.4294v1',\n",
       " 293: 'https://arxiv.org/abs/1906.10025v2',\n",
       " 294: 'https://arxiv.org/abs/1707.00797v1',\n",
       " 295: 'https://arxiv.org/abs/2204.07697v1',\n",
       " 296: 'https://arxiv.org/abs/1905.07187v1',\n",
       " 297: 'https://arxiv.org/abs/1803.06401v1',\n",
       " 298: 'https://arxiv.org/abs/2105.03684v2',\n",
       " 299: 'https://arxiv.org/abs/1301.1575v1',\n",
       " 300: 'https://arxiv.org/abs/1301.5088v1',\n",
       " 301: 'https://arxiv.org/abs/1310.5738v1',\n",
       " 302: 'https://arxiv.org/abs/1408.4072v1',\n",
       " 303: 'https://arxiv.org/abs/1607.08878v1',\n",
       " 304: 'https://arxiv.org/abs/1804.00403v1',\n",
       " 305: 'https://arxiv.org/abs/1808.09856v1',\n",
       " 306: 'https://arxiv.org/abs/1807.00297v1',\n",
       " 307: 'https://arxiv.org/abs/1807.06228v1',\n",
       " 308: 'https://arxiv.org/abs/1902.02322v1',\n",
       " 309: 'https://arxiv.org/abs/1903.07167v1',\n",
       " 310: 'https://arxiv.org/abs/1904.12054v5',\n",
       " 311: 'https://arxiv.org/abs/1911.11920v1',\n",
       " 312: 'https://arxiv.org/abs/2005.08946v1',\n",
       " 313: 'https://arxiv.org/abs/2006.13488v1',\n",
       " 314: 'https://arxiv.org/abs/2006.16189v4',\n",
       " 315: 'https://arxiv.org/abs/2002.05193v2',\n",
       " 316: 'https://arxiv.org/abs/2002.05432v1',\n",
       " 317: 'https://arxiv.org/abs/2003.06795v1',\n",
       " 318: 'https://arxiv.org/abs/1811.05266v1',\n",
       " 319: 'https://arxiv.org/abs/1811.10455v1',\n",
       " 320: 'https://arxiv.org/abs/2104.10201v2',\n",
       " 321: 'https://arxiv.org/abs/2106.02964v1',\n",
       " 322: 'https://arxiv.org/abs/2107.04851v1',\n",
       " 323: 'https://arxiv.org/abs/2201.05216v2',\n",
       " 324: 'https://arxiv.org/abs/2203.16569v1',\n",
       " 325: 'https://arxiv.org/abs/2205.05910v1',\n",
       " 326: 'https://arxiv.org/abs/2209.09362v1',\n",
       " 327: 'https://arxiv.org/abs/2306.08933v1',\n",
       " 328: 'https://arxiv.org/abs/2311.07126v1',\n",
       " 329: 'https://arxiv.org/abs/2405.15950v2',\n",
       " 330: 'https://arxiv.org/abs/2407.03595v1',\n",
       " 331: 'https://arxiv.org/abs/2407.18735v1',\n",
       " 332: 'https://arxiv.org/abs/2408.13556v1',\n",
       " 333: 'https://arxiv.org/abs/2409.04365v1',\n",
       " 334: 'https://arxiv.org/abs/2409.06938v1',\n",
       " 335: 'https://arxiv.org/abs/1709.03854v1',\n",
       " 336: 'https://arxiv.org/abs/1703.02910v1',\n",
       " 337: 'https://arxiv.org/abs/2002.09571v2',\n",
       " 338: 'https://arxiv.org/abs/1310.8320v1',\n",
       " 339: 'https://arxiv.org/abs/1406.3726v1',\n",
       " 340: 'https://arxiv.org/abs/2106.12417v2',\n",
       " 341: 'https://arxiv.org/abs/2202.02896v1',\n",
       " 342: 'https://arxiv.org/abs/2301.11257v1',\n",
       " 343: 'https://arxiv.org/abs/1611.07567v1',\n",
       " 344: 'https://arxiv.org/abs/1609.02664v1',\n",
       " 345: 'https://arxiv.org/abs/1911.00108v2',\n",
       " 346: 'https://arxiv.org/abs/2105.13448v2',\n",
       " 347: 'https://arxiv.org/abs/2204.13625v1',\n",
       " 348: 'https://arxiv.org/abs/2009.09723v1',\n",
       " 349: 'https://arxiv.org/abs/2206.00423v2',\n",
       " 350: 'https://arxiv.org/abs/2301.03595v1',\n",
       " 351: 'https://arxiv.org/abs/2402.02637v2',\n",
       " 352: 'https://arxiv.org/abs/1308.3750v1',\n",
       " 353: 'https://arxiv.org/abs/2208.04365v1',\n",
       " 354: 'https://arxiv.org/abs/2209.13963v1',\n",
       " 355: 'https://arxiv.org/abs/2408.16620v1',\n",
       " 356: 'https://arxiv.org/abs/2004.11794v2',\n",
       " 357: 'https://arxiv.org/abs/2002.04317v4',\n",
       " 358: 'https://arxiv.org/abs/0911.1386v1',\n",
       " 359: 'https://arxiv.org/abs/1202.6548v2',\n",
       " 360: 'https://arxiv.org/abs/1206.4656v1',\n",
       " 361: 'https://arxiv.org/abs/1308.4214v1',\n",
       " 362: 'https://arxiv.org/abs/1506.04776v1',\n",
       " 363: 'https://arxiv.org/abs/1601.03642v1',\n",
       " 364: 'https://arxiv.org/abs/1708.04680v1',\n",
       " 365: 'https://arxiv.org/abs/1804.01382v1',\n",
       " 366: 'https://arxiv.org/abs/2004.04686v1',\n",
       " 367: 'https://arxiv.org/abs/1906.01279v1',\n",
       " 368: 'https://arxiv.org/abs/1906.10742v2',\n",
       " 369: 'https://arxiv.org/abs/1906.11899v1',\n",
       " 370: 'https://arxiv.org/abs/0904.3667v1',\n",
       " 371: 'https://arxiv.org/abs/1710.08464v6',\n",
       " 372: 'https://arxiv.org/abs/1803.04193v1',\n",
       " 373: 'https://arxiv.org/abs/1805.03441v1',\n",
       " 374: 'https://arxiv.org/abs/1805.08239v2',\n",
       " 375: 'https://arxiv.org/abs/1807.03200v2',\n",
       " 376: 'https://arxiv.org/abs/1807.06574v1',\n",
       " 377: 'https://arxiv.org/abs/1807.08655v1',\n",
       " 378: 'https://arxiv.org/abs/1810.04491v1',\n",
       " 379: 'https://arxiv.org/abs/1903.08356v1',\n",
       " 380: 'https://arxiv.org/abs/1911.02455v1',\n",
       " 381: 'https://arxiv.org/abs/1911.04787v1',\n",
       " 382: 'https://arxiv.org/abs/2005.07534v1',\n",
       " 383: 'https://arxiv.org/abs/2005.11313v1',\n",
       " 384: 'https://arxiv.org/abs/2007.06299v1',\n",
       " 385: 'https://arxiv.org/abs/2006.14755v2',\n",
       " 386: 'https://arxiv.org/abs/2008.05906v2',\n",
       " 387: 'https://arxiv.org/abs/2008.07278v1',\n",
       " 388: 'https://arxiv.org/abs/2105.01407v1',\n",
       " 389: 'https://arxiv.org/abs/2105.06314v1',\n",
       " 390: 'https://arxiv.org/abs/1403.0745v1',\n",
       " 391: 'https://arxiv.org/abs/1606.01042v1',\n",
       " 392: 'https://arxiv.org/abs/1606.05685v2',\n",
       " 393: 'https://arxiv.org/abs/1901.09323v1',\n",
       " 394: 'https://arxiv.org/abs/1907.05297v1',\n",
       " 395: 'https://arxiv.org/abs/1907.09764v1',\n",
       " 396: 'https://arxiv.org/abs/2001.06597v1',\n",
       " 397: 'https://arxiv.org/abs/2002.11631v2',\n",
       " 398: 'https://arxiv.org/abs/2202.02414v2',\n",
       " 399: 'https://arxiv.org/abs/1612.05740v1',\n",
       " 400: 'https://arxiv.org/abs/1802.03532v2',\n",
       " 401: 'https://arxiv.org/abs/1811.04548v1',\n",
       " 402: 'https://arxiv.org/abs/1905.01330v1',\n",
       " 403: 'https://arxiv.org/abs/1910.08842v1',\n",
       " 404: 'https://arxiv.org/abs/1910.13376v2',\n",
       " 405: 'https://arxiv.org/abs/1910.13827v1',\n",
       " 406: 'https://arxiv.org/abs/2012.05940v1',\n",
       " 407: 'https://arxiv.org/abs/2101.06986v2',\n",
       " 408: 'https://arxiv.org/abs/2101.08119v1',\n",
       " 409: 'https://arxiv.org/abs/2107.05598v1',\n",
       " 410: 'https://arxiv.org/abs/2107.12156v1',\n",
       " 411: 'https://arxiv.org/abs/2109.02496v1',\n",
       " 412: 'https://arxiv.org/abs/2109.14376v1',\n",
       " 413: 'https://arxiv.org/abs/2112.04807v1',\n",
       " 414: 'https://arxiv.org/abs/2112.05554v1',\n",
       " 415: 'https://arxiv.org/abs/2205.05279v1',\n",
       " 416: 'https://arxiv.org/abs/2301.05451v1',\n",
       " 417: 'https://arxiv.org/abs/2301.12986v1',\n",
       " 418: 'https://arxiv.org/abs/2302.00105v2',\n",
       " 419: 'https://arxiv.org/abs/2307.02693v1',\n",
       " 420: 'https://arxiv.org/abs/2308.01946v1',\n",
       " 421: 'https://arxiv.org/abs/2309.00767v1',\n",
       " 422: 'https://arxiv.org/abs/2311.06633v1',\n",
       " 423: 'https://arxiv.org/abs/2402.06287v1',\n",
       " 424: 'https://arxiv.org/abs/2404.03082v2',\n",
       " 425: 'https://arxiv.org/abs/2405.08793v1',\n",
       " 426: 'https://arxiv.org/abs/2406.00073v1',\n",
       " 427: 'https://arxiv.org/abs/2406.11151v2',\n",
       " 428: 'https://arxiv.org/abs/2407.06212v1',\n",
       " 429: 'https://arxiv.org/abs/2409.19011v1',\n",
       " 430: 'https://arxiv.org/abs/2410.10908v1',\n",
       " 431: 'https://arxiv.org/abs/2005.08238v1',\n",
       " 432: 'https://arxiv.org/abs/1802.07426v3',\n",
       " 433: 'https://arxiv.org/abs/2312.03057v1',\n",
       " 434: 'https://arxiv.org/abs/2409.07679v1',\n",
       " 435: 'https://arxiv.org/abs/1807.06054v1',\n",
       " 436: 'https://arxiv.org/abs/1008.1643v2',\n",
       " 437: 'https://arxiv.org/abs/1607.06988v1',\n",
       " 438: 'https://arxiv.org/abs/1906.01432v1',\n",
       " 439: 'https://arxiv.org/abs/1906.08615v1',\n",
       " 440: 'https://arxiv.org/abs/1807.05748v2',\n",
       " 441: 'https://arxiv.org/abs/2010.01011v1',\n",
       " 442: 'https://arxiv.org/abs/1809.06995v1',\n",
       " 443: 'https://arxiv.org/abs/1905.10817v1',\n",
       " 444: 'https://arxiv.org/abs/2011.01307v1',\n",
       " 445: 'https://arxiv.org/abs/2107.03770v1',\n",
       " 446: 'https://arxiv.org/abs/2006.11014v1',\n",
       " 447: 'https://arxiv.org/abs/2212.11808v1',\n",
       " 448: 'https://arxiv.org/abs/2303.05910v2',\n",
       " 449: 'https://arxiv.org/abs/2312.05717v1',\n",
       " 450: 'https://arxiv.org/abs/1406.3100v1',\n",
       " 451: 'https://arxiv.org/abs/1708.03074v2',\n",
       " 452: 'https://arxiv.org/abs/1804.02543v1',\n",
       " 453: 'https://arxiv.org/abs/1906.08068v1',\n",
       " 454: 'https://arxiv.org/abs/1706.09557v1',\n",
       " 455: 'https://arxiv.org/abs/1803.00156v1',\n",
       " 456: 'https://arxiv.org/abs/2204.13916v1',\n",
       " 457: 'https://arxiv.org/abs/2003.08051v3',\n",
       " 458: 'https://arxiv.org/abs/2208.11236v4',\n",
       " 459: 'https://arxiv.org/abs/2301.01231v1',\n",
       " 460: 'https://arxiv.org/abs/2301.11658v1',\n",
       " 461: 'https://arxiv.org/abs/2305.19304v1',\n",
       " 462: 'https://arxiv.org/abs/2409.03667v1',\n",
       " 463: 'https://arxiv.org/abs/1001.2709v1',\n",
       " 464: 'https://arxiv.org/abs/2006.02986v1',\n",
       " 465: 'https://arxiv.org/abs/2103.07268v1',\n",
       " 466: 'https://arxiv.org/abs/1901.02046v3',\n",
       " 467: 'https://arxiv.org/abs/2011.01464v2',\n",
       " 468: 'https://arxiv.org/abs/2405.07317v1',\n",
       " 469: 'https://arxiv.org/abs/1502.05767v4',\n",
       " 470: 'https://arxiv.org/abs/2104.00871v2',\n",
       " 471: 'https://arxiv.org/abs/2109.00984v2',\n",
       " 472: 'https://arxiv.org/abs/2111.05628v2',\n",
       " 473: 'https://arxiv.org/abs/2201.12465v2',\n",
       " 474: 'https://arxiv.org/abs/2308.01475v1',\n",
       " 475: 'https://arxiv.org/abs/2309.00805v1',\n",
       " 476: 'https://arxiv.org/abs/2409.06085v1',\n",
       " 477: 'https://arxiv.org/abs/1811.00231v3',\n",
       " 478: 'https://arxiv.org/abs/1108.1766v1',\n",
       " 479: 'https://arxiv.org/abs/1810.10612v1',\n",
       " 480: 'https://arxiv.org/abs/2008.10150v2',\n",
       " 481: 'https://arxiv.org/abs/1905.11311v4',\n",
       " 482: 'https://arxiv.org/abs/2011.01813v1',\n",
       " 483: 'https://arxiv.org/abs/2101.10509v1',\n",
       " 484: 'https://arxiv.org/abs/2104.01836v2',\n",
       " 485: 'https://arxiv.org/abs/1911.00623v2',\n",
       " 486: 'https://arxiv.org/abs/0212037v1',\n",
       " 487: 'https://arxiv.org/abs/0803.2976v2',\n",
       " 488: 'https://arxiv.org/abs/1406.2622v1',\n",
       " 489: 'https://arxiv.org/abs/1708.07644v1',\n",
       " 490: 'https://arxiv.org/abs/1711.08621v3',\n",
       " 491: 'https://arxiv.org/abs/1610.07647v2',\n",
       " 492: 'https://arxiv.org/abs/1904.11546v1',\n",
       " 493: 'https://arxiv.org/abs/2102.05242v2',\n",
       " 494: 'https://arxiv.org/abs/2102.10387v1',\n",
       " 495: 'https://arxiv.org/abs/1909.07872v1',\n",
       " 496: 'https://arxiv.org/abs/1811.02361v1',\n",
       " 497: 'https://arxiv.org/abs/2207.03757v2',\n",
       " 498: 'https://arxiv.org/abs/2306.00687v1',\n",
       " 499: 'https://arxiv.org/abs/2309.16584v3',\n",
       " 500: 'https://arxiv.org/abs/2310.11340v1',\n",
       " 501: 'https://arxiv.org/abs/2311.10203v1',\n",
       " 502: 'https://arxiv.org/abs/2401.08147v1',\n",
       " 503: 'https://arxiv.org/abs/2403.19339v1',\n",
       " 504: 'https://arxiv.org/abs/2405.15251v1',\n",
       " 505: 'https://arxiv.org/abs/2406.10843v1',\n",
       " 506: 'https://arxiv.org/abs/2007.10987v1',\n",
       " 507: 'https://arxiv.org/abs/2306.00503v1',\n",
       " 508: 'https://arxiv.org/abs/2409.05898v2',\n",
       " 509: 'https://arxiv.org/abs/2202.10335v1',\n",
       " 510: 'https://arxiv.org/abs/1908.05725v1',\n",
       " 511: 'https://arxiv.org/abs/2103.16106v1',\n",
       " 512: 'https://arxiv.org/abs/2002.11041v1',\n",
       " 513: 'https://arxiv.org/abs/1902.10028v1',\n",
       " 514: 'https://arxiv.org/abs/1904.07686v1',\n",
       " 515: 'https://arxiv.org/abs/1707.09050v1',\n",
       " 516: 'https://arxiv.org/abs/2103.02037v2',\n",
       " 517: 'https://arxiv.org/abs/2110.09304v3',\n",
       " 518: 'https://arxiv.org/abs/2407.02425v1',\n",
       " 519: 'https://arxiv.org/abs/1911.11746v1',\n",
       " 520: 'https://arxiv.org/abs/2403.16451v4',\n",
       " 521: 'https://arxiv.org/abs/2102.09340v1',\n",
       " 522: 'https://arxiv.org/abs/1905.07435v1',\n",
       " 523: 'https://arxiv.org/abs/2201.12150v1',\n",
       " 524: 'https://arxiv.org/abs/2308.13645v1',\n",
       " 525: 'https://arxiv.org/abs/2401.06452v1',\n",
       " 526: 'https://arxiv.org/abs/2408.12308v2',\n",
       " 527: 'https://arxiv.org/abs/1506.07563v1',\n",
       " 528: 'https://arxiv.org/abs/1703.00084v1',\n",
       " 529: 'https://arxiv.org/abs/1708.00754v1',\n",
       " 530: 'https://arxiv.org/abs/1808.03753v1',\n",
       " 531: 'https://arxiv.org/abs/2004.11898v1',\n",
       " 532: 'https://arxiv.org/abs/1812.11477v1',\n",
       " 533: 'https://arxiv.org/abs/1705.05355v2',\n",
       " 534: 'https://arxiv.org/abs/1904.00577v2',\n",
       " 535: 'https://arxiv.org/abs/1904.10922v1',\n",
       " 536: 'https://arxiv.org/abs/1911.02490v2',\n",
       " 537: 'https://arxiv.org/abs/2007.06849v1',\n",
       " 538: 'https://arxiv.org/abs/2103.08308v1',\n",
       " 539: 'https://arxiv.org/abs/2105.15197v3',\n",
       " 540: 'https://arxiv.org/abs/2204.00887v2',\n",
       " 541: 'https://arxiv.org/abs/1901.01686v1',\n",
       " 542: 'https://arxiv.org/abs/1907.00429v2',\n",
       " 543: 'https://arxiv.org/abs/1811.06128v2',\n",
       " 544: 'https://arxiv.org/abs/2009.05097v1',\n",
       " 545: 'https://arxiv.org/abs/2012.08156v2',\n",
       " 546: 'https://arxiv.org/abs/2012.11178v1',\n",
       " 547: 'https://arxiv.org/abs/2106.08443v1',\n",
       " 548: 'https://arxiv.org/abs/2109.09394v1',\n",
       " 549: 'https://arxiv.org/abs/2205.12797v1',\n",
       " 550: 'https://arxiv.org/abs/2305.16703v1',\n",
       " 551: 'https://arxiv.org/abs/2308.01319v1',\n",
       " 552: 'https://arxiv.org/abs/2311.03037v1',\n",
       " 553: 'https://arxiv.org/abs/2407.18227v1',\n",
       " 554: 'https://arxiv.org/abs/2004.02401v1',\n",
       " 555: 'https://arxiv.org/abs/1412.3919v1',\n",
       " 556: 'https://arxiv.org/abs/2107.10976v1',\n",
       " 557: 'https://arxiv.org/abs/2309.04478v1',\n",
       " 558: 'https://arxiv.org/abs/2002.02385v1',\n",
       " 559: 'https://arxiv.org/abs/2003.04793v1',\n",
       " 560: 'https://arxiv.org/abs/1806.09471v1',\n",
       " 561: 'https://arxiv.org/abs/1808.00418v1',\n",
       " 562: 'https://arxiv.org/abs/1808.07049v1',\n",
       " 563: 'https://arxiv.org/abs/1906.03593v2',\n",
       " 564: 'https://arxiv.org/abs/2104.14504v2',\n",
       " 565: 'https://arxiv.org/abs/2308.09766v3',\n",
       " 566: 'https://arxiv.org/abs/2002.06306v1',\n",
       " 567: 'https://arxiv.org/abs/2311.10832v1',\n",
       " 568: 'https://arxiv.org/abs/1505.06279v2',\n",
       " 569: 'https://arxiv.org/abs/1808.04572v3',\n",
       " 570: 'https://arxiv.org/abs/1706.04601v1',\n",
       " 571: 'https://arxiv.org/abs/1805.08355v1',\n",
       " 572: 'https://arxiv.org/abs/2003.04960v2',\n",
       " 573: 'https://arxiv.org/abs/2012.12899v2',\n",
       " 574: 'https://arxiv.org/abs/2208.06339v2',\n",
       " 575: 'https://arxiv.org/abs/2004.05439v2',\n",
       " 576: 'https://arxiv.org/abs/1905.12588v2',\n",
       " 577: 'https://arxiv.org/abs/2310.13085v1',\n",
       " 578: 'https://arxiv.org/abs/1906.09679v1',\n",
       " 579: 'https://arxiv.org/abs/1808.06865v1',\n",
       " 580: 'https://arxiv.org/abs/1810.07339v2',\n",
       " 581: 'https://arxiv.org/abs/1904.07248v1',\n",
       " 582: 'https://arxiv.org/abs/2008.08516v3',\n",
       " 583: 'https://arxiv.org/abs/1909.05504v1',\n",
       " 584: 'https://arxiv.org/abs/1802.03522v1',\n",
       " 585: 'https://arxiv.org/abs/1811.11400v2',\n",
       " 586: 'https://arxiv.org/abs/1910.02109v3',\n",
       " 587: 'https://arxiv.org/abs/2011.07200v1',\n",
       " 588: 'https://arxiv.org/abs/2210.02704v1',\n",
       " 589: 'https://arxiv.org/abs/2211.05225v2',\n",
       " 590: 'https://arxiv.org/abs/2307.11332v1',\n",
       " 591: 'https://arxiv.org/abs/2311.02004v1',\n",
       " 592: 'https://arxiv.org/abs/2406.01552v1',\n",
       " 593: 'https://arxiv.org/abs/2208.13386v1',\n",
       " 594: 'https://arxiv.org/abs/2211.08064v2',\n",
       " 595: 'https://arxiv.org/abs/2210.02515v1',\n",
       " 596: 'https://arxiv.org/abs/2311.13341v1',\n",
       " 597: 'https://arxiv.org/abs/1906.02664v1',\n",
       " 598: 'https://arxiv.org/abs/1802.08701v2',\n",
       " 599: 'https://arxiv.org/abs/2106.11034v1',\n",
       " 600: 'https://arxiv.org/abs/2107.00821v2',\n",
       " 601: 'https://arxiv.org/abs/2408.12097v1',\n",
       " 602: 'https://arxiv.org/abs/1112.2738v1',\n",
       " 603: 'https://arxiv.org/abs/1704.06497v2',\n",
       " 604: 'https://arxiv.org/abs/1804.02527v1',\n",
       " 605: 'https://arxiv.org/abs/1804.05806v1',\n",
       " 606: 'https://arxiv.org/abs/1806.05454v1',\n",
       " 607: 'https://arxiv.org/abs/1812.00975v1',\n",
       " 608: 'https://arxiv.org/abs/1812.02903v1',\n",
       " 609: 'https://arxiv.org/abs/1610.06434v1',\n",
       " 610: 'https://arxiv.org/abs/1610.09369v1',\n",
       " 611: 'https://arxiv.org/abs/1705.10467v2',\n",
       " 612: 'https://arxiv.org/abs/1807.07987v2',\n",
       " 613: 'https://arxiv.org/abs/1810.04114v2',\n",
       " 614: 'https://arxiv.org/abs/1902.04247v2',\n",
       " 615: 'https://arxiv.org/abs/1902.11175v2',\n",
       " 616: 'https://arxiv.org/abs/1903.05196v2',\n",
       " 617: 'https://arxiv.org/abs/1903.07378v1',\n",
       " 618: 'https://arxiv.org/abs/1911.03951v1',\n",
       " 619: 'https://arxiv.org/abs/1909.01651v1',\n",
       " 620: 'https://arxiv.org/abs/2002.12755v1',\n",
       " 621: 'https://arxiv.org/abs/2003.07339v1',\n",
       " 622: 'https://arxiv.org/abs/2003.08673v1',\n",
       " 623: 'https://arxiv.org/abs/1905.10108v1',\n",
       " 624: 'https://arxiv.org/abs/2011.11660v3',\n",
       " 625: 'https://arxiv.org/abs/2101.05428v1',\n",
       " 626: 'https://arxiv.org/abs/2205.02453v1',\n",
       " 627: 'https://arxiv.org/abs/2206.02423v1',\n",
       " 628: 'https://arxiv.org/abs/2305.14606v1',\n",
       " 629: 'https://arxiv.org/abs/2404.16180v1',\n",
       " 630: 'https://arxiv.org/abs/2409.14235v1',\n",
       " 631: 'https://arxiv.org/abs/1102.2467v1',\n",
       " 632: 'https://arxiv.org/abs/1303.5976v1',\n",
       " 633: 'https://arxiv.org/abs/1410.0440v1',\n",
       " 634: 'https://arxiv.org/abs/1506.02510v1',\n",
       " 635: 'https://arxiv.org/abs/1605.00251v1',\n",
       " 636: 'https://arxiv.org/abs/1806.06798v2',\n",
       " 637: 'https://arxiv.org/abs/1808.00423v1',\n",
       " 638: 'https://arxiv.org/abs/1906.02101v2',\n",
       " 639: 'https://arxiv.org/abs/1504.08215v1',\n",
       " 640: 'https://arxiv.org/abs/1701.05487v1',\n",
       " 641: 'https://arxiv.org/abs/1702.08553v2',\n",
       " 642: 'https://arxiv.org/abs/1805.01907v2',\n",
       " 643: 'https://arxiv.org/abs/1807.06540v1',\n",
       " 644: 'https://arxiv.org/abs/1902.05017v1',\n",
       " 645: 'https://arxiv.org/abs/1904.09948v1',\n",
       " 646: 'https://arxiv.org/abs/1908.01394v1',\n",
       " 647: 'https://arxiv.org/abs/2105.04093v1',\n",
       " 648: 'https://arxiv.org/abs/2002.01182v1',\n",
       " 649: 'https://arxiv.org/abs/1603.04882v1',\n",
       " 650: 'https://arxiv.org/abs/1612.07993v1',\n",
       " 651: 'https://arxiv.org/abs/1811.00200v1',\n",
       " 652: 'https://arxiv.org/abs/2302.07263v1',\n",
       " 653: 'https://arxiv.org/abs/2310.06306v2',\n",
       " 654: 'https://arxiv.org/abs/1703.00994v1',\n",
       " 655: 'https://arxiv.org/abs/1812.00564v1',\n",
       " 656: 'https://arxiv.org/abs/1710.07991v1',\n",
       " 657: 'https://arxiv.org/abs/1805.12369v1',\n",
       " 658: 'https://arxiv.org/abs/1911.10947v1',\n",
       " 659: 'https://arxiv.org/abs/2007.05683v1',\n",
       " 660: 'https://arxiv.org/abs/2007.08025v1',\n",
       " 661: 'https://arxiv.org/abs/2108.04763v2',\n",
       " 662: 'https://arxiv.org/abs/2002.06288v1',\n",
       " 663: 'https://arxiv.org/abs/2202.05920v1',\n",
       " 664: 'https://arxiv.org/abs/1602.00203v1',\n",
       " 665: 'https://arxiv.org/abs/1801.07654v1',\n",
       " 666: 'https://arxiv.org/abs/1811.00512v2',\n",
       " 667: 'https://arxiv.org/abs/2012.10569v1',\n",
       " 668: 'https://arxiv.org/abs/2201.03947v1',\n",
       " 669: 'https://arxiv.org/abs/2208.04287v1',\n",
       " 670: 'https://arxiv.org/abs/2210.04817v1',\n",
       " 671: 'https://arxiv.org/abs/2406.15972v1',\n",
       " 672: 'https://arxiv.org/abs/1704.06885v1',\n",
       " 673: 'https://arxiv.org/abs/2107.11640v1',\n",
       " 674: 'https://arxiv.org/abs/1305.2505v1',\n",
       " 675: 'https://arxiv.org/abs/1410.3831v1',\n",
       " 676: 'https://arxiv.org/abs/1805.10451v2',\n",
       " 677: 'https://arxiv.org/abs/2202.06493v1',\n",
       " 678: 'https://arxiv.org/abs/1905.05461v2',\n",
       " 679: 'https://arxiv.org/abs/2209.14430v3',\n",
       " 680: 'https://arxiv.org/abs/2210.00770v1',\n",
       " 681: 'https://arxiv.org/abs/2303.14453v1',\n",
       " 682: 'https://arxiv.org/abs/2404.02254v2',\n",
       " 683: 'https://arxiv.org/abs/0205070v1',\n",
       " 684: 'https://arxiv.org/abs/1212.2514v1',\n",
       " 685: 'https://arxiv.org/abs/1710.10600v1',\n",
       " 686: 'https://arxiv.org/abs/1904.01631v1',\n",
       " 687: 'https://arxiv.org/abs/2002.09763v1',\n",
       " 688: 'https://arxiv.org/abs/2101.08675v1',\n",
       " 689: 'https://arxiv.org/abs/2109.07106v1',\n",
       " 690: 'https://arxiv.org/abs/2112.02655v2',\n",
       " 691: 'https://arxiv.org/abs/2301.09397v3',\n",
       " 692: 'https://arxiv.org/abs/2409.10203v1',\n",
       " 693: 'https://arxiv.org/abs/2004.10020v1',\n",
       " 694: 'https://arxiv.org/abs/2201.01289v2',\n",
       " 695: 'https://arxiv.org/abs/1206.6418v1',\n",
       " 696: 'https://arxiv.org/abs/1906.01868v3',\n",
       " 697: 'https://arxiv.org/abs/1508.01993v2',\n",
       " 698: 'https://arxiv.org/abs/1902.11233v1',\n",
       " 699: 'https://arxiv.org/abs/2006.08480v1',\n",
       " 700: 'https://arxiv.org/abs/1901.07592v2',\n",
       " 701: 'https://arxiv.org/abs/1910.00189v2',\n",
       " 702: 'https://arxiv.org/abs/2107.13671v1',\n",
       " 703: 'https://arxiv.org/abs/2306.07723v1',\n",
       " 704: 'https://arxiv.org/abs/2308.09643v1',\n",
       " 705: 'https://arxiv.org/abs/2409.08419v2',\n",
       " 706: 'https://arxiv.org/abs/1805.07507v1',\n",
       " 707: 'https://arxiv.org/abs/1811.10154v3',\n",
       " 708: 'https://arxiv.org/abs/2205.00538v1',\n",
       " 709: 'https://arxiv.org/abs/2308.09301v1',\n",
       " 710: 'https://arxiv.org/abs/2405.06433v4',\n",
       " 711: 'https://arxiv.org/abs/2406.09935v1',\n",
       " 712: 'https://arxiv.org/abs/1506.02620v2',\n",
       " 713: 'https://arxiv.org/abs/1609.01840v1',\n",
       " 714: 'https://arxiv.org/abs/1812.10437v1',\n",
       " 715: 'https://arxiv.org/abs/2009.07888v7',\n",
       " 716: 'https://arxiv.org/abs/2408.03219v2',\n",
       " 717: 'https://arxiv.org/abs/1704.08305v1',\n",
       " 718: 'https://arxiv.org/abs/1912.13169v1',\n",
       " 719: 'https://arxiv.org/abs/1502.06064v1',\n",
       " 720: 'https://arxiv.org/abs/1808.00198v1',\n",
       " 721: 'https://arxiv.org/abs/2004.01504v1',\n",
       " 722: 'https://arxiv.org/abs/2004.05107v1',\n",
       " 723: 'https://arxiv.org/abs/1504.04646v1',\n",
       " 724: 'https://arxiv.org/abs/1508.07096v1',\n",
       " 725: 'https://arxiv.org/abs/1701.04739v1',\n",
       " 726: 'https://arxiv.org/abs/1705.00564v2',\n",
       " 727: 'https://arxiv.org/abs/1903.09493v1',\n",
       " 728: 'https://arxiv.org/abs/2005.09868v1',\n",
       " 729: 'https://arxiv.org/abs/2007.05408v1',\n",
       " 730: 'https://arxiv.org/abs/2007.06776v1',\n",
       " 731: 'https://arxiv.org/abs/2007.07205v1',\n",
       " 732: 'https://arxiv.org/abs/2007.09339v1',\n",
       " 733: 'https://arxiv.org/abs/2006.14146v2',\n",
       " 734: 'https://arxiv.org/abs/2102.02279v1',\n",
       " 735: 'https://arxiv.org/abs/2105.06618v1',\n",
       " 736: 'https://arxiv.org/abs/2105.10172v1',\n",
       " 737: 'https://arxiv.org/abs/1606.04838v3',\n",
       " 738: 'https://arxiv.org/abs/1901.00770v2',\n",
       " 739: 'https://arxiv.org/abs/1907.01463v1',\n",
       " 740: 'https://arxiv.org/abs/2001.09751v2',\n",
       " 741: 'https://arxiv.org/abs/2002.08465v1',\n",
       " 742: 'https://arxiv.org/abs/2010.02576v2',\n",
       " 743: 'https://arxiv.org/abs/2202.10670v3',\n",
       " 744: 'https://arxiv.org/abs/1603.07292v1',\n",
       " 745: 'https://arxiv.org/abs/1811.11674v1',\n",
       " 746: 'https://arxiv.org/abs/1905.02168v1',\n",
       " 747: 'https://arxiv.org/abs/1905.07573v1',\n",
       " 748: 'https://arxiv.org/abs/2009.01399v1',\n",
       " 749: 'https://arxiv.org/abs/2009.10065v1',\n",
       " 750: 'https://arxiv.org/abs/2011.14572v2',\n",
       " 751: 'https://arxiv.org/abs/2012.04830v4',\n",
       " 752: 'https://arxiv.org/abs/2104.14677v1',\n",
       " 753: 'https://arxiv.org/abs/2106.10714v1',\n",
       " 754: 'https://arxiv.org/abs/2112.06712v1',\n",
       " 755: 'https://arxiv.org/abs/2203.13746v2',\n",
       " 756: 'https://arxiv.org/abs/2205.08824v1',\n",
       " 757: 'https://arxiv.org/abs/2206.12415v1',\n",
       " 758: 'https://arxiv.org/abs/2207.02960v1',\n",
       " 759: 'https://arxiv.org/abs/2208.03112v1',\n",
       " 760: 'https://arxiv.org/abs/2208.05596v2',\n",
       " 761: 'https://arxiv.org/abs/2209.00628v2',\n",
       " 762: 'https://arxiv.org/abs/2212.09067v1',\n",
       " 763: 'https://arxiv.org/abs/2212.10628v1',\n",
       " 764: 'https://arxiv.org/abs/2302.04386v1',\n",
       " 765: 'https://arxiv.org/abs/2303.06871v3',\n",
       " 766: 'https://arxiv.org/abs/2304.02381v1',\n",
       " 767: 'https://arxiv.org/abs/2306.10330v1',\n",
       " 768: 'https://arxiv.org/abs/2308.03037v1',\n",
       " 769: 'https://arxiv.org/abs/2308.14343v1',\n",
       " 770: 'https://arxiv.org/abs/2309.13246v1',\n",
       " 771: 'https://arxiv.org/abs/2310.01014v1',\n",
       " 772: 'https://arxiv.org/abs/2310.18646v1',\n",
       " 773: 'https://arxiv.org/abs/2401.11044v2',\n",
       " 774: 'https://arxiv.org/abs/2403.17381v1',\n",
       " 775: 'https://arxiv.org/abs/2403.18733v2',\n",
       " 776: 'https://arxiv.org/abs/2404.04623v1',\n",
       " 777: 'https://arxiv.org/abs/2405.00697v2',\n",
       " 778: 'https://arxiv.org/abs/2405.11601v1',\n",
       " 779: 'https://arxiv.org/abs/2405.19340v1',\n",
       " 780: 'https://arxiv.org/abs/2406.16250v1',\n",
       " 781: 'https://arxiv.org/abs/2408.01596v1',\n",
       " 782: 'https://arxiv.org/abs/2408.16683v1',\n",
       " 783: 'https://arxiv.org/abs/2410.01392v1',\n",
       " 784: 'https://arxiv.org/abs/1206.4608v1',\n",
       " 785: 'https://arxiv.org/abs/1804.04640v3',\n",
       " 786: 'https://arxiv.org/abs/1806.07846v1',\n",
       " 787: 'https://arxiv.org/abs/1812.09687v1',\n",
       " 788: 'https://arxiv.org/abs/1510.02674v1',\n",
       " 789: 'https://arxiv.org/abs/1706.03196v1',\n",
       " 790: 'https://arxiv.org/abs/1805.07483v2',\n",
       " 791: 'https://arxiv.org/abs/1807.01961v2',\n",
       " 792: 'https://arxiv.org/abs/1902.00140v2',\n",
       " 793: 'https://arxiv.org/abs/1908.00735v2',\n",
       " 794: 'https://arxiv.org/abs/1911.07644v1',\n",
       " 795: 'https://arxiv.org/abs/2007.07495v1',\n",
       " 796: 'https://arxiv.org/abs/2007.11455v1',\n",
       " 797: 'https://arxiv.org/abs/2006.06231v4',\n",
       " 798: 'https://arxiv.org/abs/2103.16378v1',\n",
       " 799: 'https://arxiv.org/abs/2001.00742v1',\n",
       " 800: 'https://arxiv.org/abs/2002.02046v1',\n",
       " 801: 'https://arxiv.org/abs/2003.12590v1',\n",
       " 802: 'https://arxiv.org/abs/2010.04651v2',\n",
       " 803: 'https://arxiv.org/abs/2202.13564v1',\n",
       " 804: 'https://arxiv.org/abs/1602.02823v1',\n",
       " 805: 'https://arxiv.org/abs/1712.00076v1',\n",
       " 806: 'https://arxiv.org/abs/1802.06091v1',\n",
       " 807: 'https://arxiv.org/abs/1802.09225v1',\n",
       " 808: 'https://arxiv.org/abs/1809.07609v2',\n",
       " 809: 'https://arxiv.org/abs/1811.05975v1',\n",
       " 810: 'https://arxiv.org/abs/1905.09849v2',\n",
       " 811: 'https://arxiv.org/abs/1912.08616v1',\n",
       " 812: 'https://arxiv.org/abs/2104.01757v1',\n",
       " 813: 'https://arxiv.org/abs/2111.02508v1',\n",
       " 814: 'https://arxiv.org/abs/2111.08507v1',\n",
       " 815: 'https://arxiv.org/abs/2203.16340v1',\n",
       " 816: 'https://arxiv.org/abs/2212.01415v1',\n",
       " 817: 'https://arxiv.org/abs/2301.13420v1',\n",
       " 818: 'https://arxiv.org/abs/2307.00476v1',\n",
       " 819: 'https://arxiv.org/abs/2401.07464v1',\n",
       " 820: 'https://arxiv.org/abs/2404.18541v1',\n",
       " 821: 'https://arxiv.org/abs/2410.09935v1',\n",
       " 822: 'https://arxiv.org/abs/2407.15283v1',\n",
       " 823: 'https://arxiv.org/abs/0201009v1',\n",
       " 824: 'https://arxiv.org/abs/1806.01756v1',\n",
       " 825: 'https://arxiv.org/abs/1908.02130v1',\n",
       " 826: 'https://arxiv.org/abs/2003.11816v1',\n",
       " 827: 'https://arxiv.org/abs/2104.07651v2',\n",
       " 828: 'https://arxiv.org/abs/2111.11537v1',\n",
       " 829: 'https://arxiv.org/abs/2201.04093v2',\n",
       " 830: 'https://arxiv.org/abs/2205.08875v1',\n",
       " 831: 'https://arxiv.org/abs/2302.09438v1',\n",
       " 832: 'https://arxiv.org/abs/2305.05531v1',\n",
       " 833: 'https://arxiv.org/abs/2306.04338v1',\n",
       " 834: 'https://arxiv.org/abs/2404.12511v1',\n",
       " 835: 'https://arxiv.org/abs/2405.06148v1',\n",
       " 836: 'https://arxiv.org/abs/2406.13166v1',\n",
       " 837: 'https://arxiv.org/abs/1507.01461v1',\n",
       " 838: 'https://arxiv.org/abs/1507.03125v1',\n",
       " 839: 'https://arxiv.org/abs/1806.09710v1',\n",
       " 840: 'https://arxiv.org/abs/1808.02213v1',\n",
       " 841: 'https://arxiv.org/abs/1412.7584v1',\n",
       " 842: 'https://arxiv.org/abs/1706.09773v4',\n",
       " 843: 'https://arxiv.org/abs/1803.00158v1',\n",
       " 844: 'https://arxiv.org/abs/1904.05381v1',\n",
       " 845: 'https://arxiv.org/abs/1908.04705v2',\n",
       " 846: 'https://arxiv.org/abs/1911.05289v1',\n",
       " 847: 'https://arxiv.org/abs/1911.11726v1',\n",
       " 848: 'https://arxiv.org/abs/2102.11107v1',\n",
       " 849: 'https://arxiv.org/abs/2105.01650v2',\n",
       " 850: 'https://arxiv.org/abs/2105.09107v1',\n",
       " 851: 'https://arxiv.org/abs/1901.00246v2',\n",
       " 852: 'https://arxiv.org/abs/2001.09636v1',\n",
       " 853: 'https://arxiv.org/abs/1905.10345v1',\n",
       " 854: 'https://arxiv.org/abs/2107.02894v1',\n",
       " 855: 'https://arxiv.org/abs/2206.08353v1',\n",
       " 856: 'https://arxiv.org/abs/2208.02587v1',\n",
       " 857: 'https://arxiv.org/abs/2401.07744v2',\n",
       " 858: 'https://arxiv.org/abs/2403.03533v1',\n",
       " 859: 'https://arxiv.org/abs/2406.10234v2',\n",
       " 860: 'https://arxiv.org/abs/2406.19054v1',\n",
       " 861: 'https://arxiv.org/abs/2004.10387v2',\n",
       " 862: 'https://arxiv.org/abs/1703.05561v1',\n",
       " 863: 'https://arxiv.org/abs/2105.10041v1',\n",
       " 864: 'https://arxiv.org/abs/2202.12577v3',\n",
       " 865: 'https://arxiv.org/abs/2210.00707v1',\n",
       " 866: 'https://arxiv.org/abs/2305.02840v1',\n",
       " 867: 'https://arxiv.org/abs/0912.0874v2',\n",
       " 868: 'https://arxiv.org/abs/1001.4019v1',\n",
       " 869: 'https://arxiv.org/abs/2211.06034v1',\n",
       " 870: 'https://arxiv.org/abs/2404.03775v1',\n",
       " 871: 'https://arxiv.org/abs/1810.06339v1',\n",
       " 872: 'https://arxiv.org/abs/1810.11295v1',\n",
       " 873: 'https://arxiv.org/abs/2007.06918v1',\n",
       " 874: 'https://arxiv.org/abs/2108.05595v1',\n",
       " 875: 'https://arxiv.org/abs/2010.01761v3',\n",
       " 876: 'https://arxiv.org/abs/2106.13103v3',\n",
       " 877: 'https://arxiv.org/abs/2112.00423v3',\n",
       " 878: 'https://arxiv.org/abs/2206.02786v1',\n",
       " 879: 'https://arxiv.org/abs/2301.01542v1',\n",
       " 880: 'https://arxiv.org/abs/2302.08981v2',\n",
       " 881: 'https://arxiv.org/abs/2305.09738v1',\n",
       " 882: 'https://arxiv.org/abs/2403.11125v2',\n",
       " 883: 'https://arxiv.org/abs/9802179v1',\n",
       " 884: 'https://arxiv.org/abs/1301.6944v1',\n",
       " 885: 'https://arxiv.org/abs/1807.02999v2',\n",
       " 886: 'https://arxiv.org/abs/1707.05532v1',\n",
       " 887: 'https://arxiv.org/abs/1905.07686v2',\n",
       " 888: 'https://arxiv.org/abs/2308.07204v1',\n",
       " 889: 'https://arxiv.org/abs/0010423v1',\n",
       " 890: 'https://arxiv.org/abs/1110.0593v1',\n",
       " 891: 'https://arxiv.org/abs/1204.6509v1',\n",
       " 892: 'https://arxiv.org/abs/1206.6446v1',\n",
       " 893: 'https://arxiv.org/abs/1301.3078v1',\n",
       " 894: 'https://arxiv.org/abs/1806.08049v1',\n",
       " 895: 'https://arxiv.org/abs/2004.13081v1',\n",
       " 896: 'https://arxiv.org/abs/1402.6013v1',\n",
       " 897: 'https://arxiv.org/abs/1705.05264v1',\n",
       " 898: 'https://arxiv.org/abs/1709.01666v1',\n",
       " 899: 'https://arxiv.org/abs/1803.08101v1',\n",
       " 900: 'https://arxiv.org/abs/1807.01334v1',\n",
       " 901: 'https://arxiv.org/abs/1903.07825v1',\n",
       " 902: 'https://arxiv.org/abs/2005.05812v1',\n",
       " 903: 'https://arxiv.org/abs/2007.02423v3',\n",
       " 904: 'https://arxiv.org/abs/2108.02814v1',\n",
       " 905: 'https://arxiv.org/abs/2008.02076v1',\n",
       " 906: 'https://arxiv.org/abs/2008.03288v1',\n",
       " 907: 'https://arxiv.org/abs/2001.06699v1',\n",
       " 908: 'https://arxiv.org/abs/2002.04679v1',\n",
       " 909: 'https://arxiv.org/abs/2003.10674v1',\n",
       " 910: 'https://arxiv.org/abs/2010.10777v4',\n",
       " 911: 'https://arxiv.org/abs/1809.00593v1',\n",
       " 912: 'https://arxiv.org/abs/1809.04684v1',\n",
       " 913: 'https://arxiv.org/abs/1811.09623v2',\n",
       " 914: 'https://arxiv.org/abs/1912.09336v1',\n",
       " 915: 'https://arxiv.org/abs/2011.11134v1',\n",
       " 916: 'https://arxiv.org/abs/2101.03219v1',\n",
       " 917: 'https://arxiv.org/abs/2104.03886v1',\n",
       " 918: 'https://arxiv.org/abs/2110.10783v1',\n",
       " 919: 'https://arxiv.org/abs/2201.09345v2',\n",
       " 920: 'https://arxiv.org/abs/2205.10825v1',\n",
       " 921: 'https://arxiv.org/abs/2209.05084v1',\n",
       " 922: 'https://arxiv.org/abs/2304.00285v1',\n",
       " 923: 'https://arxiv.org/abs/2308.00133v1',\n",
       " 924: 'https://arxiv.org/abs/2308.00718v1',\n",
       " 925: 'https://arxiv.org/abs/2308.10993v1',\n",
       " 926: 'https://arxiv.org/abs/2311.02278v1',\n",
       " 927: 'https://arxiv.org/abs/2312.07698v1',\n",
       " 928: 'https://arxiv.org/abs/2402.01778v1',\n",
       " 929: 'https://arxiv.org/abs/2402.19254v1',\n",
       " 930: 'https://arxiv.org/abs/2405.09781v1',\n",
       " 931: 'https://arxiv.org/abs/2406.08929v2',\n",
       " 932: 'https://arxiv.org/abs/2408.10291v2',\n",
       " 933: 'https://arxiv.org/abs/2408.12625v1',\n",
       " 934: 'https://arxiv.org/abs/2410.11776v1',\n",
       " 935: 'https://arxiv.org/abs/1306.6709v4',\n",
       " 936: 'https://arxiv.org/abs/2303.11191v1',\n",
       " 937: 'https://arxiv.org/abs/1903.07854v1',\n",
       " 938: 'https://arxiv.org/abs/2204.11897v2',\n",
       " 939: 'https://arxiv.org/abs/1901.09890v1',\n",
       " 940: 'https://arxiv.org/abs/1411.1490v2',\n",
       " 941: 'https://arxiv.org/abs/2109.10824v1',\n",
       " 942: 'https://arxiv.org/abs/2207.09611v1',\n",
       " 943: 'https://arxiv.org/abs/2210.14797v1',\n",
       " 944: 'https://arxiv.org/abs/2007.00487v3',\n",
       " 945: 'https://arxiv.org/abs/1511.02254v1',\n",
       " 946: 'https://arxiv.org/abs/1706.06617v1',\n",
       " 947: 'https://arxiv.org/abs/2006.11184v2',\n",
       " 948: 'https://arxiv.org/abs/2103.07018v1',\n",
       " 949: 'https://arxiv.org/abs/1901.08098v4',\n",
       " 950: 'https://arxiv.org/abs/2202.10688v2',\n",
       " 951: 'https://arxiv.org/abs/2206.12520v2',\n",
       " 952: 'https://arxiv.org/abs/2206.13190v1',\n",
       " 953: 'https://arxiv.org/abs/2212.14681v2',\n",
       " 954: 'https://arxiv.org/abs/2402.04054v2',\n",
       " 955: 'https://arxiv.org/abs/2407.04189v2',\n",
       " 956: 'https://arxiv.org/abs/1812.00524v1',\n",
       " 957: 'https://arxiv.org/abs/2108.07783v2',\n",
       " 958: 'https://arxiv.org/abs/1907.07291v1',\n",
       " 959: 'https://arxiv.org/abs/1907.12363v2',\n",
       " 960: 'https://arxiv.org/abs/2010.12896v1',\n",
       " 961: 'https://arxiv.org/abs/2202.10600v2',\n",
       " 962: 'https://arxiv.org/abs/1801.01777v4',\n",
       " 963: 'https://arxiv.org/abs/1811.04380v1',\n",
       " 964: 'https://arxiv.org/abs/1905.06549v2',\n",
       " 965: 'https://arxiv.org/abs/2112.09668v1',\n",
       " 966: 'https://arxiv.org/abs/2207.03244v2',\n",
       " 967: 'https://arxiv.org/abs/2302.14567v1',\n",
       " 968: 'https://arxiv.org/abs/2306.16557v1',\n",
       " 969: 'https://arxiv.org/abs/2310.06221v1',\n",
       " 970: 'https://arxiv.org/abs/2402.01067v1',\n",
       " 971: 'https://arxiv.org/abs/2404.18975v3',\n",
       " 972: 'https://arxiv.org/abs/2406.13877v1',\n",
       " 973: 'https://arxiv.org/abs/2407.14695v1',\n",
       " 974: 'https://arxiv.org/abs/2408.11374v1',\n",
       " 975: 'https://arxiv.org/abs/2410.03989v1',\n",
       " 976: 'https://arxiv.org/abs/1707.09835v2',\n",
       " 977: 'https://arxiv.org/abs/2406.07983v1',\n",
       " 978: 'https://arxiv.org/abs/2105.03905v3',\n",
       " 979: 'https://arxiv.org/abs/2404.13954v1',\n",
       " 980: 'https://arxiv.org/abs/1006.1138v3',\n",
       " 981: 'https://arxiv.org/abs/1206.1106v2',\n",
       " 982: 'https://arxiv.org/abs/1206.4630v1',\n",
       " 983: 'https://arxiv.org/abs/1711.03343v1',\n",
       " 984: 'https://arxiv.org/abs/1808.05443v1',\n",
       " 985: 'https://arxiv.org/abs/1412.6177v3',\n",
       " 986: 'https://arxiv.org/abs/1702.07956v5',\n",
       " 987: 'https://arxiv.org/abs/1710.10628v3',\n",
       " 988: 'https://arxiv.org/abs/1805.09733v3',\n",
       " 989: 'https://arxiv.org/abs/1807.04439v1',\n",
       " 990: 'https://arxiv.org/abs/1810.08323v1',\n",
       " 991: 'https://arxiv.org/abs/1902.01449v1',\n",
       " 992: 'https://arxiv.org/abs/1903.03234v1',\n",
       " 993: 'https://arxiv.org/abs/1911.10164v1',\n",
       " 994: 'https://arxiv.org/abs/2102.13085v1',\n",
       " 995: 'https://arxiv.org/abs/2105.07636v2',\n",
       " 996: 'https://arxiv.org/abs/1909.04751v1',\n",
       " 997: 'https://arxiv.org/abs/2002.11246v1',\n",
       " 998: 'https://arxiv.org/abs/2010.11029v2',\n",
       " 999: 'https://arxiv.org/abs/2202.13800v2',\n",
       " 1000: 'https://arxiv.org/abs/1801.09136v2',\n",
       " ...}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docid_link_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfbb9560-903f-4e2e-8c04-f1490852294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('docid_link_map.pkl', 'wb') as file:\n",
    "    pickle.dump(docid_link_map, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f294ad9-8c61-4c70-9f55-54eefc93b62c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac98cd35-2ee5-4bd2-929c-08f21600eadd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c28d53-f19d-4799-9c4a-4bac844a9f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a0a96-ce6e-431a-be7c-f5a2889ff57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0589e869-be76-494c-93c6-2c5a86b771c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c8044b-d4e9-4015-ad71-4eeabd2f856f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d270f6-deef-4884-b0a9-c4116fd78d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e257cf-c14c-443f-b46a-4ee78485053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_papers.jsonl', 'r') as file:\n",
    "    idx=0\n",
    "    for line_num, line in enumerate(file):\n",
    "        idx+=1\n",
    "        if idx==95348:\n",
    "            document = json.loads(line.strip())\n",
    "            print(document)\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
