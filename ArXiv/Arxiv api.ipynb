{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f44cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "import csv\n",
    "import time\n",
    "from requests.exceptions import ConnectionError, Timeout\n",
    "\n",
    "ARXIV_API_URL = \"http://export.arxiv.org/api/query?\"\n",
    "MAX_RESULTS_PER_QUERY = 300 \n",
    "RETRY_LIMIT = 5 \n",
    "\n",
    "def fetch_arxiv_papers(query, start_index=0, max_results=100):\n",
    "    \"\"\"Fetches a batch of papers from arXiv API with retry logic.\"\"\"\n",
    "    if max_results > MAX_RESULTS_PER_QUERY:\n",
    "        max_results = MAX_RESULTS_PER_QUERY  \n",
    "\n",
    "    query_url = f\"{ARXIV_API_URL}search_query={query}&start={start_index}&max_results={max_results}\"\n",
    "    headers = {'User-Agent': 'arxiv-fetch-script/1.0'}\n",
    "    \n",
    "    retry_count = 0\n",
    "    while retry_count < RETRY_LIMIT:\n",
    "        try:\n",
    "            response = requests.get(query_url, headers=headers, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                papers_data = xmltodict.parse(response.text)\n",
    "                if 'entry' in papers_data['feed']:\n",
    "                    return papers_data['feed']['entry']\n",
    "                else:\n",
    "                    return []\n",
    "            else:\n",
    "                print(f\"Failed to retrieve papers: {response.status_code}\")\n",
    "                return []\n",
    "        \n",
    "        except (ConnectionError, Timeout) as e:\n",
    "            print(f\"Connection error: {e}. Retrying {retry_count + 1}/{RETRY_LIMIT}...\")\n",
    "            retry_count += 1\n",
    "            time.sleep(2 ** retry_count) \n",
    "\n",
    "    print(\"Max retries reached. Exiting.\")\n",
    "    return []\n",
    "\n",
    "def extract_paper_details(paper):\n",
    "    \"\"\"Extracts details of a single paper.\"\"\"\n",
    "    authors = [author['name'] for author in paper.get('author', [])] if isinstance(paper.get('author'), list) else [paper.get('author', {}).get('name', '')]\n",
    "\n",
    "    arxiv_id = paper.get('id', '').split('/')[-1]\n",
    "    \n",
    "    return {\n",
    "        'arxiv_id': arxiv_id,\n",
    "        'title': paper.get('title', None),\n",
    "        'authors': authors,\n",
    "        'summary': paper.get('summary', None),\n",
    "        'published': paper.get('published', None),\n",
    "        'arxiv_abstract_url': paper.get('id', None),\n",
    "        'arxiv_pdf_url': paper.get('link', [])[1].get('@href', None) \n",
    "    }\n",
    "\n",
    "def build_arxiv_corpus(query, max_results=1000, delay=3):\n",
    "    \"\"\"Builds the corpus of arXiv papers for a given query, respecting API limits.\"\"\"\n",
    "    corpus = []\n",
    "    start_index = 0\n",
    "\n",
    "    while start_index < max_results:\n",
    "        print(f\"Fetching papers {start_index} to {start_index + MAX_RESULTS_PER_QUERY}...\")\n",
    "\n",
    "        # Fetch batch of papers\n",
    "        papers = fetch_arxiv_papers(query, start_index=start_index, max_results=MAX_RESULTS_PER_QUERY)\n",
    "        \n",
    "        if not papers:\n",
    "            break\n",
    "\n",
    "        for paper in papers:\n",
    "            try:\n",
    "                paper_details = extract_paper_details(paper)\n",
    "                corpus.append(paper_details)\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting paper details: {e}\")\n",
    "                continue\n",
    "        \n",
    "        start_index += MAX_RESULTS_PER_QUERY\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "def save_corpus_to_csv(corpus, filename):\n",
    "    \"\"\"Saves the corpus to a CSV file.\"\"\"\n",
    "    headers = ['arxiv_id', 'title', 'authors', 'summary', 'published', 'arxiv_abstract_url', 'arxiv_pdf_url']\n",
    "    \n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader() \n",
    "        for paper in corpus:\n",
    "            writer.writerow(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d3db236",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_terms = [\n",
    "    \"machine learning\",\n",
    "    \"artificial intelligence\",\n",
    "    \"natural language processing\",\n",
    "    \"deep learning\",\n",
    "    \"reinforcement learning\",\n",
    "    \"supervised learning\",\n",
    "    \"unsupervised learning\",\n",
    "    \"semi-supervised learning\",\n",
    "    \"transfer learning\",\n",
    "    \"self-supervised learning\",\n",
    "    \"generative adversarial networks\",\n",
    "    \"convolutional neural networks\",\n",
    "    \"recurrent neural networks\",\n",
    "    \"transformers\",\n",
    "    \"few-shot learning\",\n",
    "    \"meta learning\",\n",
    "    \"bayesian neural networks\",\n",
    "    \"autoencoders\",\n",
    "    \"graph neural networks\",\n",
    "    \"federated learning\",\n",
    "    \"multi-task learning\",\n",
    "    \"artificial general intelligence\",\n",
    "    \"explainable AI\",\n",
    "    \"AI ethics\",\n",
    "    \"AI in healthcare\",\n",
    "    \"AI in robotics\",\n",
    "    \"AI in autonomous vehicles\",\n",
    "    \"AI for cybersecurity\",\n",
    "    \"AI for education\",\n",
    "    \"AI for social good\",\n",
    "    \"language models\",\n",
    "    \"word embeddings\",\n",
    "    \"sentence embeddings\",\n",
    "    \"named entity recognition\",\n",
    "    \"sentiment analysis\",\n",
    "    \"text classification\",\n",
    "    \"machine translation\",\n",
    "    \"text summarization\",\n",
    "    \"question answering\",\n",
    "    \"information retrieval\",\n",
    "    \"text generation\",\n",
    "    \"language model fine-tuning\",\n",
    "    \"BERT\",\n",
    "    \"GPT\",\n",
    "    \"multilingual NLP\",\n",
    "    \"speech recognition\",\n",
    "    \"speech synthesis\",\n",
    "    \"natural language understanding\",\n",
    "    \"natural language inference\",\n",
    "    \"zero-shot learning in NLP\",\n",
    "    \"few-shot learning in NLP\",\n",
    "    \"computer vision\",\n",
    "    \"AI in drug discovery\",\n",
    "    \"AI in finance\",\n",
    "    \"AI for climate change\",\n",
    "    \"data augmentation\",\n",
    "    \"synthetic data generation\",\n",
    "    \"backpropagation\",\n",
    "    \"stochastic gradient descent\",\n",
    "    \"attention mechanism\",\n",
    "    \"self-attention\",\n",
    "    \"graph embeddings\",\n",
    "    \"hyperparameter tuning\",\n",
    "    \"neural architecture search\",\n",
    "    \"regularization in machine learning\",\n",
    "    \"NeurIPS\",\n",
    "    \"ICLR\",\n",
    "    \"ACL\",\n",
    "    \"EMNLP\",\n",
    "    \"AAAI\",\n",
    "    \"IJCAI\",\n",
    "    \"ICML\",\n",
    "    \"CVPR\",\n",
    "    \"few-shot learning with transformers\",\n",
    "    \"large language models\",\n",
    "    \"multi-modal learning\",\n",
    "    \"vision-language pre-training\",\n",
    "    \"AI for human-computer interaction\",\n",
    "    \"ethical considerations in AI\",\n",
    "    \"LLMs in healthcare applications\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c898e871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Corpus of 1200 papers related to machine learning has been saved to 'arxiv_corpus_0.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Corpus of 2700 papers related to artificial intelligence has been saved to 'arxiv_corpus_1.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Corpus of 400 papers related to natural language processing has been saved to 'arxiv_corpus_2.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Corpus of 1200 papers related to deep learning has been saved to 'arxiv_corpus_3.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Corpus of 2100 papers related to reinforcement learning has been saved to 'arxiv_corpus_4.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Corpus of 500 papers related to supervised learning has been saved to 'arxiv_corpus_5.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Corpus of 900 papers related to unsupervised learning has been saved to 'arxiv_corpus_6.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Corpus of 2311 papers related to semi-supervised learning has been saved to 'arxiv_corpus_7.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Fetching papers 3000 to 3300...\n",
      "Fetching papers 3300 to 3600...\n",
      "Fetching papers 3600 to 3900...\n",
      "Fetching papers 3900 to 4200...\n",
      "Corpus of 3900 papers related to transfer learning has been saved to 'arxiv_corpus_8.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Corpus of 900 papers related to self-supervised learning has been saved to 'arxiv_corpus_9.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Corpus of 300 papers related to generative adversarial networks has been saved to 'arxiv_corpus_10.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Connection error: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10). Retrying 1/5...\n",
      "Fetching papers 1500 to 1800...\n",
      "Connection error: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10). Retrying 1/5...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Fetching papers 3000 to 3300...\n",
      "Fetching papers 3300 to 3600...\n",
      "Connection error: HTTPConnectionPool(host='export.arxiv.org', port=80): Read timed out. (read timeout=10). Retrying 1/5...\n",
      "Corpus of 3300 papers related to convolutional neural networks has been saved to 'arxiv_corpus_11.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Corpus of 100 papers related to recurrent neural networks has been saved to 'arxiv_corpus_12.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Corpus of 2500 papers related to transformers has been saved to 'arxiv_corpus_13.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Corpus of 400 papers related to few-shot learning has been saved to 'arxiv_corpus_14.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Corpus of 1200 papers related to meta learning has been saved to 'arxiv_corpus_15.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Corpus of 910 papers related to bayesian neural networks has been saved to 'arxiv_corpus_16.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Corpus of 1200 papers related to autoencoders has been saved to 'arxiv_corpus_17.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Corpus of 200 papers related to graph neural networks has been saved to 'arxiv_corpus_18.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Corpus of 2100 papers related to federated learning has been saved to 'arxiv_corpus_19.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Corpus of 600 papers related to multi-task learning has been saved to 'arxiv_corpus_20.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Corpus of 900 papers related to artificial general intelligence has been saved to 'arxiv_corpus_21.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Corpus of 1200 papers related to explainable AI has been saved to 'arxiv_corpus_22.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Fetching papers 3000 to 3300...\n",
      "Fetching papers 3300 to 3600...\n",
      "Fetching papers 3600 to 3900...\n",
      "Fetching papers 3900 to 4200...\n",
      "Fetching papers 4200 to 4500...\n",
      "Fetching papers 4500 to 4800...\n",
      "Fetching papers 4800 to 5100...\n",
      "Fetching papers 5100 to 5400...\n",
      "Fetching papers 5400 to 5700...\n",
      "Fetching papers 5700 to 6000...\n",
      "Corpus of 5700 papers related to AI ethics has been saved to 'arxiv_corpus_23.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Corpus of 400 papers related to AI in healthcare has been saved to 'arxiv_corpus_24.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Corpus of 700 papers related to AI in robotics has been saved to 'arxiv_corpus_25.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Corpus of 300 papers related to AI in autonomous vehicles has been saved to 'arxiv_corpus_26.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Corpus of 100 papers related to AI for cybersecurity has been saved to 'arxiv_corpus_27.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Corpus of 600 papers related to AI for education has been saved to 'arxiv_corpus_28.csv'.\n",
      "Fetching papers 0 to 300...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching papers 300 to 600...\n",
      "Corpus of 100 papers related to AI for social good has been saved to 'arxiv_corpus_29.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Corpus of 600 papers related to language models has been saved to 'arxiv_corpus_30.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Fetching papers 3000 to 3300...\n",
      "Fetching papers 3300 to 3600...\n",
      "Fetching papers 3600 to 3900...\n",
      "Fetching papers 3900 to 4200...\n",
      "Fetching papers 4200 to 4500...\n",
      "Fetching papers 4500 to 4800...\n",
      "Fetching papers 4800 to 5100...\n",
      "Corpus of 4800 papers related to word embeddings has been saved to 'arxiv_corpus_31.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Fetching papers 3000 to 3300...\n",
      "Fetching papers 3300 to 3600...\n",
      "Fetching papers 3600 to 3900...\n",
      "Fetching papers 3900 to 4200...\n",
      "Corpus of 3900 papers related to sentence embeddings has been saved to 'arxiv_corpus_32.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Corpus of 900 papers related to named entity recognition has been saved to 'arxiv_corpus_33.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Corpus of 323 papers related to sentiment analysis has been saved to 'arxiv_corpus_34.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Fetching papers 3000 to 3300...\n",
      "Fetching papers 3300 to 3600...\n",
      "Fetching papers 3600 to 3900...\n",
      "Fetching papers 3900 to 4200...\n",
      "Corpus of 3900 papers related to text classification has been saved to 'arxiv_corpus_35.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Fetching papers 3000 to 3300...\n",
      "Fetching papers 3300 to 3600...\n",
      "Fetching papers 3600 to 3900...\n",
      "Fetching papers 3900 to 4200...\n",
      "Corpus of 3900 papers related to machine translation has been saved to 'arxiv_corpus_36.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Corpus of 2400 papers related to text summarization has been saved to 'arxiv_corpus_37.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Corpus of 600 papers related to question answering has been saved to 'arxiv_corpus_38.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Corpus of 900 papers related to information retrieval has been saved to 'arxiv_corpus_39.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Corpus of 300 papers related to text generation has been saved to 'arxiv_corpus_40.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Corpus of 1510 papers related to language model fine-tuning has been saved to 'arxiv_corpus_41.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Fetching papers 3000 to 3300...\n",
      "Fetching papers 3300 to 3600...\n",
      "Fetching papers 3600 to 3900...\n",
      "Fetching papers 3900 to 4200...\n",
      "Fetching papers 4200 to 4500...\n",
      "Fetching papers 4500 to 4800...\n",
      "Fetching papers 4800 to 5100...\n",
      "Fetching papers 5100 to 5400...\n",
      "Fetching papers 5400 to 5700...\n",
      "Fetching papers 5700 to 6000...\n",
      "Fetching papers 6000 to 6300...\n",
      "Fetching papers 6300 to 6600...\n",
      "Fetching papers 6600 to 6900...\n",
      "Corpus of 6556 papers related to BERT has been saved to 'arxiv_corpus_42.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Corpus of 2473 papers related to GPT has been saved to 'arxiv_corpus_43.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Fetching papers 3000 to 3300...\n",
      "Fetching papers 3300 to 3600...\n",
      "Fetching papers 3600 to 3900...\n",
      "Fetching papers 3900 to 4200...\n",
      "Corpus of 3900 papers related to multilingual NLP has been saved to 'arxiv_corpus_44.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Corpus of 1800 papers related to speech recognition has been saved to 'arxiv_corpus_45.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Fetching papers 3000 to 3300...\n",
      "Fetching papers 3300 to 3600...\n",
      "Corpus of 3076 papers related to speech synthesis has been saved to 'arxiv_corpus_46.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Corpus of 1500 papers related to natural language understanding has been saved to 'arxiv_corpus_47.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Corpus of 600 papers related to natural language inference has been saved to 'arxiv_corpus_48.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Corpus of 100 papers related to zero-shot learning in NLP has been saved to 'arxiv_corpus_49.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Corpus of 200 papers related to few-shot learning in NLP has been saved to 'arxiv_corpus_50.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Corpus of 300 papers related to computer vision has been saved to 'arxiv_corpus_51.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Corpus of 600 papers related to AI in drug discovery has been saved to 'arxiv_corpus_52.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Corpus of 900 papers related to AI in finance has been saved to 'arxiv_corpus_53.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Corpus of 300 papers related to AI for climate change has been saved to 'arxiv_corpus_54.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching papers 900 to 1200...\n",
      "Corpus of 700 papers related to data augmentation has been saved to 'arxiv_corpus_55.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Corpus of 600 papers related to synthetic data generation has been saved to 'arxiv_corpus_56.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Corpus of 2404 papers related to backpropagation has been saved to 'arxiv_corpus_57.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Corpus of 2400 papers related to stochastic gradient descent has been saved to 'arxiv_corpus_58.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Corpus of 900 papers related to attention mechanism has been saved to 'arxiv_corpus_59.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Corpus of 19 papers related to self-attention has been saved to 'arxiv_corpus_60.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Corpus of 1400 papers related to graph embeddings has been saved to 'arxiv_corpus_61.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Corpus of 1200 papers related to hyperparameter tuning has been saved to 'arxiv_corpus_62.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Corpus of 1200 papers related to neural architecture search has been saved to 'arxiv_corpus_63.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Corpus of 1500 papers related to regularization in machine learning has been saved to 'arxiv_corpus_64.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Corpus of 1200 papers related to NeurIPS has been saved to 'arxiv_corpus_65.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Corpus of 300 papers related to ICLR has been saved to 'arxiv_corpus_66.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Fetching papers 3000 to 3300...\n",
      "Fetching papers 3300 to 3600...\n",
      "Fetching papers 3600 to 3900...\n",
      "Corpus of 3400 papers related to ACL has been saved to 'arxiv_corpus_67.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Corpus of 2100 papers related to EMNLP has been saved to 'arxiv_corpus_68.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Corpus of 600 papers related to AAAI has been saved to 'arxiv_corpus_69.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Corpus of 2100 papers related to IJCAI has been saved to 'arxiv_corpus_70.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Fetching papers 3000 to 3300...\n",
      "Fetching papers 3300 to 3600...\n",
      "Fetching papers 3600 to 3900...\n",
      "Fetching papers 3900 to 4200...\n",
      "Fetching papers 4200 to 4500...\n",
      "Fetching papers 4500 to 4800...\n",
      "Fetching papers 4800 to 5100...\n",
      "Fetching papers 5100 to 5400...\n",
      "Corpus of 5100 papers related to ICML has been saved to 'arxiv_corpus_71.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Fetching papers 3000 to 3300...\n",
      "Fetching papers 3300 to 3600...\n",
      "Fetching papers 3600 to 3900...\n",
      "Fetching papers 3900 to 4200...\n",
      "Corpus of 3900 papers related to CVPR has been saved to 'arxiv_corpus_72.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Corpus of 100 papers related to few-shot learning with transformers has been saved to 'arxiv_corpus_73.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Corpus of 600 papers related to large language models has been saved to 'arxiv_corpus_74.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Fetching papers 900 to 1200...\n",
      "Fetching papers 1200 to 1500...\n",
      "Fetching papers 1500 to 1800...\n",
      "Fetching papers 1800 to 2100...\n",
      "Fetching papers 2100 to 2400...\n",
      "Fetching papers 2400 to 2700...\n",
      "Fetching papers 2700 to 3000...\n",
      "Fetching papers 3000 to 3300...\n",
      "Corpus of 2900 papers related to multi-modal learning has been saved to 'arxiv_corpus_75.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Corpus of 44 papers related to vision-language pre-training has been saved to 'arxiv_corpus_76.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Corpus of 200 papers related to AI for human-computer interaction has been saved to 'arxiv_corpus_77.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Corpus of 600 papers related to ethical considerations in AI has been saved to 'arxiv_corpus_78.csv'.\n",
      "Fetching papers 0 to 300...\n",
      "Fetching papers 300 to 600...\n",
      "Fetching papers 600 to 900...\n",
      "Corpus of 350 papers related to LLMs in healthcare applications has been saved to 'arxiv_corpus_79.csv'.\n"
     ]
    }
   ],
   "source": [
    "for i, query in enumerate(query_terms):\n",
    "    corpus = build_arxiv_corpus(query, max_results=10000)\n",
    "    save_corpus_to_csv(corpus, f'arxiv_corpus_{i}.csv')\n",
    "    print(f\"Corpus of {len(corpus)} papers related to {query} has been saved to 'arxiv_corpus_{i}.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0887b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_l = []\n",
    "retry_queries_list = []\n",
    "df_len_l = []\n",
    "for i, query in enumerate(query_terms):\n",
    "    df = pd.read_csv(f'arxiv_corpus_{i}.csv')\n",
    "    if df.shape[0] > 0:\n",
    "        df_l.append(df)\n",
    "        df_len_l.append(df.shape[0])\n",
    "    else:\n",
    "        retry_queries_list.append(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3e4edc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121076, 7)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mrgd = pd.concat(df_l, ignore_index=True)\n",
    "df_mrgd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ccede03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92756"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mrgd['arxiv_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7fc8e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrgd = df_mrgd.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fd66af38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([32., 18.,  5., 10.,  3.,  8.,  0.,  2.,  1.,  1.]),\n",
       " array([  19. ,  672.7, 1326.4, 1980.1, 2633.8, 3287.5, 3941.2, 4594.9,\n",
       "        5248.6, 5902.3, 6556. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df_len_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e184f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mrgd.to_csv(\"compiled_papers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a02de19a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>summary</th>\n",
       "      <th>published</th>\n",
       "      <th>arxiv_abstract_url</th>\n",
       "      <th>arxiv_pdf_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1909.03550v1</td>\n",
       "      <td>Lecture Notes: Optimization for Machine Learning</td>\n",
       "      <td>['Elad Hazan']</td>\n",
       "      <td>Lecture notes on optimization for machine lear...</td>\n",
       "      <td>2019-09-08T21:49:42Z</td>\n",
       "      <td>http://arxiv.org/abs/1909.03550v1</td>\n",
       "      <td>http://arxiv.org/pdf/1909.03550v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1811.04422v1</td>\n",
       "      <td>An Optimal Control View of Adversarial Machine...</td>\n",
       "      <td>['Xiaojin Zhu']</td>\n",
       "      <td>I describe an optimal control view of adversar...</td>\n",
       "      <td>2018-11-11T14:28:34Z</td>\n",
       "      <td>http://arxiv.org/abs/1811.04422v1</td>\n",
       "      <td>http://arxiv.org/pdf/1811.04422v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1707.04849v1</td>\n",
       "      <td>Minimax deviation strategies for machine learn...</td>\n",
       "      <td>['Michail Schlesinger', 'Evgeniy Vodolazskiy']</td>\n",
       "      <td>The article is devoted to the problem of small...</td>\n",
       "      <td>2017-07-16T09:15:08Z</td>\n",
       "      <td>http://arxiv.org/abs/1707.04849v1</td>\n",
       "      <td>http://arxiv.org/pdf/1707.04849v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1909.09246v1</td>\n",
       "      <td>Machine Learning for Clinical Predictive Analy...</td>\n",
       "      <td>['Wei-Hung Weng']</td>\n",
       "      <td>In this chapter, we provide a brief overview o...</td>\n",
       "      <td>2019-09-19T22:02:00Z</td>\n",
       "      <td>http://arxiv.org/abs/1909.09246v1</td>\n",
       "      <td>http://arxiv.org/pdf/1909.09246v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2301.09753v1</td>\n",
       "      <td>Towards Modular Machine Learning Solution Deve...</td>\n",
       "      <td>['Samiyuru Menik', 'Lakshmish Ramaswamy']</td>\n",
       "      <td>Machine learning technologies have demonstrate...</td>\n",
       "      <td>2023-01-23T22:54:34Z</td>\n",
       "      <td>http://arxiv.org/abs/2301.09753v1</td>\n",
       "      <td>http://arxiv.org/pdf/2301.09753v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121071</th>\n",
       "      <td>2302.08500v2</td>\n",
       "      <td>Auditing large language models: a three-layere...</td>\n",
       "      <td>['Jakob Mkander', 'Jonas Schuett', 'Hannah Ro...</td>\n",
       "      <td>Large language models (LLMs) represent a major...</td>\n",
       "      <td>2023-02-16T18:55:21Z</td>\n",
       "      <td>http://arxiv.org/abs/2302.08500v2</td>\n",
       "      <td>http://arxiv.org/abs/2302.08500v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121072</th>\n",
       "      <td>2405.14804v1</td>\n",
       "      <td>Can LLMs Solve longer Math Word Problems Better?</td>\n",
       "      <td>['Xin Xu', 'Tong Xiao', 'Zitong Chao', 'Zhenya...</td>\n",
       "      <td>Math Word Problems (MWPs) are crucial for eval...</td>\n",
       "      <td>2024-05-23T17:13:50Z</td>\n",
       "      <td>http://arxiv.org/abs/2405.14804v1</td>\n",
       "      <td>http://arxiv.org/pdf/2405.14804v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121073</th>\n",
       "      <td>2402.12991v2</td>\n",
       "      <td>TRAP: Targeted Random Adversarial Prompt Honey...</td>\n",
       "      <td>['Martin Gubri', 'Dennis Ulmer', 'Hwaran Lee',...</td>\n",
       "      <td>Large Language Model (LLM) services and models...</td>\n",
       "      <td>2024-02-20T13:20:39Z</td>\n",
       "      <td>http://arxiv.org/abs/2402.12991v2</td>\n",
       "      <td>http://arxiv.org/pdf/2402.12991v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121074</th>\n",
       "      <td>2407.10457v1</td>\n",
       "      <td>The Good, The Bad, and The Greedy: Evaluation ...</td>\n",
       "      <td>['Yifan Song', 'Guoyin Wang', 'Sujian Li', 'Bi...</td>\n",
       "      <td>Current evaluations of large language models (...</td>\n",
       "      <td>2024-07-15T06:12:17Z</td>\n",
       "      <td>http://arxiv.org/abs/2407.10457v1</td>\n",
       "      <td>http://arxiv.org/pdf/2407.10457v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121075</th>\n",
       "      <td>2409.02387v3</td>\n",
       "      <td>Large Language Models and Cognitive Science: A...</td>\n",
       "      <td>['Qian Niu', 'Junyu Liu', 'Ziqian Bi', 'Pohsun...</td>\n",
       "      <td>This comprehensive review explores the interse...</td>\n",
       "      <td>2024-09-04T02:30:12Z</td>\n",
       "      <td>http://arxiv.org/abs/2409.02387v3</td>\n",
       "      <td>http://arxiv.org/pdf/2409.02387v3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121076 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            arxiv_id                                              title  \\\n",
       "0       1909.03550v1   Lecture Notes: Optimization for Machine Learning   \n",
       "1       1811.04422v1  An Optimal Control View of Adversarial Machine...   \n",
       "2       1707.04849v1  Minimax deviation strategies for machine learn...   \n",
       "3       1909.09246v1  Machine Learning for Clinical Predictive Analy...   \n",
       "4       2301.09753v1  Towards Modular Machine Learning Solution Deve...   \n",
       "...              ...                                                ...   \n",
       "121071  2302.08500v2  Auditing large language models: a three-layere...   \n",
       "121072  2405.14804v1   Can LLMs Solve longer Math Word Problems Better?   \n",
       "121073  2402.12991v2  TRAP: Targeted Random Adversarial Prompt Honey...   \n",
       "121074  2407.10457v1  The Good, The Bad, and The Greedy: Evaluation ...   \n",
       "121075  2409.02387v3  Large Language Models and Cognitive Science: A...   \n",
       "\n",
       "                                                  authors  \\\n",
       "0                                          ['Elad Hazan']   \n",
       "1                                         ['Xiaojin Zhu']   \n",
       "2          ['Michail Schlesinger', 'Evgeniy Vodolazskiy']   \n",
       "3                                       ['Wei-Hung Weng']   \n",
       "4               ['Samiyuru Menik', 'Lakshmish Ramaswamy']   \n",
       "...                                                   ...   \n",
       "121071  ['Jakob Mkander', 'Jonas Schuett', 'Hannah Ro...   \n",
       "121072  ['Xin Xu', 'Tong Xiao', 'Zitong Chao', 'Zhenya...   \n",
       "121073  ['Martin Gubri', 'Dennis Ulmer', 'Hwaran Lee',...   \n",
       "121074  ['Yifan Song', 'Guoyin Wang', 'Sujian Li', 'Bi...   \n",
       "121075  ['Qian Niu', 'Junyu Liu', 'Ziqian Bi', 'Pohsun...   \n",
       "\n",
       "                                                  summary  \\\n",
       "0       Lecture notes on optimization for machine lear...   \n",
       "1       I describe an optimal control view of adversar...   \n",
       "2       The article is devoted to the problem of small...   \n",
       "3       In this chapter, we provide a brief overview o...   \n",
       "4       Machine learning technologies have demonstrate...   \n",
       "...                                                   ...   \n",
       "121071  Large language models (LLMs) represent a major...   \n",
       "121072  Math Word Problems (MWPs) are crucial for eval...   \n",
       "121073  Large Language Model (LLM) services and models...   \n",
       "121074  Current evaluations of large language models (...   \n",
       "121075  This comprehensive review explores the interse...   \n",
       "\n",
       "                   published                 arxiv_abstract_url  \\\n",
       "0       2019-09-08T21:49:42Z  http://arxiv.org/abs/1909.03550v1   \n",
       "1       2018-11-11T14:28:34Z  http://arxiv.org/abs/1811.04422v1   \n",
       "2       2017-07-16T09:15:08Z  http://arxiv.org/abs/1707.04849v1   \n",
       "3       2019-09-19T22:02:00Z  http://arxiv.org/abs/1909.09246v1   \n",
       "4       2023-01-23T22:54:34Z  http://arxiv.org/abs/2301.09753v1   \n",
       "...                      ...                                ...   \n",
       "121071  2023-02-16T18:55:21Z  http://arxiv.org/abs/2302.08500v2   \n",
       "121072  2024-05-23T17:13:50Z  http://arxiv.org/abs/2405.14804v1   \n",
       "121073  2024-02-20T13:20:39Z  http://arxiv.org/abs/2402.12991v2   \n",
       "121074  2024-07-15T06:12:17Z  http://arxiv.org/abs/2407.10457v1   \n",
       "121075  2024-09-04T02:30:12Z  http://arxiv.org/abs/2409.02387v3   \n",
       "\n",
       "                            arxiv_pdf_url  \n",
       "0       http://arxiv.org/pdf/1909.03550v1  \n",
       "1       http://arxiv.org/pdf/1811.04422v1  \n",
       "2       http://arxiv.org/pdf/1707.04849v1  \n",
       "3       http://arxiv.org/pdf/1909.09246v1  \n",
       "4       http://arxiv.org/pdf/2301.09753v1  \n",
       "...                                   ...  \n",
       "121071  http://arxiv.org/abs/2302.08500v2  \n",
       "121072  http://arxiv.org/pdf/2405.14804v1  \n",
       "121073  http://arxiv.org/pdf/2402.12991v2  \n",
       "121074  http://arxiv.org/pdf/2407.10457v1  \n",
       "121075  http://arxiv.org/pdf/2409.02387v3  \n",
       "\n",
       "[121076 rows x 7 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mrgd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2f6306",
   "metadata": {},
   "source": [
    "## Get citations for the arxiv papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cd23b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_id = \"1909.03550v1\"  \n",
    "\n",
    "api_url = f\"https://export.arxiv.org/api/query?id_list={arxiv_id}\"\n",
    "\n",
    "response = requests.get(api_url)\n",
    "if response.status_code == 200:\n",
    "\n",
    "\n",
    "    data = response.text\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Error fetching arXiv data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7fb30d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3D%26id_list%3D1909.03550v1%26start%3D0%26max_results%3D10\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=&amp;id_list=1909.03550v1&amp;start=0&amp;max_results=10</title>\n",
      "  <id>http://arxiv.org/api//xjuFX9oXDdEfd3WdnEfmI2/6aY</id>\n",
      "  <updated>2024-10-20T00:00:00-04:00</updated>\n",
      "  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">1</opensearch:totalResults>\n",
      "  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n",
      "  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">10</opensearch:itemsPerPage>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/1909.03550v1</id>\n",
      "    <updated>2019-09-08T21:49:42Z</updated>\n",
      "    <published>2019-09-08T21:49:42Z</published>\n",
      "    <title>Lecture Notes: Optimization for Machine Learning</title>\n",
      "    <summary>  Lecture notes on optimization for machine learning, derived from a course at\n",
      "Princeton University and tutorials given in MLSS, Buenos Aires, as well as\n",
      "Simons Foundation, Berkeley.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Elad Hazan</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/1909.03550v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1909.03550v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "</feed>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b27f77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd29c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
